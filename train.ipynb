{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5482b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1052.26it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1275.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "cola = load_dataset('glue','cola')\n",
    "# print(cola)\n",
    "# sst2 = load_dataset('glue','sst2')\n",
    "# print(sst2)\n",
    "mrpc = load_dataset('glue', 'mrpc')\n",
    "# print(mrpc)\n",
    "pt_cola = GLUEDataset([cola, mrpc], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb229056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "# sampler = FewShotEpisodeSampler(pt_cola, kShot=2, nWay=4, shuffle=True)\n",
    "# train_data_loader = data.DataLoader(\n",
    "#     pt_cola,\n",
    "#     batch_sampler=sampler,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "# data, targets = next(iter(train_data_loader))\n",
    "\n",
    "# for i in range(5):\n",
    "#     data, targets = next(iter(train_data_loader))\n",
    "#     print(data)\n",
    "#     print(targets)\n",
    "\n",
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(pt_cola, kShot=3, nWay=4, batchSize=8, shuffle=True)\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    pt_cola, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1847, 1.0977, 1.0563, 0.9941, 1.1267, 1.1590],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.618457317352295\n",
      "losses are tensor([1.1343, 1.0538, 1.0117, 0.9270, 1.0647, 1.1222],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.313683032989502\n",
      "losses are tensor([1.0864, 1.0126, 0.9710, 0.8642, 1.0059, 1.0882],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.028391361236572\n",
      "losses are tensor([1.0409, 0.9740, 0.9332, 0.8060, 0.9503, 1.0562],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.760585308074951\n",
      "losses are tensor([0.9982, 0.9374, 0.8976, 0.7521, 0.8979, 1.0258],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.508958339691162\n",
      "losses are tensor([1.0949, 1.2037, 1.1558, 1.1601, 1.0153, 1.0449],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.674637794494629\n",
      "losses are tensor([1.0694, 1.1863, 1.0846, 1.1357, 0.9434, 0.9573],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.376493453979492\n",
      "losses are tensor([1.0454, 1.1690, 1.0200, 1.1118, 0.8799, 0.8794],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.1053948402404785\n",
      "losses are tensor([1.0216, 1.1516, 0.9614, 1.0877, 0.8228, 0.8105],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.855450630187988\n",
      "losses are tensor([0.9977, 1.1340, 0.9089, 1.0638, 0.7718, 0.7503],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.626491546630859\n",
      "outer loop losses are [tensor(1.1983, grad_fn=<UnbindBackward0>), tensor(1.2717, grad_fn=<UnbindBackward0>), tensor(1.3934, grad_fn=<UnbindBackward0>)] and loss is 3.8634543418884277\n",
      "local param grad is tensor([[-5.7245e-05, -6.9133e-05, -2.8859e-05,  ..., -6.2459e-06,\n",
      "         -5.0785e-05,  1.8389e-05],\n",
      "        [-2.3979e-05, -1.0598e-06,  1.4668e-05,  ...,  1.6283e-05,\n",
      "         -5.0376e-05, -6.5210e-05],\n",
      "        [ 4.0287e-05,  7.5178e-06, -1.1077e-06,  ..., -4.2338e-05,\n",
      "         -7.3401e-05,  2.2562e-05],\n",
      "        ...,\n",
      "        [-1.3780e-04,  1.5008e-04,  1.3233e-04,  ...,  9.1239e-05,\n",
      "         -1.1416e-04, -9.1577e-05],\n",
      "        [ 1.4642e-04,  1.4568e-04, -7.5491e-05,  ..., -1.8708e-04,\n",
      "         -9.2497e-05, -7.2648e-05],\n",
      "        [-4.0304e-05,  2.1996e-04, -2.9539e-04,  ...,  2.6587e-05,\n",
      "         -2.0045e-04, -3.5963e-04]])\n",
      "local param grad is tensor([[-3.7721e-05, -5.2577e-05, -2.3853e-05,  ..., -1.3674e-05,\n",
      "         -3.3122e-05,  1.5739e-05],\n",
      "        [-2.1546e-05, -6.0775e-06,  5.5486e-06,  ...,  1.3648e-05,\n",
      "         -2.9915e-05, -4.5793e-05],\n",
      "        [ 2.3099e-05, -9.2679e-06, -3.5885e-06,  ..., -2.8902e-05,\n",
      "         -3.7567e-05,  2.5535e-05],\n",
      "        ...,\n",
      "        [-9.5035e-05,  9.3033e-05,  7.5816e-05,  ...,  6.9046e-05,\n",
      "         -7.9812e-05, -5.4601e-05],\n",
      "        [ 1.0661e-04,  7.9574e-05, -5.8680e-05,  ..., -1.2294e-04,\n",
      "         -5.9115e-05, -5.7451e-05],\n",
      "        [-2.2792e-07,  1.6773e-04, -2.3404e-04,  ..., -5.4026e-05,\n",
      "         -1.4285e-04, -2.1274e-04]])\n",
      "meta param grad is tensor([[-9.4966e-05, -1.2171e-04, -5.2712e-05,  ..., -1.9920e-05,\n",
      "         -8.3907e-05,  3.4128e-05],\n",
      "        [-4.5525e-05, -7.1373e-06,  2.0217e-05,  ...,  2.9930e-05,\n",
      "         -8.0291e-05, -1.1100e-04],\n",
      "        [ 6.3386e-05, -1.7501e-06, -4.6961e-06,  ..., -7.1240e-05,\n",
      "         -1.1097e-04,  4.8098e-05],\n",
      "        ...,\n",
      "        [-2.3284e-04,  2.4311e-04,  2.0815e-04,  ...,  1.6029e-04,\n",
      "         -1.9397e-04, -1.4618e-04],\n",
      "        [ 2.5303e-04,  2.2525e-04, -1.3417e-04,  ..., -3.1001e-04,\n",
      "         -1.5161e-04, -1.3010e-04],\n",
      "        [-4.0532e-05,  3.8770e-04, -5.2943e-04,  ..., -2.7439e-05,\n",
      "         -3.4330e-04, -5.7237e-04]])\n",
      "outer loop losses are [tensor(1.1983, grad_fn=<UnbindBackward0>), tensor(1.2717, grad_fn=<UnbindBackward0>), tensor(1.3934, grad_fn=<UnbindBackward0>), tensor(1.1976, grad_fn=<UnbindBackward0>), tensor(1.6342, grad_fn=<UnbindBackward0>), tensor(1.5230, grad_fn=<UnbindBackward0>), tensor(1.2446, grad_fn=<UnbindBackward0>), tensor(1.0862, grad_fn=<UnbindBackward0>), tensor(1.0126, grad_fn=<UnbindBackward0>), tensor(1.5808, grad_fn=<UnbindBackward0>), tensor(1.1076, grad_fn=<UnbindBackward0>), tensor(1.4640, grad_fn=<UnbindBackward0>)] and loss is 11.850619316101074\n",
      "local param grad is tensor([[ 2.0922e-04, -5.6665e-05, -5.4165e-05,  ...,  8.2203e-05,\n",
      "          1.6861e-04, -1.6170e-04],\n",
      "        [-4.6488e-05,  1.1742e-05,  8.9096e-05,  ...,  7.9023e-05,\n",
      "         -1.2645e-04, -6.2587e-05],\n",
      "        [-4.4576e-05, -3.2575e-05, -2.1752e-04,  ..., -1.7871e-04,\n",
      "         -5.5899e-05,  1.3049e-04],\n",
      "        ...,\n",
      "        [-4.3120e-05,  6.3451e-05,  6.8320e-05,  ...,  1.3733e-04,\n",
      "          1.4297e-04, -4.1972e-05],\n",
      "        [ 4.3196e-04,  3.4092e-04, -1.0942e-03,  ..., -3.1820e-04,\n",
      "         -8.7666e-04,  1.4432e-05],\n",
      "        [-2.7206e-04, -1.0714e-04,  6.4484e-04,  ...,  4.9139e-04,\n",
      "         -2.6518e-05,  5.3623e-04]])\n",
      "local param grad is tensor([[ 1.6275e-04, -6.5540e-05, -1.0405e-05,  ...,  1.2447e-04,\n",
      "          2.1908e-04, -1.7863e-04],\n",
      "        [-4.5402e-05,  5.6322e-05,  8.9792e-05,  ...,  9.1639e-05,\n",
      "         -1.0481e-04, -9.1251e-05],\n",
      "        [ 5.9426e-07, -9.4186e-05, -1.9978e-04,  ..., -1.9872e-04,\n",
      "         -1.1093e-04,  1.7213e-04],\n",
      "        ...,\n",
      "        [-7.3968e-05,  6.6326e-05,  6.5173e-05,  ...,  9.5191e-05,\n",
      "          1.0099e-04,  1.1352e-05],\n",
      "        [ 4.2994e-04,  3.3806e-04, -1.1131e-03,  ..., -3.4345e-04,\n",
      "         -8.8976e-04, -4.7619e-05],\n",
      "        [-1.4531e-04, -1.2121e-04,  6.4640e-04,  ...,  3.9117e-04,\n",
      "         -1.2533e-04,  6.3344e-04]])\n",
      "meta param grad is tensor([[ 2.7700e-04, -2.4392e-04, -1.1728e-04,  ...,  1.8675e-04,\n",
      "          3.0379e-04, -3.0620e-04],\n",
      "        [-1.3742e-04,  6.0927e-05,  1.9911e-04,  ...,  2.0059e-04,\n",
      "         -3.1155e-04, -2.6484e-04],\n",
      "        [ 1.9404e-05, -1.2851e-04, -4.2199e-04,  ..., -4.4867e-04,\n",
      "         -2.7780e-04,  3.5072e-04],\n",
      "        ...,\n",
      "        [-3.4993e-04,  3.7289e-04,  3.4164e-04,  ...,  3.9281e-04,\n",
      "          4.9990e-05, -1.7680e-04],\n",
      "        [ 1.1149e-03,  9.0423e-04, -2.3415e-03,  ..., -9.7166e-04,\n",
      "         -1.9180e-03, -1.6329e-04],\n",
      "        [-4.5790e-04,  1.5936e-04,  7.6182e-04,  ...,  8.5512e-04,\n",
      "         -4.9515e-04,  5.9729e-04]])\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.0129, 1.0880, 1.1125, 1.0542, 1.0016, 1.0958],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.3650288581848145\n",
      "losses are tensor([0.9235, 1.0033, 1.0416, 0.9752, 0.9315, 1.0136],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.88877010345459\n",
      "losses are tensor([0.8561, 0.9346, 0.9851, 0.9105, 0.8745, 0.9487],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.509384632110596\n",
      "losses are tensor([0.8023, 0.8771, 0.9389, 0.8567, 0.8273, 0.8990],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.201318740844727\n",
      "losses are tensor([0.7582, 0.8306, 0.9013, 0.8131, 0.7872, 0.8574],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.94773006439209\n",
      "losses are tensor([1.0255, 1.0405, 1.1153, 1.1687, 1.0918, 1.0126],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.4544501304626465\n",
      "losses are tensor([0.9657, 0.9523, 1.0429, 1.0935, 0.9932, 0.9184],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.96586799621582\n",
      "losses are tensor([0.9192, 0.8842, 0.9886, 1.0349, 0.9173, 0.8459],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.590068340301514\n",
      "losses are tensor([0.8828, 0.8305, 0.9480, 0.9894, 0.8583, 0.7898],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.298739433288574\n",
      "losses are tensor([0.8534, 0.7880, 0.9163, 0.9523, 0.8114, 0.7455],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.066969394683838\n",
      "outer loop losses are [tensor(1.1122, grad_fn=<UnbindBackward0>), tensor(1.1270, grad_fn=<UnbindBackward0>), tensor(1.0552, grad_fn=<UnbindBackward0>), tensor(1.0361, grad_fn=<UnbindBackward0>), tensor(1.0324, grad_fn=<UnbindBackward0>), tensor(1.0003, grad_fn=<UnbindBackward0>)] and loss is 6.3632049560546875\n",
      "local param grad is tensor([[-1.0661e-04,  1.5249e-05, -1.8619e-04,  ...,  2.7839e-04,\n",
      "          1.7441e-04,  1.9981e-04],\n",
      "        [-5.5655e-06, -4.1665e-05,  4.9399e-05,  ...,  9.9206e-05,\n",
      "         -2.9607e-05,  4.5983e-05],\n",
      "        [-7.7174e-05, -1.7969e-04,  2.9961e-04,  ..., -1.1487e-04,\n",
      "          2.1332e-04, -2.7049e-04],\n",
      "        ...,\n",
      "        [-3.1547e-04, -4.2109e-05,  2.4109e-04,  ...,  1.2095e-04,\n",
      "         -1.9804e-04, -3.7830e-05],\n",
      "        [-1.3066e-04, -1.9990e-04,  1.9587e-04,  ...,  2.2672e-04,\n",
      "         -1.1872e-04, -8.0345e-05],\n",
      "        [-2.7291e-04, -1.9432e-04,  3.4052e-04,  ..., -1.1653e-04,\n",
      "         -6.7887e-05, -1.9241e-04]])\n",
      "local param grad is tensor([[-8.8521e-05,  5.3978e-06, -1.6239e-04,  ...,  2.4029e-04,\n",
      "          1.5767e-04,  1.5127e-04],\n",
      "        [-1.2925e-05, -5.1004e-05,  3.6947e-05,  ...,  8.6747e-05,\n",
      "         -2.0293e-05,  3.6867e-05],\n",
      "        [-7.6083e-05, -1.6974e-04,  2.7333e-04,  ..., -1.0253e-04,\n",
      "          2.1118e-04, -2.4580e-04],\n",
      "        ...,\n",
      "        [-3.7654e-04, -7.2203e-05,  2.2516e-04,  ...,  1.8595e-04,\n",
      "         -1.2052e-04, -3.7790e-05],\n",
      "        [-5.6907e-05, -1.4305e-04,  1.2346e-04,  ...,  1.5089e-04,\n",
      "         -1.4111e-04, -5.7589e-05],\n",
      "        [-3.6359e-04, -2.1902e-04,  3.2246e-04,  ..., -3.2629e-05,\n",
      "          2.1712e-06, -1.7174e-04]])\n",
      "meta param grad is tensor([[ 8.1864e-05, -2.2327e-04, -4.6586e-04,  ...,  7.0544e-04,\n",
      "          6.3586e-04,  4.4886e-05],\n",
      "        [-1.5591e-04, -3.1742e-05,  2.8545e-04,  ...,  3.8655e-04,\n",
      "         -3.6145e-04, -1.8199e-04],\n",
      "        [-1.3385e-04, -4.7794e-04,  1.5095e-04,  ..., -6.6607e-04,\n",
      "          1.4670e-04, -1.6557e-04],\n",
      "        ...,\n",
      "        [-1.0419e-03,  2.5858e-04,  8.0789e-04,  ...,  6.9971e-04,\n",
      "         -2.6857e-04, -2.5242e-04],\n",
      "        [ 9.2737e-04,  5.6128e-04, -2.0222e-03,  ..., -5.9405e-04,\n",
      "         -2.1779e-03, -3.0122e-04],\n",
      "        [-1.0944e-03, -2.5398e-04,  1.4248e-03,  ...,  7.0596e-04,\n",
      "         -5.6087e-04,  2.3314e-04]])\n",
      "outer loop losses are [tensor(1.1122, grad_fn=<UnbindBackward0>), tensor(1.1270, grad_fn=<UnbindBackward0>), tensor(1.0552, grad_fn=<UnbindBackward0>), tensor(1.0361, grad_fn=<UnbindBackward0>), tensor(1.0324, grad_fn=<UnbindBackward0>), tensor(1.0003, grad_fn=<UnbindBackward0>), tensor(0.9773, grad_fn=<UnbindBackward0>), tensor(1.1368, grad_fn=<UnbindBackward0>), tensor(1.1243, grad_fn=<UnbindBackward0>), tensor(1.0487, grad_fn=<UnbindBackward0>), tensor(1.0856, grad_fn=<UnbindBackward0>), tensor(0.9097, grad_fn=<UnbindBackward0>)] and loss is 6.282445907592773\n",
      "local param grad is tensor([[-2.9057e-05,  3.2738e-05, -1.0245e-05,  ...,  8.2866e-05,\n",
      "         -8.3079e-05, -1.4640e-04],\n",
      "        [-1.6145e-05, -1.3538e-05, -1.8651e-05,  ..., -1.2972e-05,\n",
      "          2.3077e-05, -5.7922e-05],\n",
      "        [-5.5854e-06, -5.5706e-06,  4.0804e-05,  ..., -2.5618e-05,\n",
      "          1.0159e-04,  7.6772e-06],\n",
      "        ...,\n",
      "        [-1.4580e-04,  7.8753e-05,  5.5741e-05,  ..., -8.2828e-05,\n",
      "          1.5726e-04,  1.0107e-04],\n",
      "        [ 2.3770e-04, -3.3627e-04, -6.9347e-05,  ..., -1.7803e-04,\n",
      "          7.8845e-07,  4.1796e-04],\n",
      "        [-3.8966e-04,  5.2166e-05,  4.8826e-04,  ...,  3.3585e-04,\n",
      "          3.4981e-04, -2.9446e-06]])\n",
      "local param grad is tensor([[-3.1525e-05,  3.4387e-05, -9.0191e-06,  ...,  9.3771e-05,\n",
      "         -7.8494e-05, -1.6154e-04],\n",
      "        [-2.0001e-05, -1.8446e-05, -2.5169e-05,  ..., -1.1081e-05,\n",
      "          2.7953e-05, -6.1846e-05],\n",
      "        [-1.4836e-05, -1.1742e-05,  4.0994e-05,  ..., -5.1392e-06,\n",
      "          1.1785e-04, -1.3665e-05],\n",
      "        ...,\n",
      "        [-1.7365e-04,  8.4085e-05,  5.6443e-05,  ..., -8.8170e-05,\n",
      "          1.8924e-04,  1.0925e-04],\n",
      "        [ 2.4121e-04, -4.1281e-04, -6.3615e-05,  ..., -1.8560e-04,\n",
      "         -2.5020e-06,  4.2995e-04],\n",
      "        [-4.5624e-04,  5.5967e-05,  5.6443e-04,  ...,  3.7375e-04,\n",
      "          3.9412e-04, -1.5636e-05]])\n",
      "meta param grad is tensor([[ 2.1283e-05, -1.5614e-04, -4.8513e-04,  ...,  8.8207e-04,\n",
      "          4.7429e-04, -2.6305e-04],\n",
      "        [-1.9205e-04, -6.3726e-05,  2.4163e-04,  ...,  3.6249e-04,\n",
      "         -3.1042e-04, -3.0176e-04],\n",
      "        [-1.5427e-04, -4.9525e-04,  2.3274e-04,  ..., -6.9683e-04,\n",
      "          3.6614e-04, -1.7156e-04],\n",
      "        ...,\n",
      "        [-1.3614e-03,  4.2141e-04,  9.2007e-04,  ...,  5.2871e-04,\n",
      "          7.7936e-05, -4.2100e-05],\n",
      "        [ 1.4063e-03, -1.8780e-04, -2.1551e-03,  ..., -9.5768e-04,\n",
      "         -2.1796e-03,  5.4669e-04],\n",
      "        [-1.9403e-03, -1.4585e-04,  2.4775e-03,  ...,  1.4156e-03,\n",
      "          1.8306e-04,  2.1456e-04]])\n",
      "0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1500, 1.0360, 1.1498, 1.1181, 1.0903, 0.9852],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.5294189453125\n",
      "losses are tensor([1.1172, 0.9583, 1.0830, 1.0838, 1.0487, 0.9187],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.209752082824707\n",
      "losses are tensor([1.0850, 0.8892, 1.0244, 1.0504, 1.0080, 0.8606],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.917734146118164\n",
      "losses are tensor([1.0530, 0.8294, 0.9732, 1.0170, 0.9691, 0.8101],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.651802062988281\n",
      "losses are tensor([1.0218, 0.7768, 0.9277, 0.9845, 0.9312, 0.7642],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.406229496002197\n",
      "losses are tensor([1.1265, 1.1125, 0.9940, 1.0921, 1.0974, 1.0740],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.496621131896973\n",
      "losses are tensor([1.0774, 1.0702, 0.9424, 1.0293, 1.0425, 1.0138],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.175695896148682\n",
      "losses are tensor([1.0316, 1.0302, 0.8959, 0.9729, 0.9914, 0.9604],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.882369041442871\n",
      "losses are tensor([0.9879, 0.9915, 0.8540, 0.9226, 0.9425, 0.9118],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.610360622406006\n",
      "losses are tensor([0.9460, 0.9543, 0.8170, 0.8772, 0.8968, 0.8676],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.359050273895264\n",
      "outer loop losses are [tensor(0.9017, grad_fn=<UnbindBackward0>), tensor(1.0254, grad_fn=<UnbindBackward0>), tensor(1.4734, grad_fn=<UnbindBackward0>), tensor(1.0865, grad_fn=<UnbindBackward0>), tensor(1.5565, grad_fn=<UnbindBackward0>)] and loss is 6.043615341186523\n",
      "local param grad is tensor([[-7.0278e-05,  1.3098e-04,  9.2720e-06,  ..., -6.9323e-05,\n",
      "          7.7149e-06, -4.2592e-05],\n",
      "        [ 2.8782e-05, -4.1543e-07,  5.2429e-06,  ..., -9.2869e-05,\n",
      "         -8.3580e-05,  7.2272e-05],\n",
      "        [-2.5318e-05,  8.7239e-06,  1.2365e-05,  ..., -1.7109e-05,\n",
      "         -7.5833e-05,  7.8489e-05],\n",
      "        ...,\n",
      "        [-4.9804e-06,  7.7776e-05,  2.3532e-04,  ...,  3.8642e-04,\n",
      "         -1.2650e-04,  1.0220e-05],\n",
      "        [ 1.8832e-05, -9.1419e-05, -1.8557e-04,  ...,  1.9111e-04,\n",
      "          1.4972e-05, -1.4796e-04],\n",
      "        [-3.9752e-05,  1.0355e-04,  3.9310e-04,  ...,  1.8452e-04,\n",
      "          1.6095e-04,  1.0863e-04]])\n",
      "local param grad is tensor([[-3.2660e-05,  9.4804e-05, -8.1739e-07,  ..., -8.7032e-05,\n",
      "         -1.3757e-05, -1.2999e-05],\n",
      "        [ 5.0104e-05,  2.3805e-05,  1.2382e-05,  ..., -1.3208e-04,\n",
      "         -1.1704e-04,  7.8549e-05],\n",
      "        [-2.7830e-05,  2.7273e-05,  4.1891e-05,  ..., -3.4024e-05,\n",
      "         -1.2298e-04,  1.4928e-04],\n",
      "        ...,\n",
      "        [-2.9907e-05,  1.6792e-04,  3.8786e-04,  ...,  6.3500e-04,\n",
      "         -9.5580e-05,  2.6068e-05],\n",
      "        [-6.2218e-06, -1.3967e-04, -1.8470e-04,  ...,  4.0082e-04,\n",
      "          1.0154e-04, -1.9800e-04],\n",
      "        [-1.1603e-04,  1.5717e-04,  7.0824e-04,  ...,  4.6414e-04,\n",
      "          2.5015e-04,  2.5040e-04]])\n",
      "meta param grad is tensor([[-8.1655e-05,  6.9645e-05, -4.7667e-04,  ...,  7.2572e-04,\n",
      "          4.6825e-04, -3.1864e-04],\n",
      "        [-1.1317e-04, -4.0337e-05,  2.5926e-04,  ...,  1.3754e-04,\n",
      "         -5.1103e-04, -1.5094e-04],\n",
      "        [-2.0742e-04, -4.5925e-04,  2.8700e-04,  ..., -7.4796e-04,\n",
      "          1.6732e-04,  5.6217e-05],\n",
      "        ...,\n",
      "        [-1.3963e-03,  6.6711e-04,  1.5433e-03,  ...,  1.5501e-03,\n",
      "         -1.4415e-04, -5.8122e-06],\n",
      "        [ 1.4189e-03, -4.1889e-04, -2.5254e-03,  ..., -3.6575e-04,\n",
      "         -2.0631e-03,  2.0073e-04],\n",
      "        [-2.0961e-03,  1.1487e-04,  3.5788e-03,  ...,  2.0642e-03,\n",
      "          5.9415e-04,  5.7360e-04]])\n",
      "outer loop losses are [tensor(0.9017, grad_fn=<UnbindBackward0>), tensor(1.0254, grad_fn=<UnbindBackward0>), tensor(1.4734, grad_fn=<UnbindBackward0>), tensor(1.0865, grad_fn=<UnbindBackward0>), tensor(1.5565, grad_fn=<UnbindBackward0>), tensor(1.4037, grad_fn=<UnbindBackward0>), tensor(1.0049, grad_fn=<UnbindBackward0>), tensor(1.5538, grad_fn=<UnbindBackward0>), tensor(1.0243, grad_fn=<UnbindBackward0>), tensor(0.9623, grad_fn=<UnbindBackward0>), tensor(1.0254, grad_fn=<UnbindBackward0>), tensor(1.4163, grad_fn=<UnbindBackward0>)] and loss is 8.390742301940918\n",
      "local param grad is tensor([[ 3.3028e-05, -5.3937e-05, -3.6836e-05,  ..., -2.5202e-06,\n",
      "         -1.1388e-04,  1.8607e-05],\n",
      "        [ 1.0839e-04, -4.1957e-06,  1.5799e-05,  ...,  3.9557e-06,\n",
      "         -6.5435e-05,  8.4324e-05],\n",
      "        [-1.2746e-04, -2.6922e-06,  7.2400e-05,  ..., -7.5867e-05,\n",
      "          1.0593e-04, -3.3283e-05],\n",
      "        ...,\n",
      "        [-1.7210e-04,  6.3486e-05,  2.9345e-04,  ...,  1.5335e-04,\n",
      "          1.8462e-04, -2.9717e-04],\n",
      "        [-1.3401e-04, -1.1584e-04, -2.2479e-06,  ..., -7.5701e-05,\n",
      "          2.1187e-04, -2.0686e-04],\n",
      "        [-3.8571e-05,  2.2992e-05,  1.3218e-04,  ..., -9.4147e-05,\n",
      "         -1.0064e-04, -2.6387e-04]])\n",
      "local param grad is tensor([[ 4.1284e-05, -4.9922e-05, -3.2573e-05,  ..., -6.8176e-06,\n",
      "         -1.0362e-04,  4.9137e-06],\n",
      "        [ 6.6191e-05, -1.7440e-05,  1.8800e-05,  ..., -2.4716e-05,\n",
      "         -6.7682e-05,  5.9215e-05],\n",
      "        [-1.4493e-04, -6.4155e-06,  1.0684e-04,  ..., -6.6736e-05,\n",
      "          1.2105e-04, -4.1034e-06],\n",
      "        ...,\n",
      "        [-1.5269e-04,  5.0531e-05,  4.1831e-04,  ...,  2.5199e-04,\n",
      "          2.1097e-04, -3.5771e-04],\n",
      "        [-1.3737e-04, -1.0439e-04,  3.4128e-05,  ..., -9.5335e-05,\n",
      "          2.1002e-04, -1.9749e-04],\n",
      "        [-7.2178e-06,  3.1744e-05,  2.1105e-04,  ..., -1.1498e-05,\n",
      "         -8.7011e-05, -1.9786e-04]])\n",
      "meta param grad is tensor([[-7.3438e-06, -3.4214e-05, -5.4608e-04,  ...,  7.1638e-04,\n",
      "          2.5075e-04, -2.9512e-04],\n",
      "        [ 6.1415e-05, -6.1972e-05,  2.9386e-04,  ...,  1.1678e-04,\n",
      "         -6.4415e-04, -7.3982e-06],\n",
      "        [-4.7981e-04, -4.6836e-04,  4.6624e-04,  ..., -8.9056e-04,\n",
      "          3.9429e-04,  1.8831e-05],\n",
      "        ...,\n",
      "        [-1.7211e-03,  7.8113e-04,  2.2550e-03,  ...,  1.9555e-03,\n",
      "          2.5144e-04, -6.6070e-04],\n",
      "        [ 1.1475e-03, -6.3911e-04, -2.4935e-03,  ..., -5.3679e-04,\n",
      "         -1.6412e-03, -2.0362e-04],\n",
      "        [-2.1419e-03,  1.6961e-04,  3.9221e-03,  ...,  1.9586e-03,\n",
      "          4.0650e-04,  1.1187e-04]])\n",
      "0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1648, 1.1016, 1.1102, 1.0420, 1.1616, 1.1642],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.744410037994385\n",
      "losses are tensor([1.1327, 1.0658, 1.0731, 1.0094, 1.1292, 1.1264],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.536628246307373\n",
      "losses are tensor([1.1023, 1.0322, 1.0382, 0.9795, 1.1000, 1.0917],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.343918323516846\n",
      "losses are tensor([1.0734, 1.0005, 1.0051, 0.9521, 1.0727, 1.0597],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.163517475128174\n",
      "losses are tensor([1.0457, 0.9701, 0.9735, 0.9265, 1.0477, 1.0298],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.993203639984131\n",
      "losses are tensor([1.0688, 0.9863, 1.0410, 1.1311, 1.0704, 1.0109],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.308468818664551\n",
      "losses are tensor([1.0247, 0.9474, 0.9809, 1.0988, 0.9953, 0.9420],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.989127159118652\n",
      "losses are tensor([0.9843, 0.9132, 0.9245, 1.0694, 0.9262, 0.8784],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.695939064025879\n",
      "losses are tensor([0.9469, 0.8810, 0.8722, 1.0418, 0.8628, 0.8203],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.424942970275879\n",
      "losses are tensor([0.9117, 0.8499, 0.8235, 1.0150, 0.8037, 0.7664],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.170078277587891\n",
      "outer loop losses are [tensor(1.1547, grad_fn=<UnbindBackward0>), tensor(1.1729, grad_fn=<UnbindBackward0>), tensor(1.5309, grad_fn=<UnbindBackward0>), tensor(1.2473, grad_fn=<UnbindBackward0>), tensor(1.1311, grad_fn=<UnbindBackward0>)] and loss is 6.236970901489258\n",
      "local param grad is tensor([[ 8.6232e-06, -3.8664e-06,  3.2092e-05,  ...,  2.4567e-05,\n",
      "         -1.0569e-04, -7.6108e-05],\n",
      "        [-1.4031e-05,  9.8752e-06,  1.9469e-06,  ..., -3.0707e-05,\n",
      "         -9.6972e-06,  3.4625e-05],\n",
      "        [ 6.1134e-05, -5.4049e-05, -1.1302e-04,  ..., -2.9534e-05,\n",
      "         -3.6054e-04, -2.1247e-04],\n",
      "        ...,\n",
      "        [-2.6524e-04, -1.4724e-04,  1.8418e-04,  ...,  3.7367e-04,\n",
      "          4.8947e-05,  1.2556e-04],\n",
      "        [ 7.7534e-05, -1.6484e-04, -6.9470e-05,  ..., -1.8751e-05,\n",
      "          7.7098e-05, -1.0586e-04],\n",
      "        [-3.2158e-04,  1.4703e-04,  2.2355e-04,  ...,  1.9061e-04,\n",
      "          2.4107e-05,  1.8056e-04]])\n",
      "local param grad is tensor([[-4.4022e-06,  1.3554e-05,  3.3974e-05,  ...,  1.8744e-05,\n",
      "         -1.0614e-04, -8.0313e-05],\n",
      "        [-1.1813e-05,  2.4417e-05, -3.2968e-06,  ..., -3.6068e-05,\n",
      "         -3.9160e-06,  3.9562e-05],\n",
      "        [ 7.9328e-05, -5.3885e-05, -1.1582e-04,  ..., -4.8879e-05,\n",
      "         -3.5134e-04, -2.3003e-04],\n",
      "        ...,\n",
      "        [-2.0660e-04, -1.3385e-04,  1.9224e-04,  ...,  3.4045e-04,\n",
      "          5.6889e-05,  1.2811e-04],\n",
      "        [ 7.7631e-05, -2.1152e-04, -8.9347e-05,  ...,  2.3425e-05,\n",
      "          1.1343e-04, -9.6378e-05],\n",
      "        [-3.1902e-04,  1.9978e-04,  1.9573e-04,  ...,  1.7440e-04,\n",
      "         -4.8356e-05,  2.0052e-04]])\n",
      "meta param grad is tensor([[-3.1229e-06, -2.4527e-05, -4.8002e-04,  ...,  7.5969e-04,\n",
      "          3.8917e-05, -4.5154e-04],\n",
      "        [ 3.5571e-05, -2.7680e-05,  2.9251e-04,  ...,  5.0008e-05,\n",
      "         -6.5776e-04,  6.6789e-05],\n",
      "        [-3.3935e-04, -5.7630e-04,  2.3740e-04,  ..., -9.6898e-04,\n",
      "         -3.1758e-04, -4.2367e-04],\n",
      "        ...,\n",
      "        [-2.1929e-03,  5.0004e-04,  2.6314e-03,  ...,  2.6696e-03,\n",
      "          3.5728e-04, -4.0703e-04],\n",
      "        [ 1.3027e-03, -1.0155e-03, -2.6523e-03,  ..., -5.3211e-04,\n",
      "         -1.4507e-03, -4.0586e-04],\n",
      "        [-2.7825e-03,  5.1642e-04,  4.3414e-03,  ...,  2.3236e-03,\n",
      "          3.8225e-04,  4.9295e-04]])\n",
      "outer loop losses are [tensor(1.1547, grad_fn=<UnbindBackward0>), tensor(1.1729, grad_fn=<UnbindBackward0>), tensor(1.5309, grad_fn=<UnbindBackward0>), tensor(1.2473, grad_fn=<UnbindBackward0>), tensor(1.1311, grad_fn=<UnbindBackward0>), tensor(1.2759, grad_fn=<UnbindBackward0>), tensor(1.1135, grad_fn=<UnbindBackward0>), tensor(1.0642, grad_fn=<UnbindBackward0>), tensor(1.5564, grad_fn=<UnbindBackward0>), tensor(1.0656, grad_fn=<UnbindBackward0>), tensor(1.0184, grad_fn=<UnbindBackward0>), tensor(1.0698, grad_fn=<UnbindBackward0>)] and loss is 8.163834571838379\n",
      "local param grad is tensor([[ 7.8720e-05, -1.3430e-05, -6.2922e-05,  ...,  1.7092e-05,\n",
      "          2.9454e-06, -1.6423e-05],\n",
      "        [ 9.6058e-05,  1.0162e-05, -3.2450e-05,  ..., -6.0664e-05,\n",
      "          6.1485e-05,  7.0567e-05],\n",
      "        [-9.3630e-06,  9.6192e-06,  7.1714e-05,  ..., -4.8443e-05,\n",
      "         -4.0562e-05, -1.0573e-05],\n",
      "        ...,\n",
      "        [-4.3995e-04, -8.8624e-05,  4.9149e-04,  ...,  1.5038e-04,\n",
      "         -1.7962e-04,  8.7546e-06],\n",
      "        [-4.6450e-04,  2.7674e-05, -4.8665e-05,  ..., -1.0302e-04,\n",
      "          1.7310e-04, -6.6583e-05],\n",
      "        [-6.0389e-04, -2.2870e-04,  8.5487e-04,  ...,  4.0968e-04,\n",
      "          2.9371e-04, -2.8861e-04]])\n",
      "local param grad is tensor([[ 7.1061e-05, -1.0227e-05, -7.0201e-05,  ..., -7.4658e-06,\n",
      "         -9.3479e-06, -3.5229e-05],\n",
      "        [ 5.4407e-05,  2.2881e-05, -6.1981e-05,  ..., -7.5139e-05,\n",
      "          7.1507e-05,  4.9364e-05],\n",
      "        [-1.7172e-05,  6.8513e-06,  6.7141e-05,  ...,  2.7808e-06,\n",
      "          1.5179e-05,  2.7247e-05],\n",
      "        ...,\n",
      "        [-3.8132e-04, -1.0870e-04,  5.8676e-04,  ...,  1.7620e-04,\n",
      "         -1.8640e-04,  1.5214e-05],\n",
      "        [-5.0550e-04,  3.3302e-05, -7.7649e-05,  ..., -9.5477e-05,\n",
      "          1.6087e-04, -1.1095e-04],\n",
      "        [-4.5627e-04, -2.2509e-04,  9.6581e-04,  ...,  4.6263e-04,\n",
      "          1.9668e-04, -1.9343e-04]])\n",
      "meta param grad is tensor([[ 1.4666e-04, -4.8184e-05, -6.1314e-04,  ...,  7.6932e-04,\n",
      "          3.2514e-05, -5.0319e-04],\n",
      "        [ 1.8604e-04,  5.3638e-06,  1.9807e-04,  ..., -8.5795e-05,\n",
      "         -5.2477e-04,  1.8672e-04],\n",
      "        [-3.6589e-04, -5.5982e-04,  3.7625e-04,  ..., -1.0146e-03,\n",
      "         -3.4296e-04, -4.0700e-04],\n",
      "        ...,\n",
      "        [-3.0142e-03,  3.0271e-04,  3.7097e-03,  ...,  2.9962e-03,\n",
      "         -8.7475e-06, -3.8306e-04],\n",
      "        [ 3.3268e-04, -9.5449e-04, -2.7786e-03,  ..., -7.3061e-04,\n",
      "         -1.1167e-03, -5.8339e-04],\n",
      "        [-3.8426e-03,  6.2637e-05,  6.1620e-03,  ...,  3.1959e-03,\n",
      "          8.7264e-04,  1.0913e-05]])\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.9947, 1.0784, 1.1906, 1.2061, 1.0914, 1.1038],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.665034294128418\n",
      "losses are tensor([0.9370, 1.0203, 1.1453, 1.1442, 1.0479, 1.0636],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.35822057723999\n",
      "losses are tensor([0.8862, 0.9694, 1.1055, 1.0895, 1.0092, 1.0275],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.087203025817871\n",
      "losses are tensor([0.8413, 0.9238, 1.0697, 1.0403, 0.9738, 0.9944],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.843218803405762\n",
      "losses are tensor([0.8009, 0.8832, 1.0385, 0.9963, 0.9417, 0.9637],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.6242547035217285\n",
      "losses are tensor([1.0878, 1.1881, 1.0774, 1.0214, 1.1660, 1.0334],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.574105262756348\n",
      "losses are tensor([1.0403, 1.1347, 1.0137, 0.9667, 1.1071, 0.9803],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.242859840393066\n",
      "losses are tensor([0.9963, 1.0859, 0.9552, 0.9164, 1.0526, 0.9311],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.937490940093994\n",
      "losses are tensor([0.9558, 1.0412, 0.9013, 0.8712, 1.0021, 0.8856],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.6572184562683105\n",
      "losses are tensor([0.9191, 1.0013, 0.8516, 0.8307, 0.9550, 0.8434],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.401143550872803\n",
      "outer loop losses are [tensor(1.0659, grad_fn=<UnbindBackward0>), tensor(1.5612, grad_fn=<UnbindBackward0>), tensor(1.0938, grad_fn=<UnbindBackward0>), tensor(1.4818, grad_fn=<UnbindBackward0>), tensor(1.0847, grad_fn=<UnbindBackward0>), tensor(1.0489, grad_fn=<UnbindBackward0>), tensor(1.5265, grad_fn=<UnbindBackward0>), tensor(1.1612, grad_fn=<UnbindBackward0>), tensor(1.1887, grad_fn=<UnbindBackward0>), tensor(1.4979, grad_fn=<UnbindBackward0>)] and loss is 12.710646629333496\n",
      "local param grad is tensor([[-2.7788e-05,  2.2480e-04, -1.3736e-04,  ...,  1.0886e-04,\n",
      "         -1.7028e-04, -1.3087e-05],\n",
      "        [ 4.8411e-05, -4.7401e-05, -1.1318e-05,  ..., -1.1162e-04,\n",
      "         -1.8658e-04,  3.1198e-04],\n",
      "        [-6.3895e-05,  4.0906e-05, -4.3542e-05,  ...,  1.6364e-04,\n",
      "          9.9840e-05,  8.6018e-05],\n",
      "        ...,\n",
      "        [ 2.6765e-04,  6.5638e-04, -2.8530e-04,  ...,  1.4034e-04,\n",
      "         -7.7058e-05, -1.1089e-04],\n",
      "        [-3.5166e-05, -6.3613e-04,  6.6631e-05,  ..., -4.2083e-04,\n",
      "          9.7449e-05,  2.4976e-04],\n",
      "        [ 2.4498e-04,  8.8482e-04,  1.2919e-04,  ...,  4.8031e-04,\n",
      "          3.4713e-04, -2.8134e-04]])\n",
      "local param grad is tensor([[-3.2056e-05,  1.6085e-04, -1.3644e-04,  ...,  7.9037e-05,\n",
      "         -1.0170e-04, -1.6183e-05],\n",
      "        [ 4.3769e-05, -4.7376e-05, -1.4447e-05,  ..., -7.9485e-05,\n",
      "         -1.3491e-04,  2.5492e-04],\n",
      "        [-6.2786e-05,  2.1664e-05, -2.3941e-05,  ...,  1.1086e-04,\n",
      "          6.8767e-05,  5.4610e-05],\n",
      "        ...,\n",
      "        [ 2.5309e-04,  5.9169e-04, -3.4887e-04,  ...,  1.1052e-04,\n",
      "         -5.9379e-05, -1.0719e-04],\n",
      "        [-3.3085e-05, -5.2358e-04,  1.5037e-04,  ..., -2.9079e-04,\n",
      "          3.3206e-05,  1.5356e-04],\n",
      "        [ 1.9367e-04,  8.3279e-04,  1.9479e-04,  ...,  5.4035e-04,\n",
      "          3.4017e-04, -2.3274e-04]])\n",
      "meta param grad is tensor([[ 8.6814e-05,  3.3746e-04, -8.8694e-04,  ...,  9.5722e-04,\n",
      "         -2.3947e-04, -5.3246e-04],\n",
      "        [ 2.7822e-04, -8.9413e-05,  1.7231e-04,  ..., -2.7690e-04,\n",
      "         -8.4627e-04,  7.5362e-04],\n",
      "        [-4.9257e-04, -4.9726e-04,  3.0877e-04,  ..., -7.4014e-04,\n",
      "         -1.7436e-04, -2.6637e-04],\n",
      "        ...,\n",
      "        [-2.4934e-03,  1.5508e-03,  3.0755e-03,  ...,  3.2470e-03,\n",
      "         -1.4518e-04, -6.0114e-04],\n",
      "        [ 2.6443e-04, -2.1142e-03, -2.5616e-03,  ..., -1.4422e-03,\n",
      "         -9.8603e-04, -1.8007e-04],\n",
      "        [-3.4040e-03,  1.7802e-03,  6.4860e-03,  ...,  4.2165e-03,\n",
      "          1.5599e-03, -5.0316e-04]])\n",
      "outer loop losses are [tensor(1.0659, grad_fn=<UnbindBackward0>), tensor(1.5612, grad_fn=<UnbindBackward0>), tensor(1.0938, grad_fn=<UnbindBackward0>), tensor(1.4818, grad_fn=<UnbindBackward0>), tensor(1.0847, grad_fn=<UnbindBackward0>), tensor(1.0489, grad_fn=<UnbindBackward0>), tensor(1.5265, grad_fn=<UnbindBackward0>), tensor(1.1612, grad_fn=<UnbindBackward0>), tensor(1.1887, grad_fn=<UnbindBackward0>), tensor(1.4979, grad_fn=<UnbindBackward0>), tensor(0.9770, grad_fn=<UnbindBackward0>), tensor(1.0090, grad_fn=<UnbindBackward0>)] and loss is 1.9859998226165771\n",
      "local param grad is tensor([[-3.7110e-05, -1.7016e-05, -2.4514e-05,  ..., -1.5841e-05,\n",
      "         -1.9040e-05, -4.5464e-06],\n",
      "        [-4.3687e-05,  8.4300e-05, -2.1697e-06,  ..., -8.3954e-06,\n",
      "         -4.9865e-06, -4.9555e-05],\n",
      "        [ 7.9552e-05, -1.2139e-04,  7.4582e-05,  ..., -3.0558e-05,\n",
      "          4.4883e-05,  7.9685e-05],\n",
      "        ...,\n",
      "        [-8.3337e-05, -5.3398e-05,  1.6246e-04,  ...,  5.6024e-05,\n",
      "          1.0241e-04, -2.9833e-05],\n",
      "        [-4.9615e-05, -1.9672e-05, -3.8959e-06,  ...,  2.5468e-05,\n",
      "         -8.6092e-05,  1.2900e-05],\n",
      "        [-2.3882e-04, -5.7071e-04,  8.6040e-04,  ...,  1.5068e-04,\n",
      "          3.0205e-04, -5.7496e-05]])\n",
      "local param grad is tensor([[-6.6788e-05, -3.0560e-05, -3.5794e-05,  ..., -2.4948e-05,\n",
      "         -3.3056e-05, -3.8842e-06],\n",
      "        [-7.5807e-05,  1.5787e-04,  3.2146e-06,  ..., -1.7322e-05,\n",
      "         -2.7740e-06, -9.2391e-05],\n",
      "        [ 1.4545e-04, -2.2186e-04,  1.3658e-04,  ..., -5.8768e-05,\n",
      "          9.3652e-05,  1.3761e-04],\n",
      "        ...,\n",
      "        [-1.4176e-04, -8.1961e-05,  2.7607e-04,  ...,  1.2897e-04,\n",
      "          1.8649e-04, -7.1744e-05],\n",
      "        [-6.4838e-05, -2.2337e-05,  2.8385e-06,  ...,  4.7221e-05,\n",
      "         -1.4989e-04,  4.5465e-05],\n",
      "        [-4.1751e-04, -9.9878e-04,  1.5177e-03,  ...,  2.9104e-04,\n",
      "          5.5500e-04, -1.0989e-04]])\n",
      "meta param grad is tensor([[-1.7085e-05,  2.8989e-04, -9.4725e-04,  ...,  9.1643e-04,\n",
      "         -2.9157e-04, -5.4090e-04],\n",
      "        [ 1.5872e-04,  1.5276e-04,  1.7335e-04,  ..., -3.0261e-04,\n",
      "         -8.5403e-04,  6.1168e-04],\n",
      "        [-2.6756e-04, -8.4051e-04,  5.1993e-04,  ..., -8.2947e-04,\n",
      "         -3.5822e-05, -4.9072e-05],\n",
      "        ...,\n",
      "        [-2.7185e-03,  1.4154e-03,  3.5140e-03,  ...,  3.4320e-03,\n",
      "          1.4371e-04, -7.0271e-04],\n",
      "        [ 1.4998e-04, -2.1562e-03, -2.5627e-03,  ..., -1.3695e-03,\n",
      "         -1.2220e-03, -1.2170e-04],\n",
      "        [-4.0603e-03,  2.1077e-04,  8.8641e-03,  ...,  4.6583e-03,\n",
      "          2.4170e-03, -6.7055e-04]])\n",
      "0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.0817, 1.0359, 0.9803, 0.9853, 1.1066, 1.0944],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.284253120422363\n",
      "losses are tensor([1.0149, 0.9670, 0.9098, 0.9251, 1.0496, 1.0346],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.901021957397461\n",
      "losses are tensor([0.9537, 0.9081, 0.8452, 0.8730, 0.9989, 0.9783],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.557217121124268\n",
      "losses are tensor([0.8983, 0.8546, 0.7864, 0.8251, 0.9524, 0.9261],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.242880344390869\n",
      "losses are tensor([0.8475, 0.8062, 0.7333, 0.7814, 0.9088, 0.8777],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.954805374145508\n",
      "losses are tensor([1.1083, 1.0172, 1.0451, 1.1996, 1.1244, 1.0980],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.5925612449646\n",
      "losses are tensor([1.0582, 0.9640, 0.9861, 1.1379, 1.0813, 1.0585],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.286011695861816\n",
      "losses are tensor([1.0111, 0.9152, 0.9335, 1.0825, 1.0404, 1.0218],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.004451751708984\n",
      "losses are tensor([0.9689, 0.8705, 0.8871, 1.0318, 1.0021, 0.9880],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.748411655426025\n",
      "losses are tensor([0.9304, 0.8293, 0.8449, 0.9854, 0.9668, 0.9572],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.513956546783447\n",
      "outer loop losses are [tensor(0.9541, grad_fn=<UnbindBackward0>), tensor(1.0769, grad_fn=<UnbindBackward0>), tensor(1.5064, grad_fn=<UnbindBackward0>), tensor(0.8961, grad_fn=<UnbindBackward0>), tensor(1.6421, grad_fn=<UnbindBackward0>), tensor(1.4413, grad_fn=<UnbindBackward0>), tensor(1.5340, grad_fn=<UnbindBackward0>), tensor(1.1358, grad_fn=<UnbindBackward0>), tensor(1.5938, grad_fn=<UnbindBackward0>), tensor(1.5439, grad_fn=<UnbindBackward0>), tensor(0.9701, grad_fn=<UnbindBackward0>), tensor(0.9114, grad_fn=<UnbindBackward0>)] and loss is 15.205985069274902\n",
      "local param grad is tensor([[ 1.1133e-04, -1.2153e-04, -1.0022e-04,  ..., -3.1230e-05,\n",
      "         -1.6112e-04,  1.3301e-04],\n",
      "        [ 9.5743e-05,  4.8529e-05,  8.8694e-05,  ..., -5.8444e-07,\n",
      "         -5.5669e-05,  1.0672e-04],\n",
      "        [-5.1713e-05,  6.1504e-05, -1.1120e-05,  ...,  4.3996e-05,\n",
      "         -1.9898e-04,  1.1913e-04],\n",
      "        ...,\n",
      "        [-2.0015e-04,  2.2980e-04,  1.0341e-04,  ...,  2.1347e-05,\n",
      "          2.0973e-04,  9.3664e-05],\n",
      "        [-1.4661e-05, -2.1619e-04, -1.9494e-04,  ..., -1.4843e-04,\n",
      "          5.5512e-04, -7.2958e-04],\n",
      "        [-3.7430e-04,  4.7651e-04,  4.2205e-04,  ..., -7.1893e-05,\n",
      "          7.7076e-05,  2.8973e-04]])\n",
      "local param grad is tensor([[ 8.5721e-05, -1.1683e-04, -1.1011e-04,  ..., -3.5019e-05,\n",
      "         -1.3897e-04,  1.0100e-04],\n",
      "        [ 7.4885e-05,  6.1025e-05,  7.9911e-05,  ..., -2.9614e-05,\n",
      "         -5.3252e-05,  7.9672e-05],\n",
      "        [-9.8244e-07,  9.7424e-05, -3.6872e-05,  ...,  2.8748e-05,\n",
      "         -2.0897e-04,  1.2337e-04],\n",
      "        ...,\n",
      "        [-2.7929e-04,  2.9016e-04,  2.5587e-04,  ...,  1.3333e-04,\n",
      "          2.8718e-04,  7.5599e-05],\n",
      "        [-1.2815e-04, -4.1829e-04, -2.0330e-04,  ..., -2.6132e-04,\n",
      "          8.2433e-04, -1.0367e-03],\n",
      "        [-5.9067e-04,  5.3401e-04,  8.1040e-04,  ...,  4.7874e-05,\n",
      "          2.5030e-04,  3.7047e-04]])\n",
      "meta param grad is tensor([[ 1.7997e-04,  5.1530e-05, -1.1576e-03,  ...,  8.5018e-04,\n",
      "         -5.9166e-04, -3.0688e-04],\n",
      "        [ 3.2935e-04,  2.6231e-04,  3.4196e-04,  ..., -3.3281e-04,\n",
      "         -9.6295e-04,  7.9806e-04],\n",
      "        [-3.2026e-04, -6.8158e-04,  4.7194e-04,  ..., -7.5672e-04,\n",
      "         -4.4377e-04,  1.9343e-04],\n",
      "        ...,\n",
      "        [-3.1980e-03,  1.9354e-03,  3.8733e-03,  ...,  3.5867e-03,\n",
      "          6.4062e-04, -5.3345e-04],\n",
      "        [ 7.1674e-06, -2.7907e-03, -2.9609e-03,  ..., -1.7793e-03,\n",
      "          1.5744e-04, -1.8880e-03],\n",
      "        [-5.0253e-03,  1.2213e-03,  1.0097e-02,  ...,  4.6342e-03,\n",
      "          2.7444e-03, -1.0342e-05]])\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1723, 1.0903, 1.1341, 0.9927, 1.1694, 1.1054],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.6641950607299805\n",
      "losses are tensor([1.1051, 1.0394, 1.0429, 0.9090, 1.1205, 1.0239],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.240835189819336\n",
      "losses are tensor([1.0520, 0.9976, 0.9705, 0.8443, 1.0805, 0.9591],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.904070854187012\n",
      "losses are tensor([1.0086, 0.9616, 0.9113, 0.7958, 1.0473, 0.9059],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.630494117736816\n",
      "losses are tensor([0.9711, 0.9298, 0.8613, 0.7551, 1.0188, 0.8627],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.398755073547363\n",
      "losses are tensor([1.1798, 1.1099, 1.1089, 1.1904, 1.1312, 1.1577],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.877835750579834\n",
      "losses are tensor([1.1243, 1.0320, 1.0588, 1.1286, 1.0574, 1.0879],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.4890618324279785\n",
      "losses are tensor([1.0762, 0.9670, 1.0158, 1.0767, 0.9938, 1.0284],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.15782356262207\n",
      "losses are tensor([1.0347, 0.9119, 0.9790, 1.0328, 0.9391, 0.9770],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.874541282653809\n",
      "losses are tensor([0.9984, 0.8656, 0.9472, 0.9953, 0.8921, 0.9332],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.631768703460693\n",
      "outer loop losses are [tensor(1.6077, grad_fn=<UnbindBackward0>), tensor(1.1616, grad_fn=<UnbindBackward0>), tensor(0.9924, grad_fn=<UnbindBackward0>), tensor(1.0497, grad_fn=<UnbindBackward0>), tensor(1.1868, grad_fn=<UnbindBackward0>), tensor(1.0728, grad_fn=<UnbindBackward0>)] and loss is 7.071017742156982\n",
      "local param grad is tensor([[ 2.2727e-05, -1.9157e-05, -6.5989e-06,  ..., -5.2643e-05,\n",
      "         -5.6007e-05, -1.2510e-05],\n",
      "        [ 9.4099e-07,  2.7260e-06, -2.8862e-06,  ..., -1.4838e-05,\n",
      "         -3.5066e-05,  2.3380e-05],\n",
      "        [-2.8277e-05, -1.1498e-05,  7.6552e-05,  ...,  7.6726e-05,\n",
      "         -6.4402e-06, -1.1516e-05],\n",
      "        ...,\n",
      "        [-1.5074e-04,  1.7309e-04,  2.2413e-04,  ...,  2.3271e-05,\n",
      "         -2.1905e-06, -3.0877e-05],\n",
      "        [ 1.2651e-04,  4.1140e-06, -8.2718e-05,  ..., -5.9768e-05,\n",
      "          2.6044e-05, -1.2361e-04],\n",
      "        [ 1.0911e-04,  9.2716e-05,  2.5428e-04,  ...,  3.2011e-05,\n",
      "         -2.1600e-04,  2.1476e-05]])\n",
      "local param grad is tensor([[ 1.3889e-05, -1.8096e-05, -1.0726e-05,  ..., -5.9454e-05,\n",
      "         -5.5519e-05, -2.2291e-06],\n",
      "        [ 4.1715e-06,  5.9481e-06, -1.8923e-06,  ..., -1.4961e-05,\n",
      "         -2.9020e-05,  2.4422e-05],\n",
      "        [-9.6159e-06, -2.1700e-06,  8.0529e-05,  ...,  7.6039e-05,\n",
      "          4.9246e-07, -1.2660e-05],\n",
      "        ...,\n",
      "        [-1.6442e-04,  1.8429e-04,  2.3145e-04,  ...,  6.3990e-05,\n",
      "         -1.2125e-05,  1.6655e-06],\n",
      "        [ 7.7911e-05,  3.5697e-05, -7.0365e-05,  ..., -3.4567e-05,\n",
      "          3.8391e-05, -1.2521e-04],\n",
      "        [ 9.2706e-05,  9.7728e-05,  2.4572e-04,  ...,  5.8262e-05,\n",
      "         -1.8578e-04,  1.0410e-05]])\n",
      "meta param grad is tensor([[ 2.1658e-04,  1.4277e-05, -1.1749e-03,  ...,  7.3808e-04,\n",
      "         -7.0319e-04, -3.2162e-04],\n",
      "        [ 3.3446e-04,  2.7099e-04,  3.3718e-04,  ..., -3.6261e-04,\n",
      "         -1.0270e-03,  8.4587e-04],\n",
      "        [-3.5815e-04, -6.9525e-04,  6.2902e-04,  ..., -6.0396e-04,\n",
      "         -4.4972e-04,  1.6925e-04],\n",
      "        ...,\n",
      "        [-3.5131e-03,  2.2928e-03,  4.3289e-03,  ...,  3.6740e-03,\n",
      "          6.2631e-04, -5.6266e-04],\n",
      "        [ 2.1159e-04, -2.7509e-03, -3.1140e-03,  ..., -1.8736e-03,\n",
      "          2.2188e-04, -2.1368e-03],\n",
      "        [-4.8234e-03,  1.4117e-03,  1.0597e-02,  ...,  4.7245e-03,\n",
      "          2.3426e-03,  2.1544e-05]])\n",
      "outer loop losses are [tensor(1.6077, grad_fn=<UnbindBackward0>), tensor(1.1616, grad_fn=<UnbindBackward0>), tensor(0.9924, grad_fn=<UnbindBackward0>), tensor(1.0497, grad_fn=<UnbindBackward0>), tensor(1.1868, grad_fn=<UnbindBackward0>), tensor(1.0728, grad_fn=<UnbindBackward0>), tensor(1.0948, grad_fn=<UnbindBackward0>), tensor(1.6968, grad_fn=<UnbindBackward0>), tensor(1.0586, grad_fn=<UnbindBackward0>), tensor(1.1100, grad_fn=<UnbindBackward0>), tensor(1.1308, grad_fn=<UnbindBackward0>), tensor(1.0680, grad_fn=<UnbindBackward0>)] and loss is 7.158920764923096\n",
      "local param grad is tensor([[ 4.8289e-05,  7.8254e-05, -1.4804e-04,  ..., -5.6377e-05,\n",
      "         -1.4958e-04,  2.6890e-05],\n",
      "        [-4.2573e-05, -4.2964e-05,  5.8656e-05,  ...,  1.7964e-05,\n",
      "          6.7371e-06, -3.3479e-05],\n",
      "        [ 1.4983e-04,  5.8490e-05,  1.1897e-04,  ...,  5.0840e-05,\n",
      "          3.1805e-05, -9.9376e-06],\n",
      "        ...,\n",
      "        [ 1.9095e-05,  1.1640e-04,  5.1696e-04,  ...,  2.3747e-04,\n",
      "          2.4861e-04,  1.1459e-04],\n",
      "        [ 3.5754e-05, -4.8170e-04,  8.8736e-05,  ...,  2.3637e-04,\n",
      "          1.6676e-04, -4.8344e-04],\n",
      "        [ 2.2343e-04,  6.1909e-05,  3.8539e-04,  ...,  1.9668e-04,\n",
      "          5.0495e-04,  4.9949e-04]])\n",
      "local param grad is tensor([[ 3.9243e-05,  6.8296e-05, -1.3393e-04,  ..., -4.6375e-05,\n",
      "         -1.3065e-04,  3.2753e-05],\n",
      "        [-3.6874e-05, -4.5443e-05,  5.0869e-05,  ...,  2.3981e-05,\n",
      "          1.0320e-05, -3.9237e-05],\n",
      "        [ 1.4167e-04,  5.1218e-05,  1.0981e-04,  ...,  4.0964e-05,\n",
      "          2.8435e-05, -1.2792e-05],\n",
      "        ...,\n",
      "        [ 3.3719e-05,  9.9395e-05,  4.5953e-04,  ...,  2.0602e-04,\n",
      "          2.1245e-04,  1.1292e-04],\n",
      "        [ 4.5912e-05, -4.4448e-04,  8.5984e-05,  ...,  2.1706e-04,\n",
      "          1.7047e-04, -4.5082e-04],\n",
      "        [ 2.1293e-04,  4.3502e-05,  3.2850e-04,  ...,  1.5924e-04,\n",
      "          4.3365e-04,  4.5546e-04]])\n",
      "meta param grad is tensor([[ 3.0411e-04,  1.6083e-04, -1.4569e-03,  ...,  6.3533e-04,\n",
      "         -9.8341e-04, -2.6198e-04],\n",
      "        [ 2.5502e-04,  1.8258e-04,  4.4671e-04,  ..., -3.2067e-04,\n",
      "         -1.0100e-03,  7.7315e-04],\n",
      "        [-6.6656e-05, -5.8554e-04,  8.5781e-04,  ..., -5.1215e-04,\n",
      "         -3.8948e-04,  1.4652e-04],\n",
      "        ...,\n",
      "        [-3.4603e-03,  2.5086e-03,  5.3054e-03,  ...,  4.1175e-03,\n",
      "          1.0874e-03, -3.3515e-04],\n",
      "        [ 2.9326e-04, -3.6771e-03, -2.9393e-03,  ..., -1.4202e-03,\n",
      "          5.5911e-04, -3.0711e-03],\n",
      "        [-4.3871e-03,  1.5171e-03,  1.1310e-02,  ...,  5.0804e-03,\n",
      "          3.2812e-03,  9.7650e-04]])\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.0785, 1.0458, 1.0438, 0.8840, 1.1449, 1.0687],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.265613079071045\n",
      "losses are tensor([0.9891, 0.9738, 0.9921, 0.8170, 1.0661, 0.9983],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.8363728523254395\n",
      "losses are tensor([0.9151, 0.9096, 0.9455, 0.7611, 1.0006, 0.9364],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.468307971954346\n",
      "losses are tensor([0.8516, 0.8521, 0.9024, 0.7136, 0.9450, 0.8804],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.145122528076172\n",
      "losses are tensor([0.7968, 0.7996, 0.8622, 0.6732, 0.8968, 0.8290],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.857547283172607\n",
      "losses are tensor([0.9713, 1.1028, 0.9567, 1.0795, 1.1544, 1.0684],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.333107948303223\n",
      "losses are tensor([0.8828, 1.0492, 0.8680, 1.0370, 1.0831, 1.0072],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.927303314208984\n",
      "losses are tensor([0.8073, 1.0003, 0.7919, 0.9987, 1.0201, 0.9529],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.571199417114258\n",
      "losses are tensor([0.7425, 0.9552, 0.7257, 0.9636, 0.9644, 0.9031],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.254456520080566\n",
      "losses are tensor([0.6865, 0.9138, 0.6684, 0.9317, 0.9150, 0.8580],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.97350549697876\n",
      "outer loop losses are [tensor(0.8905, grad_fn=<UnbindBackward0>), tensor(1.5275, grad_fn=<UnbindBackward0>), tensor(1.0178, grad_fn=<UnbindBackward0>), tensor(1.0590, grad_fn=<UnbindBackward0>), tensor(1.4652, grad_fn=<UnbindBackward0>)] and loss is 5.959999084472656\n",
      "local param grad is tensor([[-6.2510e-05,  3.5744e-05,  2.2489e-05,  ...,  4.2699e-05,\n",
      "         -6.1628e-06, -6.3998e-05],\n",
      "        [-2.1185e-04, -4.7760e-05,  3.5450e-05,  ..., -5.9375e-05,\n",
      "         -8.9085e-05, -3.0500e-04],\n",
      "        [ 3.5690e-05,  1.5920e-06, -1.5933e-04,  ..., -1.5180e-04,\n",
      "          1.9415e-04, -1.0875e-04],\n",
      "        ...,\n",
      "        [-3.8052e-04,  8.2793e-05,  5.1177e-04,  ...,  4.9916e-04,\n",
      "          2.8295e-04, -2.5682e-04],\n",
      "        [ 5.3533e-04, -4.5913e-05, -1.8690e-04,  ..., -5.7680e-04,\n",
      "         -1.3742e-04, -1.1252e-04],\n",
      "        [-4.0195e-04, -5.2013e-04,  8.6919e-04,  ...,  1.4215e-03,\n",
      "          3.4670e-04, -3.9793e-04]])\n",
      "local param grad is tensor([[-5.4536e-05,  1.7846e-05, -1.7463e-06,  ...,  1.3752e-05,\n",
      "         -2.6425e-06, -5.4518e-05],\n",
      "        [-1.8218e-04, -1.8076e-05,  1.4874e-05,  ..., -7.1673e-05,\n",
      "         -7.0636e-05, -2.9407e-04],\n",
      "        [ 2.2523e-05, -1.2950e-05, -1.3001e-04,  ..., -9.3811e-05,\n",
      "          1.5104e-04, -1.5777e-05],\n",
      "        ...,\n",
      "        [-3.5283e-04,  6.3749e-05,  5.1476e-04,  ...,  4.7378e-04,\n",
      "          2.5952e-04, -2.4600e-04],\n",
      "        [ 4.0107e-04,  1.3511e-05, -1.1883e-04,  ..., -4.0915e-04,\n",
      "         -1.2622e-04, -1.8512e-04],\n",
      "        [-3.3314e-04, -2.5495e-04,  7.5712e-04,  ...,  1.1900e-03,\n",
      "          2.5980e-04, -3.4757e-04]])\n",
      "meta param grad is tensor([[ 1.8707e-04,  2.1442e-04, -1.4361e-03,  ...,  6.9178e-04,\n",
      "         -9.9222e-04, -3.8050e-04],\n",
      "        [-1.3902e-04,  1.1675e-04,  4.9703e-04,  ..., -4.5171e-04,\n",
      "         -1.1697e-03,  1.7408e-04],\n",
      "        [-8.4426e-06, -5.9690e-04,  5.6846e-04,  ..., -7.5777e-04,\n",
      "         -4.4279e-05,  2.1993e-05],\n",
      "        ...,\n",
      "        [-4.1937e-03,  2.6551e-03,  6.3319e-03,  ...,  5.0904e-03,\n",
      "          1.6298e-03, -8.3798e-04],\n",
      "        [ 1.2297e-03, -3.7095e-03, -3.2450e-03,  ..., -2.4061e-03,\n",
      "          2.9547e-04, -3.3687e-03],\n",
      "        [-5.1222e-03,  7.4207e-04,  1.2937e-02,  ...,  7.6920e-03,\n",
      "          3.8877e-03,  2.3099e-04]])\n",
      "outer loop losses are [tensor(0.8905, grad_fn=<UnbindBackward0>), tensor(1.5275, grad_fn=<UnbindBackward0>), tensor(1.0178, grad_fn=<UnbindBackward0>), tensor(1.0590, grad_fn=<UnbindBackward0>), tensor(1.4652, grad_fn=<UnbindBackward0>), tensor(1.0742, grad_fn=<UnbindBackward0>), tensor(1.5800, grad_fn=<UnbindBackward0>), tensor(1.0712, grad_fn=<UnbindBackward0>), tensor(1.4156, grad_fn=<UnbindBackward0>), tensor(0.8547, grad_fn=<UnbindBackward0>), tensor(1.7064, grad_fn=<UnbindBackward0>), tensor(1.1185, grad_fn=<UnbindBackward0>)] and loss is 8.820630073547363\n",
      "local param grad is tensor([[ 3.2267e-05, -6.5740e-05,  2.1128e-04,  ..., -7.8042e-05,\n",
      "          2.6276e-05, -2.4430e-04],\n",
      "        [ 4.7680e-05,  9.6336e-05, -7.9613e-05,  ..., -2.2421e-04,\n",
      "         -1.1697e-04, -1.6669e-04],\n",
      "        [ 2.0479e-05,  6.4929e-05,  1.5929e-04,  ...,  6.4347e-05,\n",
      "         -9.3480e-05,  2.0001e-05],\n",
      "        ...,\n",
      "        [-4.7103e-04, -4.8732e-04,  8.9899e-04,  ...,  4.4185e-04,\n",
      "          4.5240e-04,  6.4884e-04],\n",
      "        [-3.0832e-04, -4.9195e-05,  6.2290e-04,  ...,  7.0069e-05,\n",
      "          3.4887e-04, -4.5107e-04],\n",
      "        [-7.3750e-04, -4.6049e-04,  1.3133e-03,  ...,  4.9305e-04,\n",
      "          1.8131e-04,  4.9080e-05]])\n",
      "local param grad is tensor([[ 5.7292e-05, -3.7402e-05,  1.5733e-04,  ..., -1.1025e-04,\n",
      "         -9.6378e-06, -2.5219e-04],\n",
      "        [ 4.2040e-05,  1.1664e-04, -1.3496e-04,  ..., -2.5710e-04,\n",
      "         -1.0303e-04, -1.7125e-04],\n",
      "        [ 3.8315e-05,  6.5880e-05,  1.9024e-04,  ...,  9.1047e-05,\n",
      "         -1.0594e-04,  8.0208e-05],\n",
      "        ...,\n",
      "        [-4.6840e-04, -5.4167e-04,  1.0467e-03,  ...,  5.6867e-04,\n",
      "          4.8678e-04,  6.9211e-04],\n",
      "        [-2.8140e-04, -1.2241e-04,  6.1556e-04,  ...,  2.0627e-04,\n",
      "          4.1158e-04, -2.8561e-04],\n",
      "        [-6.0819e-04, -4.8995e-04,  1.3673e-03,  ...,  5.3068e-04,\n",
      "          1.9266e-04,  1.6813e-04]])\n",
      "meta param grad is tensor([[ 2.7663e-04,  1.1128e-04, -1.0675e-03,  ...,  5.0349e-04,\n",
      "         -9.7558e-04, -8.7699e-04],\n",
      "        [-4.9301e-05,  3.2972e-04,  2.8246e-04,  ..., -9.3302e-04,\n",
      "         -1.3897e-03, -1.6386e-04],\n",
      "        [ 5.0351e-05, -4.6609e-04,  9.1799e-04,  ..., -6.0237e-04,\n",
      "         -2.4370e-04,  1.2220e-04],\n",
      "        ...,\n",
      "        [-5.1331e-03,  1.6261e-03,  8.2776e-03,  ...,  6.1009e-03,\n",
      "          2.5690e-03,  5.0298e-04],\n",
      "        [ 6.3994e-04, -3.8811e-03, -2.0066e-03,  ..., -2.1298e-03,\n",
      "          1.0559e-03, -4.1054e-03],\n",
      "        [-6.4679e-03, -2.0837e-04,  1.5617e-02,  ...,  8.7157e-03,\n",
      "          4.2617e-03,  4.4820e-04]])\n",
      "0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:410: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "pfomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3670cc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=2)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# val_protomaml_loader = data.DataLoader(\n",
    "#     None, batch_sampler=val_protomaml_sampler, num_workers=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b555c562",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0]\n",
      "[2, 3, 2, 4, 4, 3]\n",
      "[7, 5, 6, 5, 6, 7]\n",
      "[10, 8, 8, 9, 10, 9]\n",
      "[13, 12, 11, 12, 13, 11]\n",
      "[15, 14, 14, 15]\n",
      "[17, 16, 17, 16]\n",
      "[19, 18, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    batch = next(iter(val_protomaml_loader))\n",
    "#     print(batch)\n",
    "    for episode_i in range(len(batch[0])):\n",
    "        data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "        supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2]\n",
    "        print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
