{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5482b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "# cola = load_dataset('glue','cola')\n",
    "# # print(cola)\n",
    "# # sst2 = load_dataset('glue','sst2')\n",
    "# # print(sst2)\n",
    "# mrpc = load_dataset('glue', 'mrpc')\n",
    "# # print(mrpc)\n",
    "# pt_cola = GLUEDataset([cola, mrpc], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb68d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 339.66it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 38.59it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 491.19it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 53.50it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 16.66it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 420.68it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 188.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "meta_ds = GLUEMetaDataset(k=4,numTasks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d24829a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(meta_ds, kShot=4, batchSize=4)\n",
    "\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    meta_ds, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=4)\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3670cc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=2)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(val_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef372a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.validation_step(next(iter(val_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | metaLearner | PrototypeMetaModel | 108 M \n",
      "---------------------------------------------------\n",
      "7.3 M     Trainable params\n",
      "101 M     Non-trainable params\n",
      "108 M     Total params\n",
      "434.029   Total estimated model params size (MB)\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2988698482513428\n",
      "inner loop training loss is 1.2566895484924316\n",
      "inner loop training loss is 1.216719627380371\n",
      "inner loop training loss is 1.1787711381912231\n",
      "inner loop training loss is 1.1426889896392822\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.7598679065704346\n",
      "inner loop training loss is 3.676438331604004\n",
      "inner loop training loss is 3.6006839275360107\n",
      "inner loop training loss is 3.5310091972351074\n",
      "inner loop training loss is 3.4662742614746094\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.4543853998184204\n",
      "inner loop training loss is 3.3849124908447266\n",
      "inner loop training loss is 3.319882869720459\n",
      "inner loop training loss is 3.2585484981536865\n",
      "inner loop training loss is 3.2003565430641174\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.738795757293701\n",
      "inner loop training loss is 3.6768338680267334\n",
      "inner loop training loss is 3.619408130645752\n",
      "inner loop training loss is 3.565463066101074\n",
      "inner loop training loss is 3.5142712593078613\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.9257335662841797\n",
      "inner loop training loss is 3.8735568523406982\n",
      "inner loop training loss is 3.823704719543457\n",
      "inner loop training loss is 3.7756351232528687\n",
      "inner loop training loss is 3.728991150856018\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.827376365661621\n",
      "inner loop training loss is 1.809514045715332\n",
      "inner loop training loss is 1.7923640012741089\n",
      "inner loop training loss is 1.7757697105407715\n",
      "inner loop training loss is 1.759627342224121\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 0.7720471024513245\n",
      "inner loop training loss is 0.746883749961853\n",
      "inner loop training loss is 0.7231295108795166\n",
      "inner loop training loss is 0.7006722092628479\n",
      "inner loop training loss is 0.6794153451919556\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.356619119644165\n",
      "inner loop training loss is 1.3331997394561768\n",
      "inner loop training loss is 1.3103984594345093\n",
      "inner loop training loss is 1.288194179534912\n",
      "inner loop training loss is 1.2665624618530273\n",
      "validation accuracy is 0.425\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.683499574661255\n",
      "inner loop training loss is 11.339662075042725\n",
      "inner loop training loss is 10.290467739105225\n",
      "inner loop training loss is 10.483100652694702\n",
      "inner loop training loss is 12.004337549209595\n",
      "outer loop training loss is 13.211946487426758\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.429452061653137\n",
      "inner loop training loss is 4.691464185714722\n",
      "inner loop training loss is 4.447746276855469\n",
      "inner loop training loss is 4.903918266296387\n",
      "inner loop training loss is 4.733074426651001\n",
      "outer loop training loss is 6.072249889373779\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.803644895553589\n",
      "inner loop training loss is 4.975503444671631\n",
      "inner loop training loss is 5.432434320449829\n",
      "inner loop training loss is 5.054335594177246\n",
      "inner loop training loss is 4.974179029464722\n",
      "outer loop training loss is 5.323497533798218\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.398473262786865\n",
      "inner loop training loss is 5.037761926651001\n",
      "inner loop training loss is 4.3623340129852295\n",
      "inner loop training loss is 4.6275634765625\n",
      "inner loop training loss is 4.6429784297943115\n",
      "outer loop training loss is 5.749014377593994\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.652139782905579\n",
      "inner loop training loss is 5.379281520843506\n",
      "inner loop training loss is 4.33525550365448\n",
      "inner loop training loss is 4.178326606750488\n",
      "inner loop training loss is 4.430331468582153\n",
      "outer loop training loss is 5.390393018722534\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.729658365249634\n",
      "inner loop training loss is 5.610666513442993\n",
      "inner loop training loss is 5.861936807632446\n",
      "inner loop training loss is 5.302731513977051\n",
      "inner loop training loss is 5.549757957458496\n",
      "outer loop training loss is 5.499351501464844\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.988881349563599\n",
      "inner loop training loss is 4.813971519470215\n",
      "inner loop training loss is 4.971235275268555\n",
      "inner loop training loss is 4.815669059753418\n",
      "inner loop training loss is 4.747719764709473\n",
      "outer loop training loss is 5.802187919616699\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.640033721923828\n",
      "inner loop training loss is 5.257380962371826\n",
      "inner loop training loss is 4.2542572021484375\n",
      "inner loop training loss is 4.576797008514404\n",
      "inner loop training loss is 3.9816609621047974\n",
      "outer loop training loss is 5.610120058059692\n",
      "outer loop accuracy is 0.4264705882352941 and total loss is tensor(52.6588, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2469303607940674\n",
      "inner loop training loss is 1.2074127197265625\n",
      "inner loop training loss is 1.16989266872406\n",
      "inner loop training loss is 1.1342215538024902\n",
      "inner loop training loss is 1.1002651453018188\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.5955933332443237\n",
      "inner loop training loss is 3.51630437374115\n",
      "inner loop training loss is 3.444008469581604\n",
      "inner loop training loss is 3.3772873878479004\n",
      "inner loop training loss is 3.315114378929138\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.200300455093384\n",
      "inner loop training loss is 3.1354746222496033\n",
      "inner loop training loss is 3.0746588706970215\n",
      "inner loop training loss is 3.017196297645569\n",
      "inner loop training loss is 2.962587893009186\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.4860527515411377\n",
      "inner loop training loss is 3.4268521070480347\n",
      "inner loop training loss is 3.371910572052002\n",
      "inner loop training loss is 3.3202033042907715\n",
      "inner loop training loss is 3.271028995513916\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.7459752559661865\n",
      "inner loop training loss is 3.69519305229187\n",
      "inner loop training loss is 3.646373152732849\n",
      "inner loop training loss is 3.599114418029785\n",
      "inner loop training loss is 3.553141474723816\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.7156462669372559\n",
      "inner loop training loss is 1.6973183155059814\n",
      "inner loop training loss is 1.6797717809677124\n",
      "inner loop training loss is 1.6628341674804688\n",
      "inner loop training loss is 1.6463795900344849\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 0.6450011730194092\n",
      "inner loop training loss is 0.6252300143241882\n",
      "inner loop training loss is 0.6065553426742554\n",
      "inner loop training loss is 0.5888813138008118\n",
      "inner loop training loss is 0.5721284747123718\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2761704921722412\n",
      "inner loop training loss is 1.253936767578125\n",
      "inner loop training loss is 1.2323026657104492\n",
      "inner loop training loss is 1.2112399339675903\n",
      "inner loop training loss is 1.1907275915145874\n",
      "validation accuracy is 0.4\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.476161241531372\n",
      "inner loop training loss is 4.87105917930603\n",
      "inner loop training loss is 5.086706161499023\n",
      "inner loop training loss is 4.791152238845825\n",
      "inner loop training loss is 5.630542755126953\n",
      "outer loop training loss is 5.243102788925171\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.349510669708252\n",
      "inner loop training loss is 4.524515867233276\n",
      "inner loop training loss is 4.4114603996276855\n",
      "inner loop training loss is 4.30955696105957\n",
      "inner loop training loss is 3.629737615585327\n",
      "outer loop training loss is 5.857463121414185\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.234057784080505\n",
      "inner loop training loss is 4.370404958724976\n",
      "inner loop training loss is 4.2340264320373535\n",
      "inner loop training loss is 3.836336851119995\n",
      "inner loop training loss is 4.175197124481201\n",
      "outer loop training loss is 5.781840085983276\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.95070481300354\n",
      "inner loop training loss is 4.8179885149002075\n",
      "inner loop training loss is 5.125739336013794\n",
      "inner loop training loss is 4.5510172843933105\n",
      "inner loop training loss is 3.72477650642395\n",
      "outer loop training loss is 5.410911798477173\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.812628746032715\n",
      "inner loop training loss is 4.481604814529419\n",
      "inner loop training loss is 4.721234321594238\n",
      "inner loop training loss is 5.0525243282318115\n",
      "inner loop training loss is 4.995052337646484\n",
      "outer loop training loss is 5.8674139976501465\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.527875065803528\n",
      "inner loop training loss is 3.9898715019226074\n",
      "inner loop training loss is 3.9019150733947754\n",
      "inner loop training loss is 4.325295448303223\n",
      "inner loop training loss is 3.9929189682006836\n",
      "outer loop training loss is 5.405960321426392\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.768359661102295\n",
      "inner loop training loss is 10.822601556777954\n",
      "inner loop training loss is 10.560270071029663\n",
      "inner loop training loss is 10.242841720581055\n",
      "inner loop training loss is 10.534809350967407\n",
      "outer loop training loss is 15.008273124694824\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.319753885269165\n",
      "inner loop training loss is 4.428442358970642\n",
      "inner loop training loss is 4.3415515422821045\n",
      "inner loop training loss is 5.510629415512085\n",
      "inner loop training loss is 4.267253041267395\n",
      "outer loop training loss is 6.070504903793335\n",
      "outer loop accuracy is 0.4411764705882353 and total loss is tensor(54.6455, grad_fn=<AddBackward0>)\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.028429627418518\n",
      "inner loop training loss is 1.0081865787506104\n",
      "inner loop training loss is 0.988562285900116\n",
      "inner loop training loss is 0.9695266485214233\n",
      "inner loop training loss is 0.9510517120361328\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.6403943300247192\n",
      "inner loop training loss is 3.562764883041382\n",
      "inner loop training loss is 3.4907102584838867\n",
      "inner loop training loss is 3.42275607585907\n",
      "inner loop training loss is 3.357975482940674\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 2.998501420021057\n",
      "inner loop training loss is 2.9469597339630127\n",
      "inner loop training loss is 2.8974756002426147\n",
      "inner loop training loss is 2.849853515625\n",
      "inner loop training loss is 2.803937554359436\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.863306760787964\n",
      "inner loop training loss is 3.805917501449585\n",
      "inner loop training loss is 3.7513047456741333\n",
      "inner loop training loss is 3.6989948749542236\n",
      "inner loop training loss is 3.64863920211792\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.616746664047241\n",
      "inner loop training loss is 3.5658934116363525\n",
      "inner loop training loss is 3.5168282985687256\n",
      "inner loop training loss is 3.469273090362549\n",
      "inner loop training loss is 3.4230337142944336\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2877099514007568\n",
      "inner loop training loss is 1.2680495977401733\n",
      "inner loop training loss is 1.249045729637146\n",
      "inner loop training loss is 1.2305896282196045\n",
      "inner loop training loss is 1.2126049995422363\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2389620542526245\n",
      "inner loop training loss is 1.2110604047775269\n",
      "inner loop training loss is 1.1849253177642822\n",
      "inner loop training loss is 1.1601743698120117\n",
      "inner loop training loss is 1.136572241783142\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 0.814674437046051\n",
      "inner loop training loss is 0.7877331376075745\n",
      "inner loop training loss is 0.7623416185379028\n",
      "inner loop training loss is 0.7383719086647034\n",
      "inner loop training loss is 0.7157106399536133\n",
      "validation accuracy is 0.475\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.622603416442871\n",
      "inner loop training loss is 4.0549410581588745\n",
      "inner loop training loss is 4.016633152961731\n",
      "inner loop training loss is 4.4292755126953125\n",
      "inner loop training loss is 4.270373344421387\n",
      "outer loop training loss is 5.15065598487854\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.46609115600586\n",
      "inner loop training loss is 10.391382217407227\n",
      "inner loop training loss is 10.392466306686401\n",
      "inner loop training loss is 10.469560384750366\n",
      "inner loop training loss is 10.014195680618286\n",
      "outer loop training loss is 14.77957010269165\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.196491956710815\n",
      "inner loop training loss is 4.235554933547974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 3.9011387825012207\n",
      "inner loop training loss is 4.169217467308044\n",
      "inner loop training loss is 3.828054428100586\n",
      "outer loop training loss is 6.151514768600464\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.667069673538208\n",
      "inner loop training loss is 10.606245040893555\n",
      "inner loop training loss is 11.573100805282593\n",
      "inner loop training loss is 10.444779872894287\n",
      "inner loop training loss is 10.387210845947266\n",
      "outer loop training loss is 14.1695556640625\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.283499002456665\n",
      "inner loop training loss is 4.101740717887878\n",
      "inner loop training loss is 3.360090136528015\n",
      "inner loop training loss is 3.5504798889160156\n",
      "inner loop training loss is 3.091010332107544\n",
      "outer loop training loss is 4.92769718170166\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.689795017242432\n",
      "inner loop training loss is 4.995008230209351\n",
      "inner loop training loss is 5.065510272979736\n",
      "inner loop training loss is 5.309258460998535\n",
      "inner loop training loss is 4.188013195991516\n",
      "outer loop training loss is 5.473833084106445\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.202985167503357\n",
      "inner loop training loss is 4.532850027084351\n",
      "inner loop training loss is 4.684619426727295\n",
      "inner loop training loss is 4.443675756454468\n",
      "inner loop training loss is 4.62445855140686\n",
      "outer loop training loss is 5.26926851272583\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.001964569091797\n",
      "inner loop training loss is 10.827708721160889\n",
      "inner loop training loss is 10.80146050453186\n",
      "inner loop training loss is 11.125419855117798\n",
      "inner loop training loss is 10.913941144943237\n",
      "outer loop training loss is 12.450791358947754\n",
      "outer loop accuracy is 0.42105263157894735 and total loss is tensor(68.3729, grad_fn=<AddBackward0>)\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.0508198738098145\n",
      "inner loop training loss is 1.0250115394592285\n",
      "inner loop training loss is 1.0003175735473633\n",
      "inner loop training loss is 0.976591944694519\n",
      "inner loop training loss is 0.9537421464920044\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.668006658554077\n",
      "inner loop training loss is 3.5753417015075684\n",
      "inner loop training loss is 3.4888535737991333\n",
      "inner loop training loss is 3.407504916191101\n",
      "inner loop training loss is 3.3305517435073853\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.2407915592193604\n",
      "inner loop training loss is 3.1834710836410522\n",
      "inner loop training loss is 3.128934144973755\n",
      "inner loop training loss is 3.0764883756637573\n",
      "inner loop training loss is 3.025696039199829\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.0356156826019287\n",
      "inner loop training loss is 2.9831387996673584\n",
      "inner loop training loss is 2.9346535205841064\n",
      "inner loop training loss is 2.889096736907959\n",
      "inner loop training loss is 2.8458251953125\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 3.9348546266555786\n",
      "inner loop training loss is 3.881793975830078\n",
      "inner loop training loss is 3.833343029022217\n",
      "inner loop training loss is 3.788048505783081\n",
      "inner loop training loss is 3.744984745979309\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 1.2663321495056152\n",
      "inner loop training loss is 1.2305457592010498\n",
      "inner loop training loss is 1.197797179222107\n",
      "inner loop training loss is 1.167202353477478\n",
      "inner loop training loss is 1.1383439302444458\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 0.5941641330718994\n",
      "inner loop training loss is 0.5373457670211792\n",
      "inner loop training loss is 0.4938626289367676\n",
      "inner loop training loss is 0.45885732769966125\n",
      "inner loop training loss is 0.42949798703193665\n"
     ]
    }
   ],
   "source": [
    "from training.trainer import train_model\n",
    "\n",
    "protomaml_model = train_model(\n",
    "    ProtoFOMAML,\n",
    "    train_loader=train_protomaml_loader,\n",
    "    val_loader=val_protomaml_loader,\n",
    "    outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=4, warmupSteps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104cb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
