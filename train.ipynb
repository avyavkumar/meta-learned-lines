{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5482b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 786.73it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 714.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "cola = load_dataset('glue','cola')\n",
    "# print(cola)\n",
    "# sst2 = load_dataset('glue','sst2')\n",
    "# print(sst2)\n",
    "mrpc = load_dataset('glue', 'mrpc')\n",
    "# print(mrpc)\n",
    "pt_cola = GLUEDataset([cola, mrpc], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb229056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(pt_cola, kShot=4, nWay=4, batchSize=8, shuffle=True)\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    pt_cola, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=4)\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3670cc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=2)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd5e4d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.validation_step(next(iter(val_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | metaLearner | PrototypeMetaModel | 108 M \n",
      "---------------------------------------------------\n",
      "28.5 M    Trainable params\n",
      "80.0 M    Non-trainable params\n",
      "108 M     Total params\n",
      "434.029   Total estimated model params size (MB)\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 7.814536094665527\n",
      "inner loop training loss is 6.339726448059082\n",
      "inner loop training loss is 8.375788688659668\n",
      "inner loop training loss is 7.571019649505615\n",
      "outer loop losses are [tensor(0.4439, grad_fn=<UnbindBackward0>), tensor(0.8937, grad_fn=<UnbindBackward0>), tensor(0.9641, grad_fn=<UnbindBackward0>), tensor(1.3359, grad_fn=<UnbindBackward0>), tensor(1.6016, grad_fn=<UnbindBackward0>), tensor(1.5502, grad_fn=<UnbindBackward0>), tensor(1.8715, grad_fn=<UnbindBackward0>), tensor(1.7682, grad_fn=<UnbindBackward0>), tensor(1.4481, grad_fn=<UnbindBackward0>)] and loss is 11.877238273620605\n",
      "outer loop losses are [tensor(0.4439, grad_fn=<UnbindBackward0>), tensor(0.8937, grad_fn=<UnbindBackward0>), tensor(0.9641, grad_fn=<UnbindBackward0>), tensor(1.3359, grad_fn=<UnbindBackward0>), tensor(1.6016, grad_fn=<UnbindBackward0>), tensor(1.5502, grad_fn=<UnbindBackward0>), tensor(1.8715, grad_fn=<UnbindBackward0>), tensor(1.7682, grad_fn=<UnbindBackward0>), tensor(1.4481, grad_fn=<UnbindBackward0>), tensor(1.3244, grad_fn=<UnbindBackward0>), tensor(1.3927, grad_fn=<UnbindBackward0>), tensor(1.5594, grad_fn=<UnbindBackward0>), tensor(1.0703, grad_fn=<UnbindBackward0>), tensor(1.4868, grad_fn=<UnbindBackward0>), tensor(1.4491, grad_fn=<UnbindBackward0>), tensor(1.8795, grad_fn=<UnbindBackward0>)] and loss is 10.16238784790039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 7.290179252624512\n",
      "inner loop training loss is 6.441580772399902\n",
      "inner loop training loss is 8.439443588256836\n",
      "inner loop training loss is 6.5053887367248535\n",
      "outer loop losses are [tensor(0.6826, grad_fn=<UnbindBackward0>), tensor(0.7970, grad_fn=<UnbindBackward0>), tensor(1.5267, grad_fn=<UnbindBackward0>), tensor(1.6516, grad_fn=<UnbindBackward0>), tensor(1.5395, grad_fn=<UnbindBackward0>), tensor(1.2794, grad_fn=<UnbindBackward0>)] and loss is 7.4768757820129395\n",
      "outer loop losses are [tensor(0.6826, grad_fn=<UnbindBackward0>), tensor(0.7970, grad_fn=<UnbindBackward0>), tensor(1.5267, grad_fn=<UnbindBackward0>), tensor(1.6516, grad_fn=<UnbindBackward0>), tensor(1.5395, grad_fn=<UnbindBackward0>), tensor(1.2794, grad_fn=<UnbindBackward0>), tensor(1.9092, grad_fn=<UnbindBackward0>), tensor(1.2700, grad_fn=<UnbindBackward0>), tensor(1.1583, grad_fn=<UnbindBackward0>), tensor(1.3126, grad_fn=<UnbindBackward0>), tensor(0.8755, grad_fn=<UnbindBackward0>), tensor(1.3247, grad_fn=<UnbindBackward0>), tensor(0.5475, grad_fn=<UnbindBackward0>), tensor(0.9547, grad_fn=<UnbindBackward0>), tensor(1.5610, grad_fn=<UnbindBackward0>), tensor(0.9749, grad_fn=<UnbindBackward0>)] and loss is 11.888306617736816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 7.532393932342529\n",
      "inner loop training loss is 6.138989448547363\n",
      "inner loop training loss is 6.597479820251465\n",
      "inner loop training loss is 6.194498538970947\n",
      "outer loop losses are [tensor(1.2902, grad_fn=<UnbindBackward0>), tensor(1.1480, grad_fn=<UnbindBackward0>), tensor(0.8863, grad_fn=<UnbindBackward0>), tensor(0.7710, grad_fn=<UnbindBackward0>), tensor(0.7245, grad_fn=<UnbindBackward0>), tensor(0.6721, grad_fn=<UnbindBackward0>), tensor(1.1576, grad_fn=<UnbindBackward0>)] and loss is 6.649644374847412\n",
      "outer loop losses are [tensor(1.2902, grad_fn=<UnbindBackward0>), tensor(1.1480, grad_fn=<UnbindBackward0>), tensor(0.8863, grad_fn=<UnbindBackward0>), tensor(0.7710, grad_fn=<UnbindBackward0>), tensor(0.7245, grad_fn=<UnbindBackward0>), tensor(0.6721, grad_fn=<UnbindBackward0>), tensor(1.1576, grad_fn=<UnbindBackward0>), tensor(1.6240, grad_fn=<UnbindBackward0>), tensor(0.9472, grad_fn=<UnbindBackward0>), tensor(1.1056, grad_fn=<UnbindBackward0>), tensor(0.9108, grad_fn=<UnbindBackward0>), tensor(0.7938, grad_fn=<UnbindBackward0>), tensor(0.7696, grad_fn=<UnbindBackward0>), tensor(0.6957, grad_fn=<UnbindBackward0>), tensor(0.8960, grad_fn=<UnbindBackward0>), tensor(1.2017, grad_fn=<UnbindBackward0>)] and loss is 8.944357872009277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 7.2612104415893555\n",
      "inner loop training loss is 4.972771167755127\n",
      "inner loop training loss is 7.236823558807373\n",
      "inner loop training loss is 6.237913608551025\n",
      "outer loop losses are [tensor(1.6759, grad_fn=<UnbindBackward0>), tensor(1.4615, grad_fn=<UnbindBackward0>), tensor(1.7248, grad_fn=<UnbindBackward0>), tensor(1.1017, grad_fn=<UnbindBackward0>), tensor(1.4889, grad_fn=<UnbindBackward0>), tensor(1.7468, grad_fn=<UnbindBackward0>), tensor(0.8218, grad_fn=<UnbindBackward0>), tensor(1.0567, grad_fn=<UnbindBackward0>)] and loss is 11.078216552734375\n",
      "outer loop losses are [tensor(1.6759, grad_fn=<UnbindBackward0>), tensor(1.4615, grad_fn=<UnbindBackward0>), tensor(1.7248, grad_fn=<UnbindBackward0>), tensor(1.1017, grad_fn=<UnbindBackward0>), tensor(1.4889, grad_fn=<UnbindBackward0>), tensor(1.7468, grad_fn=<UnbindBackward0>), tensor(0.8218, grad_fn=<UnbindBackward0>), tensor(1.0567, grad_fn=<UnbindBackward0>), tensor(1.7023, grad_fn=<UnbindBackward0>), tensor(1.0227, grad_fn=<UnbindBackward0>), tensor(1.2922, grad_fn=<UnbindBackward0>), tensor(1.3687, grad_fn=<UnbindBackward0>), tensor(2.0292, grad_fn=<UnbindBackward0>), tensor(1.9927, grad_fn=<UnbindBackward0>), tensor(0.8718, grad_fn=<UnbindBackward0>), tensor(1.1451, grad_fn=<UnbindBackward0>)] and loss is 11.424617767333984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 8.3507080078125\n",
      "inner loop training loss is 6.473556995391846\n",
      "inner loop training loss is 6.661890029907227\n",
      "inner loop training loss is 6.22280216217041\n",
      "outer loop losses are [tensor(1.5328, grad_fn=<UnbindBackward0>), tensor(1.4020, grad_fn=<UnbindBackward0>), tensor(1.4067, grad_fn=<UnbindBackward0>), tensor(0.9413, grad_fn=<UnbindBackward0>), tensor(1.1731, grad_fn=<UnbindBackward0>), tensor(1.2645, grad_fn=<UnbindBackward0>), tensor(1.2925, grad_fn=<UnbindBackward0>), tensor(1.7954, grad_fn=<UnbindBackward0>), tensor(1.4418, grad_fn=<UnbindBackward0>)] and loss is 12.250015258789062\n",
      "outer loop losses are [tensor(1.5328, grad_fn=<UnbindBackward0>), tensor(1.4020, grad_fn=<UnbindBackward0>), tensor(1.4067, grad_fn=<UnbindBackward0>), tensor(0.9413, grad_fn=<UnbindBackward0>), tensor(1.1731, grad_fn=<UnbindBackward0>), tensor(1.2645, grad_fn=<UnbindBackward0>), tensor(1.2925, grad_fn=<UnbindBackward0>), tensor(1.7954, grad_fn=<UnbindBackward0>), tensor(1.4418, grad_fn=<UnbindBackward0>), tensor(1.3944, grad_fn=<UnbindBackward0>), tensor(1.5340, grad_fn=<UnbindBackward0>), tensor(1.2728, grad_fn=<UnbindBackward0>), tensor(1.2693, grad_fn=<UnbindBackward0>), tensor(1.5514, grad_fn=<UnbindBackward0>), tensor(1.7282, grad_fn=<UnbindBackward0>), tensor(1.0037, grad_fn=<UnbindBackward0>)] and loss is 9.753741264343262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 8.740747451782227\n",
      "inner loop training loss is 6.400721549987793\n",
      "inner loop training loss is 7.692292213439941\n",
      "inner loop training loss is 6.155643463134766\n",
      "outer loop losses are [tensor(1.1963, grad_fn=<UnbindBackward0>), tensor(0.7469, grad_fn=<UnbindBackward0>), tensor(1.4318, grad_fn=<UnbindBackward0>), tensor(1.0061, grad_fn=<UnbindBackward0>), tensor(0.9640, grad_fn=<UnbindBackward0>)] and loss is 5.34511661529541\n",
      "outer loop losses are [tensor(1.1963, grad_fn=<UnbindBackward0>), tensor(0.7469, grad_fn=<UnbindBackward0>), tensor(1.4318, grad_fn=<UnbindBackward0>), tensor(1.0061, grad_fn=<UnbindBackward0>), tensor(0.9640, grad_fn=<UnbindBackward0>), tensor(0.7405, grad_fn=<UnbindBackward0>), tensor(0.7460, grad_fn=<UnbindBackward0>), tensor(0.8358, grad_fn=<UnbindBackward0>), tensor(0.8149, grad_fn=<UnbindBackward0>), tensor(1.0017, grad_fn=<UnbindBackward0>), tensor(1.8875, grad_fn=<UnbindBackward0>), tensor(1.3121, grad_fn=<UnbindBackward0>), tensor(0.8263, grad_fn=<UnbindBackward0>), tensor(1.3635, grad_fn=<UnbindBackward0>), tensor(0.8320, grad_fn=<UnbindBackward0>), tensor(1.6957, grad_fn=<UnbindBackward0>)] and loss is 12.056058883666992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 7.639782428741455\n",
      "inner loop training loss is 7.293095588684082\n",
      "inner loop training loss is 7.335538864135742\n",
      "inner loop training loss is 6.361055850982666\n",
      "outer loop losses are [tensor(0.9392, grad_fn=<UnbindBackward0>), tensor(0.8565, grad_fn=<UnbindBackward0>), tensor(0.8440, grad_fn=<UnbindBackward0>), tensor(2.1402, grad_fn=<UnbindBackward0>), tensor(1.0838, grad_fn=<UnbindBackward0>), tensor(1.5263, grad_fn=<UnbindBackward0>), tensor(1.2818, grad_fn=<UnbindBackward0>), tensor(1.2176, grad_fn=<UnbindBackward0>), tensor(1.7435, grad_fn=<UnbindBackward0>), tensor(1.1926, grad_fn=<UnbindBackward0>)] and loss is 12.825444221496582\n",
      "outer loop losses are [tensor(0.9392, grad_fn=<UnbindBackward0>), tensor(0.8565, grad_fn=<UnbindBackward0>), tensor(0.8440, grad_fn=<UnbindBackward0>), tensor(2.1402, grad_fn=<UnbindBackward0>), tensor(1.0838, grad_fn=<UnbindBackward0>), tensor(1.5263, grad_fn=<UnbindBackward0>), tensor(1.2818, grad_fn=<UnbindBackward0>), tensor(1.2176, grad_fn=<UnbindBackward0>), tensor(1.7435, grad_fn=<UnbindBackward0>), tensor(1.1926, grad_fn=<UnbindBackward0>), tensor(1.1691, grad_fn=<UnbindBackward0>), tensor(1.6644, grad_fn=<UnbindBackward0>), tensor(1.3185, grad_fn=<UnbindBackward0>), tensor(2.1940, grad_fn=<UnbindBackward0>), tensor(0.7360, grad_fn=<UnbindBackward0>), tensor(1.3401, grad_fn=<UnbindBackward0>)] and loss is 8.422239303588867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 8.005010604858398\n",
      "inner loop training loss is 6.393675327301025\n",
      "inner loop training loss is 7.046948432922363\n",
      "inner loop training loss is 5.487839698791504\n",
      "outer loop losses are [tensor(1.7588, grad_fn=<UnbindBackward0>), tensor(1.0159, grad_fn=<UnbindBackward0>), tensor(0.9210, grad_fn=<UnbindBackward0>), tensor(1.5942, grad_fn=<UnbindBackward0>), tensor(1.9072, grad_fn=<UnbindBackward0>), tensor(1.1291, grad_fn=<UnbindBackward0>), tensor(1.0952, grad_fn=<UnbindBackward0>), tensor(1.0907, grad_fn=<UnbindBackward0>), tensor(1.3081, grad_fn=<UnbindBackward0>)] and loss is 11.820279121398926\n",
      "outer loop losses are [tensor(1.7588, grad_fn=<UnbindBackward0>), tensor(1.0159, grad_fn=<UnbindBackward0>), tensor(0.9210, grad_fn=<UnbindBackward0>), tensor(1.5942, grad_fn=<UnbindBackward0>), tensor(1.9072, grad_fn=<UnbindBackward0>), tensor(1.1291, grad_fn=<UnbindBackward0>), tensor(1.0952, grad_fn=<UnbindBackward0>), tensor(1.0907, grad_fn=<UnbindBackward0>), tensor(1.3081, grad_fn=<UnbindBackward0>), tensor(0.6805, grad_fn=<UnbindBackward0>), tensor(1.6638, grad_fn=<UnbindBackward0>), tensor(0.8412, grad_fn=<UnbindBackward0>), tensor(0.9448, grad_fn=<UnbindBackward0>), tensor(2.1470, grad_fn=<UnbindBackward0>), tensor(2.1045, grad_fn=<UnbindBackward0>), tensor(0.9616, grad_fn=<UnbindBackward0>)] and loss is 9.343488693237305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='val_acc')` could not find the monitored key in the returned metrics: ['inner_loop_training_loss', 'inner_loop_training_accuracy', 'outer_loop_training_accuracy', 'outer_loop_training_loss', 'epoch', 'step']. HINT: Did you call `log('val_acc', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 3.306030511856079\n",
      "inner loop training loss is 1.1866931915283203\n",
      "inner loop training loss is 2.974766254425049\n",
      "inner loop training loss is 1.382756233215332\n",
      "outer loop losses are [tensor(0.0540, grad_fn=<UnbindBackward0>), tensor(2.7895, grad_fn=<UnbindBackward0>), tensor(0.9212, grad_fn=<UnbindBackward0>), tensor(0.1010, grad_fn=<UnbindBackward0>), tensor(0.4794, grad_fn=<UnbindBackward0>), tensor(1.6935, grad_fn=<UnbindBackward0>), tensor(0.1416, grad_fn=<UnbindBackward0>), tensor(0.8026, grad_fn=<UnbindBackward0>), tensor(0.5992, grad_fn=<UnbindBackward0>), tensor(2.3426, grad_fn=<UnbindBackward0>)] and loss is 9.924542427062988\n",
      "outer loop losses are [tensor(0.0540, grad_fn=<UnbindBackward0>), tensor(2.7895, grad_fn=<UnbindBackward0>), tensor(0.9212, grad_fn=<UnbindBackward0>), tensor(0.1010, grad_fn=<UnbindBackward0>), tensor(0.4794, grad_fn=<UnbindBackward0>), tensor(1.6935, grad_fn=<UnbindBackward0>), tensor(0.1416, grad_fn=<UnbindBackward0>), tensor(0.8026, grad_fn=<UnbindBackward0>), tensor(0.5992, grad_fn=<UnbindBackward0>), tensor(2.3426, grad_fn=<UnbindBackward0>), tensor(0.2970, grad_fn=<UnbindBackward0>), tensor(1.1708, grad_fn=<UnbindBackward0>), tensor(0.2261, grad_fn=<UnbindBackward0>), tensor(3.3891, grad_fn=<UnbindBackward0>), tensor(1.4337, grad_fn=<UnbindBackward0>), tensor(4.0507, grad_fn=<UnbindBackward0>)] and loss is 10.567336082458496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 20.550539016723633\n",
      "inner loop training loss is 1.2576614618301392\n",
      "inner loop training loss is 1.4252432584762573\n",
      "inner loop training loss is 0.5869701504707336\n",
      "outer loop losses are [tensor(0.0424, grad_fn=<UnbindBackward0>), tensor(5.5961, grad_fn=<UnbindBackward0>), tensor(0.0377, grad_fn=<UnbindBackward0>), tensor(2.4570, grad_fn=<UnbindBackward0>), tensor(5.5394, grad_fn=<UnbindBackward0>), tensor(0.0705, grad_fn=<UnbindBackward0>), tensor(1.0638, grad_fn=<UnbindBackward0>)] and loss is 14.806829452514648\n",
      "outer loop losses are [tensor(0.0424, grad_fn=<UnbindBackward0>), tensor(5.5961, grad_fn=<UnbindBackward0>), tensor(0.0377, grad_fn=<UnbindBackward0>), tensor(2.4570, grad_fn=<UnbindBackward0>), tensor(5.5394, grad_fn=<UnbindBackward0>), tensor(0.0705, grad_fn=<UnbindBackward0>), tensor(1.0638, grad_fn=<UnbindBackward0>), tensor(5.9783, grad_fn=<UnbindBackward0>), tensor(0.0020, grad_fn=<UnbindBackward0>), tensor(0.1664, grad_fn=<UnbindBackward0>), tensor(0.0926, grad_fn=<UnbindBackward0>), tensor(1.8454, grad_fn=<UnbindBackward0>), tensor(0.0108, grad_fn=<UnbindBackward0>), tensor(0.0093, grad_fn=<UnbindBackward0>), tensor(0.1362, grad_fn=<UnbindBackward0>), tensor(1.6143, grad_fn=<UnbindBackward0>)] and loss is 9.855328559875488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 2.780585289001465\n",
      "inner loop training loss is 2.4530742168426514\n",
      "inner loop training loss is 3.9466352462768555\n",
      "inner loop training loss is 2.715121030807495\n",
      "outer loop losses are [tensor(0.1376, grad_fn=<UnbindBackward0>), tensor(2.1741, grad_fn=<UnbindBackward0>), tensor(0.5200, grad_fn=<UnbindBackward0>), tensor(0.4515, grad_fn=<UnbindBackward0>), tensor(0.6721, grad_fn=<UnbindBackward0>), tensor(1.0415, grad_fn=<UnbindBackward0>), tensor(3.0134, grad_fn=<UnbindBackward0>)] and loss is 8.010254859924316\n",
      "outer loop losses are [tensor(0.1376, grad_fn=<UnbindBackward0>), tensor(2.1741, grad_fn=<UnbindBackward0>), tensor(0.5200, grad_fn=<UnbindBackward0>), tensor(0.4515, grad_fn=<UnbindBackward0>), tensor(0.6721, grad_fn=<UnbindBackward0>), tensor(1.0415, grad_fn=<UnbindBackward0>), tensor(3.0134, grad_fn=<UnbindBackward0>), tensor(0.7153, grad_fn=<UnbindBackward0>), tensor(0.8373, grad_fn=<UnbindBackward0>), tensor(0.0142, grad_fn=<UnbindBackward0>), tensor(0.9242, grad_fn=<UnbindBackward0>), tensor(0.4551, grad_fn=<UnbindBackward0>), tensor(0.1873, grad_fn=<UnbindBackward0>), tensor(0.8053, grad_fn=<UnbindBackward0>), tensor(1.0260, grad_fn=<UnbindBackward0>), tensor(0.2926, grad_fn=<UnbindBackward0>)] and loss is 5.257234573364258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 3.6391611099243164\n",
      "inner loop training loss is 1.1164443492889404\n",
      "inner loop training loss is 3.222484827041626\n",
      "inner loop training loss is 1.9178435802459717\n",
      "outer loop losses are [tensor(0.4683, grad_fn=<UnbindBackward0>), tensor(1.9378, grad_fn=<UnbindBackward0>), tensor(0.0184, grad_fn=<UnbindBackward0>), tensor(1.1514, grad_fn=<UnbindBackward0>), tensor(1.2289, grad_fn=<UnbindBackward0>), tensor(3.6635, grad_fn=<UnbindBackward0>), tensor(1.0918, grad_fn=<UnbindBackward0>), tensor(0.0367, grad_fn=<UnbindBackward0>)] and loss is 9.596772193908691\n",
      "outer loop losses are [tensor(0.4683, grad_fn=<UnbindBackward0>), tensor(1.9378, grad_fn=<UnbindBackward0>), tensor(0.0184, grad_fn=<UnbindBackward0>), tensor(1.1514, grad_fn=<UnbindBackward0>), tensor(1.2289, grad_fn=<UnbindBackward0>), tensor(3.6635, grad_fn=<UnbindBackward0>), tensor(1.0918, grad_fn=<UnbindBackward0>), tensor(0.0367, grad_fn=<UnbindBackward0>), tensor(0.1601, grad_fn=<UnbindBackward0>), tensor(0.2675, grad_fn=<UnbindBackward0>), tensor(1.6342, grad_fn=<UnbindBackward0>), tensor(1.3510, grad_fn=<UnbindBackward0>), tensor(0.1777, grad_fn=<UnbindBackward0>), tensor(0.2409, grad_fn=<UnbindBackward0>), tensor(1.3679, grad_fn=<UnbindBackward0>), tensor(1.3685, grad_fn=<UnbindBackward0>)] and loss is 6.56796932220459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 2.5784730911254883\n",
      "inner loop training loss is 1.6946817636489868\n",
      "inner loop training loss is 2.689823865890503\n",
      "inner loop training loss is 1.4605998992919922\n",
      "outer loop losses are [tensor(3.8655, grad_fn=<UnbindBackward0>), tensor(1.9513, grad_fn=<UnbindBackward0>), tensor(0.1422, grad_fn=<UnbindBackward0>), tensor(3.0499, grad_fn=<UnbindBackward0>), tensor(0.0263, grad_fn=<UnbindBackward0>), tensor(2.3039, grad_fn=<UnbindBackward0>), tensor(0.0325, grad_fn=<UnbindBackward0>), tensor(0.1248, grad_fn=<UnbindBackward0>)] and loss is 11.496334075927734\n",
      "outer loop losses are [tensor(3.8655, grad_fn=<UnbindBackward0>), tensor(1.9513, grad_fn=<UnbindBackward0>), tensor(0.1422, grad_fn=<UnbindBackward0>), tensor(3.0499, grad_fn=<UnbindBackward0>), tensor(0.0263, grad_fn=<UnbindBackward0>), tensor(2.3039, grad_fn=<UnbindBackward0>), tensor(0.0325, grad_fn=<UnbindBackward0>), tensor(0.1248, grad_fn=<UnbindBackward0>), tensor(0.1948, grad_fn=<UnbindBackward0>), tensor(0.8231, grad_fn=<UnbindBackward0>), tensor(0.4297, grad_fn=<UnbindBackward0>), tensor(1.1246, grad_fn=<UnbindBackward0>), tensor(0.3558, grad_fn=<UnbindBackward0>), tensor(1.0170, grad_fn=<UnbindBackward0>), tensor(4.6141, grad_fn=<UnbindBackward0>), tensor(1.1609, grad_fn=<UnbindBackward0>)] and loss is 9.71989917755127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 2.738774299621582\n",
      "inner loop training loss is 1.3432363271713257\n",
      "inner loop training loss is 2.881110191345215\n",
      "inner loop training loss is 1.5916589498519897\n",
      "outer loop losses are [tensor(0.9574, grad_fn=<UnbindBackward0>), tensor(1.0307, grad_fn=<UnbindBackward0>), tensor(0.2353, grad_fn=<UnbindBackward0>), tensor(36.0052, grad_fn=<UnbindBackward0>), tensor(0.3496, grad_fn=<UnbindBackward0>), tensor(0.2720, grad_fn=<UnbindBackward0>), tensor(0.1910, grad_fn=<UnbindBackward0>), tensor(0.2907, grad_fn=<UnbindBackward0>), tensor(0.9850, grad_fn=<UnbindBackward0>), tensor(1.0049, grad_fn=<UnbindBackward0>), tensor(0.1059, grad_fn=<UnbindBackward0>)] and loss is 41.427757263183594\n",
      "outer loop losses are [tensor(0.9574, grad_fn=<UnbindBackward0>), tensor(1.0307, grad_fn=<UnbindBackward0>), tensor(0.2353, grad_fn=<UnbindBackward0>), tensor(36.0052, grad_fn=<UnbindBackward0>), tensor(0.3496, grad_fn=<UnbindBackward0>), tensor(0.2720, grad_fn=<UnbindBackward0>), tensor(0.1910, grad_fn=<UnbindBackward0>), tensor(0.2907, grad_fn=<UnbindBackward0>), tensor(0.9850, grad_fn=<UnbindBackward0>), tensor(1.0049, grad_fn=<UnbindBackward0>), tensor(0.1059, grad_fn=<UnbindBackward0>), tensor(1.7051, grad_fn=<UnbindBackward0>), tensor(0.2496, grad_fn=<UnbindBackward0>), tensor(0.8554, grad_fn=<UnbindBackward0>), tensor(0.2015, grad_fn=<UnbindBackward0>), tensor(0.1011, grad_fn=<UnbindBackward0>)] and loss is 3.112614870071411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 3.6931962966918945\n",
      "inner loop training loss is 3.288109064102173\n",
      "inner loop training loss is 4.898181438446045\n",
      "inner loop training loss is 3.1376113891601562\n",
      "outer loop losses are [tensor(0.2256, grad_fn=<UnbindBackward0>), tensor(1.4613, grad_fn=<UnbindBackward0>), tensor(1.5422, grad_fn=<UnbindBackward0>), tensor(0.3448, grad_fn=<UnbindBackward0>), tensor(0.0905, grad_fn=<UnbindBackward0>), tensor(1.8591, grad_fn=<UnbindBackward0>), tensor(0.2444, grad_fn=<UnbindBackward0>)] and loss is 5.76801872253418\n",
      "outer loop losses are [tensor(0.2256, grad_fn=<UnbindBackward0>), tensor(1.4613, grad_fn=<UnbindBackward0>), tensor(1.5422, grad_fn=<UnbindBackward0>), tensor(0.3448, grad_fn=<UnbindBackward0>), tensor(0.0905, grad_fn=<UnbindBackward0>), tensor(1.8591, grad_fn=<UnbindBackward0>), tensor(0.2444, grad_fn=<UnbindBackward0>), tensor(0.8709, grad_fn=<UnbindBackward0>), tensor(0.5800, grad_fn=<UnbindBackward0>), tensor(0.4373, grad_fn=<UnbindBackward0>), tensor(0.4225, grad_fn=<UnbindBackward0>), tensor(0.3774, grad_fn=<UnbindBackward0>), tensor(0.1410, grad_fn=<UnbindBackward0>), tensor(0.4372, grad_fn=<UnbindBackward0>), tensor(0.6742, grad_fn=<UnbindBackward0>), tensor(0.8295, grad_fn=<UnbindBackward0>)] and loss is 4.77003288269043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 17.620685577392578\n",
      "inner loop training loss is 4.190965175628662\n",
      "inner loop training loss is 7.022798538208008\n",
      "inner loop training loss is 1.0577987432479858\n",
      "outer loop losses are [tensor(2.2755, grad_fn=<UnbindBackward0>), tensor(0.7809, grad_fn=<UnbindBackward0>), tensor(0.8071, grad_fn=<UnbindBackward0>), tensor(1.4350, grad_fn=<UnbindBackward0>), tensor(0.5492, grad_fn=<UnbindBackward0>), tensor(0.0725, grad_fn=<UnbindBackward0>), tensor(0.0703, grad_fn=<UnbindBackward0>), tensor(0.8628, grad_fn=<UnbindBackward0>), tensor(0.8860, grad_fn=<UnbindBackward0>), tensor(0.8282, grad_fn=<UnbindBackward0>), tensor(1.8371, grad_fn=<UnbindBackward0>)] and loss is 10.40449333190918\n",
      "outer loop losses are [tensor(2.2755, grad_fn=<UnbindBackward0>), tensor(0.7809, grad_fn=<UnbindBackward0>), tensor(0.8071, grad_fn=<UnbindBackward0>), tensor(1.4350, grad_fn=<UnbindBackward0>), tensor(0.5492, grad_fn=<UnbindBackward0>), tensor(0.0725, grad_fn=<UnbindBackward0>), tensor(0.0703, grad_fn=<UnbindBackward0>), tensor(0.8628, grad_fn=<UnbindBackward0>), tensor(0.8860, grad_fn=<UnbindBackward0>), tensor(0.8282, grad_fn=<UnbindBackward0>), tensor(1.8371, grad_fn=<UnbindBackward0>), tensor(1.4493, grad_fn=<UnbindBackward0>), tensor(4.0740, grad_fn=<UnbindBackward0>), tensor(1.4812, grad_fn=<UnbindBackward0>), tensor(1.1179, grad_fn=<UnbindBackward0>), tensor(0.2827, grad_fn=<UnbindBackward0>)] and loss is 8.405060768127441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.113068103790283\n",
      "inner loop training loss is 3.33506441116333\n",
      "inner loop training loss is 3.9162163734436035\n",
      "inner loop training loss is 3.0187196731567383\n",
      "outer loop losses are [tensor(1.4001, grad_fn=<UnbindBackward0>), tensor(1.8156, grad_fn=<UnbindBackward0>), tensor(1.1918, grad_fn=<UnbindBackward0>), tensor(0.9500, grad_fn=<UnbindBackward0>), tensor(0.3639, grad_fn=<UnbindBackward0>), tensor(0.8402, grad_fn=<UnbindBackward0>), tensor(0.3285, grad_fn=<UnbindBackward0>), tensor(0.4762, grad_fn=<UnbindBackward0>)] and loss is 7.366283893585205\n",
      "outer loop losses are [tensor(1.4001, grad_fn=<UnbindBackward0>), tensor(1.8156, grad_fn=<UnbindBackward0>), tensor(1.1918, grad_fn=<UnbindBackward0>), tensor(0.9500, grad_fn=<UnbindBackward0>), tensor(0.3639, grad_fn=<UnbindBackward0>), tensor(0.8402, grad_fn=<UnbindBackward0>), tensor(0.3285, grad_fn=<UnbindBackward0>), tensor(0.4762, grad_fn=<UnbindBackward0>), tensor(1.0284, grad_fn=<UnbindBackward0>), tensor(0.4459, grad_fn=<UnbindBackward0>), tensor(0.3716, grad_fn=<UnbindBackward0>), tensor(0.4675, grad_fn=<UnbindBackward0>), tensor(0.4019, grad_fn=<UnbindBackward0>), tensor(0.4848, grad_fn=<UnbindBackward0>), tensor(1.0150, grad_fn=<UnbindBackward0>), tensor(1.1332, grad_fn=<UnbindBackward0>)] and loss is 5.3483171463012695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.016768455505371\n",
      "inner loop training loss is 3.216505527496338\n",
      "inner loop training loss is 4.28672981262207\n",
      "inner loop training loss is 3.4468374252319336\n",
      "outer loop losses are [tensor(0.4525, grad_fn=<UnbindBackward0>), tensor(0.4627, grad_fn=<UnbindBackward0>), tensor(0.8381, grad_fn=<UnbindBackward0>), tensor(0.9727, grad_fn=<UnbindBackward0>), tensor(1.0453, grad_fn=<UnbindBackward0>), tensor(0.5686, grad_fn=<UnbindBackward0>), tensor(1.0047, grad_fn=<UnbindBackward0>), tensor(1.1383, grad_fn=<UnbindBackward0>), tensor(0.4236, grad_fn=<UnbindBackward0>), tensor(0.8631, grad_fn=<UnbindBackward0>), tensor(1.0470, grad_fn=<UnbindBackward0>)] and loss is 8.816751480102539\n",
      "outer loop losses are [tensor(0.4525, grad_fn=<UnbindBackward0>), tensor(0.4627, grad_fn=<UnbindBackward0>), tensor(0.8381, grad_fn=<UnbindBackward0>), tensor(0.9727, grad_fn=<UnbindBackward0>), tensor(1.0453, grad_fn=<UnbindBackward0>), tensor(0.5686, grad_fn=<UnbindBackward0>), tensor(1.0047, grad_fn=<UnbindBackward0>), tensor(1.1383, grad_fn=<UnbindBackward0>), tensor(0.4236, grad_fn=<UnbindBackward0>), tensor(0.8631, grad_fn=<UnbindBackward0>), tensor(1.0470, grad_fn=<UnbindBackward0>), tensor(1.0651, grad_fn=<UnbindBackward0>), tensor(0.5387, grad_fn=<UnbindBackward0>), tensor(1.2906, grad_fn=<UnbindBackward0>), tensor(1.0483, grad_fn=<UnbindBackward0>), tensor(0.8133, grad_fn=<UnbindBackward0>)] and loss is 4.755845546722412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.668692111968994\n",
      "inner loop training loss is 4.489593982696533\n",
      "inner loop training loss is 3.8396146297454834\n",
      "inner loop training loss is 3.98193621635437\n",
      "outer loop losses are [tensor(0.7133, grad_fn=<UnbindBackward0>), tensor(0.7385, grad_fn=<UnbindBackward0>), tensor(0.7667, grad_fn=<UnbindBackward0>), tensor(0.8649, grad_fn=<UnbindBackward0>), tensor(0.6331, grad_fn=<UnbindBackward0>), tensor(4.2477, grad_fn=<UnbindBackward0>), tensor(0.7698, grad_fn=<UnbindBackward0>), tensor(0.6078, grad_fn=<UnbindBackward0>)] and loss is 9.341832160949707\n",
      "outer loop losses are [tensor(0.7133, grad_fn=<UnbindBackward0>), tensor(0.7385, grad_fn=<UnbindBackward0>), tensor(0.7667, grad_fn=<UnbindBackward0>), tensor(0.8649, grad_fn=<UnbindBackward0>), tensor(0.6331, grad_fn=<UnbindBackward0>), tensor(4.2477, grad_fn=<UnbindBackward0>), tensor(0.7698, grad_fn=<UnbindBackward0>), tensor(0.6078, grad_fn=<UnbindBackward0>), tensor(0.6573, grad_fn=<UnbindBackward0>), tensor(0.5575, grad_fn=<UnbindBackward0>), tensor(0.4356, grad_fn=<UnbindBackward0>), tensor(1.1746, grad_fn=<UnbindBackward0>), tensor(0.3799, grad_fn=<UnbindBackward0>), tensor(0.6014, grad_fn=<UnbindBackward0>), tensor(0.6426, grad_fn=<UnbindBackward0>), tensor(0.8572, grad_fn=<UnbindBackward0>)] and loss is 5.306149482727051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 3.891749858856201\n",
      "inner loop training loss is 3.8361928462982178\n",
      "inner loop training loss is 4.288435459136963\n",
      "inner loop training loss is 4.021636009216309\n",
      "outer loop losses are [tensor(0.7733, grad_fn=<UnbindBackward0>), tensor(0.5689, grad_fn=<UnbindBackward0>), tensor(0.7588, grad_fn=<UnbindBackward0>), tensor(0.5051, grad_fn=<UnbindBackward0>), tensor(0.8007, grad_fn=<UnbindBackward0>), tensor(1.1469, grad_fn=<UnbindBackward0>), tensor(0.7615, grad_fn=<UnbindBackward0>), tensor(0.6643, grad_fn=<UnbindBackward0>)] and loss is 5.979495048522949\n",
      "outer loop losses are [tensor(0.7733, grad_fn=<UnbindBackward0>), tensor(0.5689, grad_fn=<UnbindBackward0>), tensor(0.7588, grad_fn=<UnbindBackward0>), tensor(0.5051, grad_fn=<UnbindBackward0>), tensor(0.8007, grad_fn=<UnbindBackward0>), tensor(1.1469, grad_fn=<UnbindBackward0>), tensor(0.7615, grad_fn=<UnbindBackward0>), tensor(0.6643, grad_fn=<UnbindBackward0>), tensor(0.4191, grad_fn=<UnbindBackward0>), tensor(0.7591, grad_fn=<UnbindBackward0>), tensor(1.5892, grad_fn=<UnbindBackward0>), tensor(0.9744, grad_fn=<UnbindBackward0>), tensor(0.5853, grad_fn=<UnbindBackward0>), tensor(1.0134, grad_fn=<UnbindBackward0>), tensor(0.6447, grad_fn=<UnbindBackward0>), tensor(0.8311, grad_fn=<UnbindBackward0>)] and loss is 6.81638765335083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.417733192443848\n",
      "inner loop training loss is 3.0375986099243164\n",
      "inner loop training loss is 3.9856643676757812\n",
      "inner loop training loss is 3.0272409915924072\n",
      "outer loop losses are [tensor(1.2403, grad_fn=<UnbindBackward0>), tensor(0.6960, grad_fn=<UnbindBackward0>), tensor(0.8396, grad_fn=<UnbindBackward0>), tensor(0.4268, grad_fn=<UnbindBackward0>), tensor(1.1926, grad_fn=<UnbindBackward0>), tensor(1.0382, grad_fn=<UnbindBackward0>), tensor(0.6925, grad_fn=<UnbindBackward0>), tensor(0.6124, grad_fn=<UnbindBackward0>), tensor(0.5207, grad_fn=<UnbindBackward0>)] and loss is 7.259275913238525\n",
      "outer loop losses are [tensor(1.2403, grad_fn=<UnbindBackward0>), tensor(0.6960, grad_fn=<UnbindBackward0>), tensor(0.8396, grad_fn=<UnbindBackward0>), tensor(0.4268, grad_fn=<UnbindBackward0>), tensor(1.1926, grad_fn=<UnbindBackward0>), tensor(1.0382, grad_fn=<UnbindBackward0>), tensor(0.6925, grad_fn=<UnbindBackward0>), tensor(0.6124, grad_fn=<UnbindBackward0>), tensor(0.5207, grad_fn=<UnbindBackward0>), tensor(0.3917, grad_fn=<UnbindBackward0>), tensor(0.4776, grad_fn=<UnbindBackward0>), tensor(0.7373, grad_fn=<UnbindBackward0>), tensor(1.5019, grad_fn=<UnbindBackward0>), tensor(0.8227, grad_fn=<UnbindBackward0>), tensor(0.2411, grad_fn=<UnbindBackward0>), tensor(2.2240, grad_fn=<UnbindBackward0>)] and loss is 6.396252632141113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.917942523956299\n",
      "inner loop training loss is 5.127142429351807\n",
      "inner loop training loss is 4.143039226531982\n",
      "inner loop training loss is 3.497042655944824\n",
      "outer loop losses are [tensor(0.6471, grad_fn=<UnbindBackward0>), tensor(0.9858, grad_fn=<UnbindBackward0>), tensor(0.7470, grad_fn=<UnbindBackward0>), tensor(0.9554, grad_fn=<UnbindBackward0>), tensor(0.7231, grad_fn=<UnbindBackward0>), tensor(0.9570, grad_fn=<UnbindBackward0>), tensor(0.5142, grad_fn=<UnbindBackward0>)] and loss is 5.529560089111328\n",
      "outer loop losses are [tensor(0.6471, grad_fn=<UnbindBackward0>), tensor(0.9858, grad_fn=<UnbindBackward0>), tensor(0.7470, grad_fn=<UnbindBackward0>), tensor(0.9554, grad_fn=<UnbindBackward0>), tensor(0.7231, grad_fn=<UnbindBackward0>), tensor(0.9570, grad_fn=<UnbindBackward0>), tensor(0.5142, grad_fn=<UnbindBackward0>), tensor(0.7559, grad_fn=<UnbindBackward0>), tensor(0.6646, grad_fn=<UnbindBackward0>), tensor(0.8204, grad_fn=<UnbindBackward0>), tensor(0.8910, grad_fn=<UnbindBackward0>), tensor(0.8532, grad_fn=<UnbindBackward0>), tensor(0.7029, grad_fn=<UnbindBackward0>), tensor(0.6160, grad_fn=<UnbindBackward0>), tensor(0.7130, grad_fn=<UnbindBackward0>), tensor(0.5147, grad_fn=<UnbindBackward0>)] and loss is 6.531555652618408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.294049263000488\n",
      "inner loop training loss is 3.732762336730957\n",
      "inner loop training loss is 5.2520012855529785\n",
      "inner loop training loss is 3.2857768535614014\n",
      "outer loop losses are [tensor(0.9032, grad_fn=<UnbindBackward0>), tensor(0.9272, grad_fn=<UnbindBackward0>), tensor(1.0629, grad_fn=<UnbindBackward0>), tensor(0.5712, grad_fn=<UnbindBackward0>), tensor(0.7112, grad_fn=<UnbindBackward0>), tensor(0.8573, grad_fn=<UnbindBackward0>), tensor(0.6095, grad_fn=<UnbindBackward0>), tensor(0.4727, grad_fn=<UnbindBackward0>), tensor(1.1009, grad_fn=<UnbindBackward0>), tensor(0.6803, grad_fn=<UnbindBackward0>), tensor(0.9668, grad_fn=<UnbindBackward0>), tensor(0.9995, grad_fn=<UnbindBackward0>), tensor(0.7468, grad_fn=<UnbindBackward0>), tensor(0.8111, grad_fn=<UnbindBackward0>)] and loss is 11.420404434204102\n",
      "outer loop losses are [tensor(0.9032, grad_fn=<UnbindBackward0>), tensor(0.9272, grad_fn=<UnbindBackward0>), tensor(1.0629, grad_fn=<UnbindBackward0>), tensor(0.5712, grad_fn=<UnbindBackward0>), tensor(0.7112, grad_fn=<UnbindBackward0>), tensor(0.8573, grad_fn=<UnbindBackward0>), tensor(0.6095, grad_fn=<UnbindBackward0>), tensor(0.4727, grad_fn=<UnbindBackward0>), tensor(1.1009, grad_fn=<UnbindBackward0>), tensor(0.6803, grad_fn=<UnbindBackward0>), tensor(0.9668, grad_fn=<UnbindBackward0>), tensor(0.9995, grad_fn=<UnbindBackward0>), tensor(0.7468, grad_fn=<UnbindBackward0>), tensor(0.8111, grad_fn=<UnbindBackward0>), tensor(1.4262, grad_fn=<UnbindBackward0>), tensor(1.1042, grad_fn=<UnbindBackward0>)] and loss is 2.5304479598999023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.246643543243408\n",
      "inner loop training loss is 4.233194351196289\n",
      "inner loop training loss is 3.8621773719787598\n",
      "inner loop training loss is 3.812945604324341\n",
      "outer loop losses are [tensor(0.3548, grad_fn=<UnbindBackward0>), tensor(0.4322, grad_fn=<UnbindBackward0>), tensor(0.5631, grad_fn=<UnbindBackward0>), tensor(0.9590, grad_fn=<UnbindBackward0>), tensor(1.0826, grad_fn=<UnbindBackward0>), tensor(0.9288, grad_fn=<UnbindBackward0>), tensor(1.0203, grad_fn=<UnbindBackward0>), tensor(0.5116, grad_fn=<UnbindBackward0>), tensor(0.8327, grad_fn=<UnbindBackward0>)] and loss is 6.6850690841674805\n",
      "outer loop losses are [tensor(0.3548, grad_fn=<UnbindBackward0>), tensor(0.4322, grad_fn=<UnbindBackward0>), tensor(0.5631, grad_fn=<UnbindBackward0>), tensor(0.9590, grad_fn=<UnbindBackward0>), tensor(1.0826, grad_fn=<UnbindBackward0>), tensor(0.9288, grad_fn=<UnbindBackward0>), tensor(1.0203, grad_fn=<UnbindBackward0>), tensor(0.5116, grad_fn=<UnbindBackward0>), tensor(0.8327, grad_fn=<UnbindBackward0>), tensor(0.4787, grad_fn=<UnbindBackward0>), tensor(1.1728, grad_fn=<UnbindBackward0>), tensor(1.0980, grad_fn=<UnbindBackward0>), tensor(0.7636, grad_fn=<UnbindBackward0>), tensor(0.5738, grad_fn=<UnbindBackward0>), tensor(0.8995, grad_fn=<UnbindBackward0>), tensor(0.6913, grad_fn=<UnbindBackward0>)] and loss is 5.677780628204346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.66544771194458\n",
      "inner loop training loss is 4.453804016113281\n",
      "inner loop training loss is 5.539395332336426\n",
      "inner loop training loss is 5.0413594245910645\n",
      "outer loop losses are [tensor(0.9392, grad_fn=<UnbindBackward0>), tensor(0.6662, grad_fn=<UnbindBackward0>), tensor(0.7664, grad_fn=<UnbindBackward0>), tensor(0.7670, grad_fn=<UnbindBackward0>), tensor(0.8481, grad_fn=<UnbindBackward0>), tensor(0.8144, grad_fn=<UnbindBackward0>)] and loss is 4.8012518882751465\n",
      "outer loop losses are [tensor(0.9392, grad_fn=<UnbindBackward0>), tensor(0.6662, grad_fn=<UnbindBackward0>), tensor(0.7664, grad_fn=<UnbindBackward0>), tensor(0.7670, grad_fn=<UnbindBackward0>), tensor(0.8481, grad_fn=<UnbindBackward0>), tensor(0.8144, grad_fn=<UnbindBackward0>), tensor(0.9894, grad_fn=<UnbindBackward0>), tensor(0.7136, grad_fn=<UnbindBackward0>), tensor(0.6820, grad_fn=<UnbindBackward0>), tensor(0.7000, grad_fn=<UnbindBackward0>), tensor(0.5805, grad_fn=<UnbindBackward0>), tensor(1.0458, grad_fn=<UnbindBackward0>), tensor(0.7497, grad_fn=<UnbindBackward0>), tensor(1.8930, grad_fn=<UnbindBackward0>), tensor(0.8393, grad_fn=<UnbindBackward0>), tensor(0.7565, grad_fn=<UnbindBackward0>)] and loss is 8.949875831604004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.212070941925049\n",
      "inner loop training loss is 4.3297858238220215\n",
      "inner loop training loss is 5.099668979644775\n",
      "inner loop training loss is 4.33537483215332\n",
      "outer loop losses are [tensor(1.1994, grad_fn=<UnbindBackward0>), tensor(0.6217, grad_fn=<UnbindBackward0>), tensor(0.9360, grad_fn=<UnbindBackward0>), tensor(0.7144, grad_fn=<UnbindBackward0>), tensor(0.8611, grad_fn=<UnbindBackward0>), tensor(0.6348, grad_fn=<UnbindBackward0>), tensor(1.0107, grad_fn=<UnbindBackward0>)] and loss is 5.978172779083252\n",
      "outer loop losses are [tensor(1.1994, grad_fn=<UnbindBackward0>), tensor(0.6217, grad_fn=<UnbindBackward0>), tensor(0.9360, grad_fn=<UnbindBackward0>), tensor(0.7144, grad_fn=<UnbindBackward0>), tensor(0.8611, grad_fn=<UnbindBackward0>), tensor(0.6348, grad_fn=<UnbindBackward0>), tensor(1.0107, grad_fn=<UnbindBackward0>), tensor(0.3223, grad_fn=<UnbindBackward0>), tensor(1.1892, grad_fn=<UnbindBackward0>), tensor(0.8860, grad_fn=<UnbindBackward0>), tensor(1.4467, grad_fn=<UnbindBackward0>), tensor(1.0063, grad_fn=<UnbindBackward0>), tensor(0.5859, grad_fn=<UnbindBackward0>), tensor(0.8411, grad_fn=<UnbindBackward0>), tensor(0.7170, grad_fn=<UnbindBackward0>), tensor(0.3093, grad_fn=<UnbindBackward0>)] and loss is 7.303673267364502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.38002347946167\n",
      "inner loop training loss is 4.708630084991455\n",
      "inner loop training loss is 5.997718334197998\n",
      "inner loop training loss is 5.582882404327393\n",
      "outer loop losses are [tensor(0.9860, grad_fn=<UnbindBackward0>), tensor(1.0768, grad_fn=<UnbindBackward0>), tensor(0.6402, grad_fn=<UnbindBackward0>), tensor(1.2018, grad_fn=<UnbindBackward0>), tensor(0.3430, grad_fn=<UnbindBackward0>), tensor(0.6380, grad_fn=<UnbindBackward0>), tensor(1.1929, grad_fn=<UnbindBackward0>), tensor(0.5791, grad_fn=<UnbindBackward0>), tensor(0.8802, grad_fn=<UnbindBackward0>)] and loss is 7.53805685043335\n",
      "outer loop losses are [tensor(0.9860, grad_fn=<UnbindBackward0>), tensor(1.0768, grad_fn=<UnbindBackward0>), tensor(0.6402, grad_fn=<UnbindBackward0>), tensor(1.2018, grad_fn=<UnbindBackward0>), tensor(0.3430, grad_fn=<UnbindBackward0>), tensor(0.6380, grad_fn=<UnbindBackward0>), tensor(1.1929, grad_fn=<UnbindBackward0>), tensor(0.5791, grad_fn=<UnbindBackward0>), tensor(0.8802, grad_fn=<UnbindBackward0>), tensor(0.6611, grad_fn=<UnbindBackward0>), tensor(0.9814, grad_fn=<UnbindBackward0>), tensor(1.5363, grad_fn=<UnbindBackward0>), tensor(0.7083, grad_fn=<UnbindBackward0>), tensor(1.0079, grad_fn=<UnbindBackward0>), tensor(0.7586, grad_fn=<UnbindBackward0>), tensor(0.9209, grad_fn=<UnbindBackward0>)] and loss is 6.574530601501465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.162169933319092\n",
      "inner loop training loss is 4.614087104797363\n",
      "inner loop training loss is 5.905231475830078\n",
      "inner loop training loss is 5.01278018951416\n",
      "outer loop losses are [tensor(0.6407, grad_fn=<UnbindBackward0>), tensor(1.2658, grad_fn=<UnbindBackward0>), tensor(0.6553, grad_fn=<UnbindBackward0>), tensor(0.8231, grad_fn=<UnbindBackward0>), tensor(0.6743, grad_fn=<UnbindBackward0>), tensor(0.6606, grad_fn=<UnbindBackward0>), tensor(0.9459, grad_fn=<UnbindBackward0>), tensor(0.9196, grad_fn=<UnbindBackward0>), tensor(0.5857, grad_fn=<UnbindBackward0>), tensor(0.9074, grad_fn=<UnbindBackward0>), tensor(0.6680, grad_fn=<UnbindBackward0>), tensor(0.7896, grad_fn=<UnbindBackward0>), tensor(0.6412, grad_fn=<UnbindBackward0>)] and loss is 10.17705249786377\n",
      "outer loop losses are [tensor(0.6407, grad_fn=<UnbindBackward0>), tensor(1.2658, grad_fn=<UnbindBackward0>), tensor(0.6553, grad_fn=<UnbindBackward0>), tensor(0.8231, grad_fn=<UnbindBackward0>), tensor(0.6743, grad_fn=<UnbindBackward0>), tensor(0.6606, grad_fn=<UnbindBackward0>), tensor(0.9459, grad_fn=<UnbindBackward0>), tensor(0.9196, grad_fn=<UnbindBackward0>), tensor(0.5857, grad_fn=<UnbindBackward0>), tensor(0.9074, grad_fn=<UnbindBackward0>), tensor(0.6680, grad_fn=<UnbindBackward0>), tensor(0.7896, grad_fn=<UnbindBackward0>), tensor(0.6412, grad_fn=<UnbindBackward0>), tensor(0.8111, grad_fn=<UnbindBackward0>), tensor(0.7236, grad_fn=<UnbindBackward0>), tensor(0.7373, grad_fn=<UnbindBackward0>)] and loss is 2.271937370300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.446051597595215\n",
      "inner loop training loss is 5.037274360656738\n",
      "inner loop training loss is 5.62559175491333\n",
      "inner loop training loss is 5.2317938804626465\n",
      "outer loop losses are [tensor(0.6928, grad_fn=<UnbindBackward0>), tensor(0.8738, grad_fn=<UnbindBackward0>), tensor(0.8976, grad_fn=<UnbindBackward0>), tensor(1.5593, grad_fn=<UnbindBackward0>), tensor(0.8102, grad_fn=<UnbindBackward0>), tensor(0.8371, grad_fn=<UnbindBackward0>), tensor(0.7634, grad_fn=<UnbindBackward0>), tensor(0.8004, grad_fn=<UnbindBackward0>)] and loss is 7.234619140625\n",
      "outer loop losses are [tensor(0.6928, grad_fn=<UnbindBackward0>), tensor(0.8738, grad_fn=<UnbindBackward0>), tensor(0.8976, grad_fn=<UnbindBackward0>), tensor(1.5593, grad_fn=<UnbindBackward0>), tensor(0.8102, grad_fn=<UnbindBackward0>), tensor(0.8371, grad_fn=<UnbindBackward0>), tensor(0.7634, grad_fn=<UnbindBackward0>), tensor(0.8004, grad_fn=<UnbindBackward0>), tensor(1.2788, grad_fn=<UnbindBackward0>), tensor(0.9619, grad_fn=<UnbindBackward0>), tensor(0.9793, grad_fn=<UnbindBackward0>), tensor(0.6968, grad_fn=<UnbindBackward0>), tensor(0.8811, grad_fn=<UnbindBackward0>), tensor(0.9678, grad_fn=<UnbindBackward0>), tensor(0.5112, grad_fn=<UnbindBackward0>), tensor(0.5876, grad_fn=<UnbindBackward0>)] and loss is 6.864538669586182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.110079765319824\n",
      "inner loop training loss is 5.007311820983887\n",
      "inner loop training loss is 4.353231430053711\n",
      "inner loop training loss is 4.0438642501831055\n",
      "outer loop losses are [tensor(0.6616, grad_fn=<UnbindBackward0>), tensor(1.1001, grad_fn=<UnbindBackward0>), tensor(0.9837, grad_fn=<UnbindBackward0>), tensor(0.6576, grad_fn=<UnbindBackward0>), tensor(0.6108, grad_fn=<UnbindBackward0>), tensor(0.6745, grad_fn=<UnbindBackward0>), tensor(1.0156, grad_fn=<UnbindBackward0>), tensor(0.7944, grad_fn=<UnbindBackward0>)] and loss is 6.498350143432617\n",
      "outer loop losses are [tensor(0.6616, grad_fn=<UnbindBackward0>), tensor(1.1001, grad_fn=<UnbindBackward0>), tensor(0.9837, grad_fn=<UnbindBackward0>), tensor(0.6576, grad_fn=<UnbindBackward0>), tensor(0.6108, grad_fn=<UnbindBackward0>), tensor(0.6745, grad_fn=<UnbindBackward0>), tensor(1.0156, grad_fn=<UnbindBackward0>), tensor(0.7944, grad_fn=<UnbindBackward0>), tensor(0.2827, grad_fn=<UnbindBackward0>), tensor(1.0209, grad_fn=<UnbindBackward0>), tensor(1.5035, grad_fn=<UnbindBackward0>), tensor(0.5937, grad_fn=<UnbindBackward0>), tensor(0.9596, grad_fn=<UnbindBackward0>), tensor(0.6554, grad_fn=<UnbindBackward0>), tensor(0.8173, grad_fn=<UnbindBackward0>), tensor(0.4900, grad_fn=<UnbindBackward0>)] and loss is 6.323118209838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.80278205871582\n",
      "inner loop training loss is 5.13126802444458\n",
      "inner loop training loss is 5.259980201721191\n",
      "inner loop training loss is 4.56481409072876\n",
      "outer loop losses are [tensor(0.5999, grad_fn=<UnbindBackward0>), tensor(0.6668, grad_fn=<UnbindBackward0>), tensor(0.9978, grad_fn=<UnbindBackward0>), tensor(0.6596, grad_fn=<UnbindBackward0>), tensor(0.8337, grad_fn=<UnbindBackward0>), tensor(1.6430, grad_fn=<UnbindBackward0>), tensor(0.9329, grad_fn=<UnbindBackward0>), tensor(0.9328, grad_fn=<UnbindBackward0>), tensor(0.4993, grad_fn=<UnbindBackward0>)] and loss is 7.765660285949707\n",
      "outer loop losses are [tensor(0.5999, grad_fn=<UnbindBackward0>), tensor(0.6668, grad_fn=<UnbindBackward0>), tensor(0.9978, grad_fn=<UnbindBackward0>), tensor(0.6596, grad_fn=<UnbindBackward0>), tensor(0.8337, grad_fn=<UnbindBackward0>), tensor(1.6430, grad_fn=<UnbindBackward0>), tensor(0.9329, grad_fn=<UnbindBackward0>), tensor(0.9328, grad_fn=<UnbindBackward0>), tensor(0.4993, grad_fn=<UnbindBackward0>), tensor(0.8877, grad_fn=<UnbindBackward0>), tensor(0.6217, grad_fn=<UnbindBackward0>), tensor(0.8992, grad_fn=<UnbindBackward0>), tensor(0.9852, grad_fn=<UnbindBackward0>), tensor(0.5384, grad_fn=<UnbindBackward0>), tensor(0.9049, grad_fn=<UnbindBackward0>), tensor(0.7228, grad_fn=<UnbindBackward0>)] and loss is 5.559844970703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.553448677062988\n",
      "inner loop training loss is 5.275846481323242\n",
      "inner loop training loss is 5.789167404174805\n",
      "inner loop training loss is 5.580369472503662\n",
      "outer loop losses are [tensor(0.7267, grad_fn=<UnbindBackward0>), tensor(1.9716, grad_fn=<UnbindBackward0>), tensor(0.7931, grad_fn=<UnbindBackward0>), tensor(0.6815, grad_fn=<UnbindBackward0>), tensor(0.6980, grad_fn=<UnbindBackward0>), tensor(0.7869, grad_fn=<UnbindBackward0>), tensor(0.8330, grad_fn=<UnbindBackward0>), tensor(0.8519, grad_fn=<UnbindBackward0>)] and loss is 7.34274435043335\n",
      "outer loop losses are [tensor(0.7267, grad_fn=<UnbindBackward0>), tensor(1.9716, grad_fn=<UnbindBackward0>), tensor(0.7931, grad_fn=<UnbindBackward0>), tensor(0.6815, grad_fn=<UnbindBackward0>), tensor(0.6980, grad_fn=<UnbindBackward0>), tensor(0.7869, grad_fn=<UnbindBackward0>), tensor(0.8330, grad_fn=<UnbindBackward0>), tensor(0.8519, grad_fn=<UnbindBackward0>), tensor(0.9828, grad_fn=<UnbindBackward0>), tensor(0.5567, grad_fn=<UnbindBackward0>), tensor(0.5981, grad_fn=<UnbindBackward0>), tensor(0.9531, grad_fn=<UnbindBackward0>), tensor(0.7883, grad_fn=<UnbindBackward0>), tensor(0.5345, grad_fn=<UnbindBackward0>), tensor(0.9401, grad_fn=<UnbindBackward0>), tensor(1.0825, grad_fn=<UnbindBackward0>)] and loss is 6.436059951782227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.106863975524902\n",
      "inner loop training loss is 4.366962432861328\n",
      "inner loop training loss is 4.698808670043945\n",
      "inner loop training loss is 4.1727495193481445\n",
      "outer loop losses are [tensor(0.6000, grad_fn=<UnbindBackward0>), tensor(1.0302, grad_fn=<UnbindBackward0>), tensor(0.7189, grad_fn=<UnbindBackward0>), tensor(1.9192, grad_fn=<UnbindBackward0>), tensor(1.0152, grad_fn=<UnbindBackward0>), tensor(1.1492, grad_fn=<UnbindBackward0>), tensor(0.8919, grad_fn=<UnbindBackward0>), tensor(0.5020, grad_fn=<UnbindBackward0>), tensor(0.5991, grad_fn=<UnbindBackward0>)] and loss is 8.425626754760742\n",
      "outer loop losses are [tensor(0.6000, grad_fn=<UnbindBackward0>), tensor(1.0302, grad_fn=<UnbindBackward0>), tensor(0.7189, grad_fn=<UnbindBackward0>), tensor(1.9192, grad_fn=<UnbindBackward0>), tensor(1.0152, grad_fn=<UnbindBackward0>), tensor(1.1492, grad_fn=<UnbindBackward0>), tensor(0.8919, grad_fn=<UnbindBackward0>), tensor(0.5020, grad_fn=<UnbindBackward0>), tensor(0.5991, grad_fn=<UnbindBackward0>), tensor(0.4197, grad_fn=<UnbindBackward0>), tensor(0.9713, grad_fn=<UnbindBackward0>), tensor(1.0864, grad_fn=<UnbindBackward0>), tensor(0.4304, grad_fn=<UnbindBackward0>), tensor(0.6827, grad_fn=<UnbindBackward0>), tensor(0.4707, grad_fn=<UnbindBackward0>), tensor(0.8671, grad_fn=<UnbindBackward0>)] and loss is 4.92838191986084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.936229705810547\n",
      "inner loop training loss is 5.167829990386963\n",
      "inner loop training loss is 4.889267444610596\n",
      "inner loop training loss is 4.386970520019531\n",
      "outer loop losses are [tensor(0.9675, grad_fn=<UnbindBackward0>), tensor(0.9898, grad_fn=<UnbindBackward0>), tensor(1.8141, grad_fn=<UnbindBackward0>), tensor(0.6765, grad_fn=<UnbindBackward0>), tensor(0.6250, grad_fn=<UnbindBackward0>), tensor(1.4418, grad_fn=<UnbindBackward0>), tensor(0.5125, grad_fn=<UnbindBackward0>), tensor(0.6089, grad_fn=<UnbindBackward0>)] and loss is 7.6360626220703125\n",
      "outer loop losses are [tensor(0.9675, grad_fn=<UnbindBackward0>), tensor(0.9898, grad_fn=<UnbindBackward0>), tensor(1.8141, grad_fn=<UnbindBackward0>), tensor(0.6765, grad_fn=<UnbindBackward0>), tensor(0.6250, grad_fn=<UnbindBackward0>), tensor(1.4418, grad_fn=<UnbindBackward0>), tensor(0.5125, grad_fn=<UnbindBackward0>), tensor(0.6089, grad_fn=<UnbindBackward0>), tensor(0.7998, grad_fn=<UnbindBackward0>), tensor(0.6158, grad_fn=<UnbindBackward0>), tensor(0.5674, grad_fn=<UnbindBackward0>), tensor(0.8759, grad_fn=<UnbindBackward0>), tensor(0.5995, grad_fn=<UnbindBackward0>), tensor(0.9825, grad_fn=<UnbindBackward0>), tensor(0.8210, grad_fn=<UnbindBackward0>), tensor(1.6645, grad_fn=<UnbindBackward0>)] and loss is 6.926438331604004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.727623462677002\n",
      "inner loop training loss is 3.9513285160064697\n",
      "inner loop training loss is 5.034641742706299\n",
      "inner loop training loss is 4.400092601776123\n",
      "outer loop losses are [tensor(0.8744, grad_fn=<UnbindBackward0>), tensor(1.0884, grad_fn=<UnbindBackward0>), tensor(0.6110, grad_fn=<UnbindBackward0>), tensor(1.0514, grad_fn=<UnbindBackward0>), tensor(0.6518, grad_fn=<UnbindBackward0>), tensor(0.6009, grad_fn=<UnbindBackward0>), tensor(0.4159, grad_fn=<UnbindBackward0>), tensor(0.4258, grad_fn=<UnbindBackward0>), tensor(0.5463, grad_fn=<UnbindBackward0>), tensor(1.0509, grad_fn=<UnbindBackward0>), tensor(0.8310, grad_fn=<UnbindBackward0>), tensor(1.0817, grad_fn=<UnbindBackward0>), tensor(1.2848, grad_fn=<UnbindBackward0>), tensor(1.1282, grad_fn=<UnbindBackward0>)] and loss is 11.642616271972656\n",
      "outer loop losses are [tensor(0.8744, grad_fn=<UnbindBackward0>), tensor(1.0884, grad_fn=<UnbindBackward0>), tensor(0.6110, grad_fn=<UnbindBackward0>), tensor(1.0514, grad_fn=<UnbindBackward0>), tensor(0.6518, grad_fn=<UnbindBackward0>), tensor(0.6009, grad_fn=<UnbindBackward0>), tensor(0.4159, grad_fn=<UnbindBackward0>), tensor(0.4258, grad_fn=<UnbindBackward0>), tensor(0.5463, grad_fn=<UnbindBackward0>), tensor(1.0509, grad_fn=<UnbindBackward0>), tensor(0.8310, grad_fn=<UnbindBackward0>), tensor(1.0817, grad_fn=<UnbindBackward0>), tensor(1.2848, grad_fn=<UnbindBackward0>), tensor(1.1282, grad_fn=<UnbindBackward0>), tensor(0.5027, grad_fn=<UnbindBackward0>), tensor(1.0082, grad_fn=<UnbindBackward0>)] and loss is 1.5108144283294678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 4.752189636230469\n",
      "inner loop training loss is 4.828998565673828\n",
      "inner loop training loss is 4.835038661956787\n",
      "inner loop training loss is 4.8066606521606445\n",
      "outer loop losses are [tensor(0.6058, grad_fn=<UnbindBackward0>), tensor(0.7395, grad_fn=<UnbindBackward0>), tensor(0.7044, grad_fn=<UnbindBackward0>), tensor(0.6449, grad_fn=<UnbindBackward0>), tensor(0.6962, grad_fn=<UnbindBackward0>), tensor(0.7284, grad_fn=<UnbindBackward0>)] and loss is 4.119276523590088\n",
      "outer loop losses are [tensor(0.6058, grad_fn=<UnbindBackward0>), tensor(0.7395, grad_fn=<UnbindBackward0>), tensor(0.7044, grad_fn=<UnbindBackward0>), tensor(0.6449, grad_fn=<UnbindBackward0>), tensor(0.6962, grad_fn=<UnbindBackward0>), tensor(0.7284, grad_fn=<UnbindBackward0>), tensor(0.7081, grad_fn=<UnbindBackward0>), tensor(1.2026, grad_fn=<UnbindBackward0>), tensor(0.7261, grad_fn=<UnbindBackward0>), tensor(0.7082, grad_fn=<UnbindBackward0>), tensor(0.7397, grad_fn=<UnbindBackward0>), tensor(1.3731, grad_fn=<UnbindBackward0>), tensor(0.6594, grad_fn=<UnbindBackward0>), tensor(0.6063, grad_fn=<UnbindBackward0>), tensor(0.6836, grad_fn=<UnbindBackward0>), tensor(0.6924, grad_fn=<UnbindBackward0>)] and loss is 8.099648475646973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner loop training loss is 5.111295700073242\n",
      "inner loop training loss is 4.604230880737305\n"
     ]
    }
   ],
   "source": [
    "from training.trainer import train_model\n",
    "\n",
    "protomaml_model = train_model(\n",
    "    ProtoFOMAML,\n",
    "    train_loader=train_protomaml_loader,\n",
    "    val_loader=val_protomaml_loader,\n",
    "    outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=7, batchSize=16, warmupSteps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd82a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
