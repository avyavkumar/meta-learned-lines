{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbc045a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from lines.Line import Line\n",
    "# from prototypes.models.PrototypeClassifierModels import CLASSIFIER_MODEL_2NN\n",
    "# from training.CrossEntropyTraining import ClassifierTrainerWithCrossEntropyNoGen\n",
    "# from datautils.LEOPARDEncoderUtils import get_labelled_LEOPARD_training_data as get_labelled_training_data\n",
    "# from lines.LineGenerator import LineGenerator\n",
    "# # from evaluation.Evaluator import Evaluator\n",
    "\n",
    "# test_params = {\n",
    "#   'type': 'cross_entropy',\n",
    "#   'encoder': \"bert\",\n",
    "# }\n",
    "\n",
    "# trainer_params = {\n",
    "#   'shuffle': True,\n",
    "#   'num_workers': 0\n",
    "# }\n",
    "\n",
    "# batch_size = {\n",
    "#   4: 4, 8: 8, 16: 12\n",
    "# }\n",
    "\n",
    "# epochs = {\n",
    "#   4: {\n",
    "#     2: 15, 3: 30, 4: 45, 5: 60, 6: 75\n",
    "#   },\n",
    "#   8: {\n",
    "#     2: 30, 3: 40, 4: 50, 5: 70, 6: 100\n",
    "#   },\n",
    "#   16: {\n",
    "#     2: 15, 3: 25, 4: 35, 5: 45, 6: 55\n",
    "#   },\n",
    "# }\n",
    "\n",
    "# learning_rates = {\n",
    "#   4: 5e-3,\n",
    "#   8: 1e-2,\n",
    "#   16: 5e-2\n",
    "# }\n",
    "\n",
    "# training_params = {\n",
    "#   'encoder': \"bert\",\n",
    "#   'epochs': epochs,\n",
    "#   'warmupSteps': 100,\n",
    "#   'reduction': 'none',\n",
    "#   'learning_rate': learning_rates,\n",
    "#   'printValidationPlot': True,\n",
    "#   'printValidationLoss': False\n",
    "# }\n",
    "\n",
    "# for category in [\"rating_dvd\"]:\n",
    "#     training_params['category'] = category\n",
    "#     for episode in range(10):\n",
    "#         training_params['episode'] = episode\n",
    "#         for shot in [4]:\n",
    "#             training_params['shot'] = shot\n",
    "#             trainer_params['batch_size'] = batch_size[shot]\n",
    "#             training_encodings, training_labels, label_keys = get_labelled_training_data(category, shot, episode)\n",
    "            \n",
    "#             if training_encodings.shape[0] == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             training_set = {}\n",
    "#             training_set['encodings'] = training_encodings\n",
    "#             training_set['labels'] = training_labels\n",
    "            \n",
    "#             lineGenerator = LineGenerator(training_set, CLASSIFIER_MODEL_2NN)\n",
    "#             classifierTrainer = ClassifierTrainerWithCrossEntropyNoGen(lineGenerator, trainer_params, training_params, label_keys)\n",
    "#             classifierTrainer.trainPrototypes(training_params, trainer_params, training_set)\n",
    "#             lines = classifierTrainer.getLines()\n",
    "            \n",
    "# #             evaluator = Evaluator(label_keys, test_params)\n",
    "# #             evaluator.evaluate(category, shot, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b5482b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1061.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1488.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n",
      "<training_datasets.GLUEDataset.GLUEDataset object at 0x7fcf40716e00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "cola = load_dataset('glue','cola')\n",
    "print(cola)\n",
    "# sst2 = load_dataset('glue','sst2')\n",
    "# print(sst2)\n",
    "mrpc = load_dataset('glue', 'mrpc')\n",
    "print(mrpc)\n",
    "pt_cola = GLUEDataset([cola, mrpc], 'train')\n",
    "print(pt_cola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb229056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "# sampler = FewShotEpisodeSampler(pt_cola, kShot=2, nWay=4, shuffle=True)\n",
    "# train_data_loader = data.DataLoader(\n",
    "#     pt_cola,\n",
    "#     batch_sampler=sampler,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "# data, targets = next(iter(train_data_loader))\n",
    "\n",
    "# for i in range(5):\n",
    "#     data, targets = next(iter(train_data_loader))\n",
    "#     print(data)\n",
    "#     print(targets)\n",
    "\n",
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(pt_cola, kShot=2, nWay=4, batchSize=3, shuffle=True)\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    pt_cola, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "for i in range(1):\n",
    "    batch = next(iter(train_protomaml_loader))\n",
    "    for episode_i in range(len(batch[0])):\n",
    "        data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "        supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]])\n",
      "tensor([-276.6008, -257.7414, -268.7680, -257.0078])\n",
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]])\n",
      "tensor([-276.6008, -257.7414, -268.7680, -257.0078])\n",
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]])\n",
      "tensor([-276.6008, -257.7414, -268.7680, -257.0078])\n",
      "tensor([[ 0.8513,  0.2463, -0.0664,  ..., -0.4092,  0.5120, -0.4031],\n",
      "        [ 0.7107,  0.1759, -0.5804,  ..., -0.5555,  0.1123, -0.1246],\n",
      "        [ 0.6555, -0.0618,  0.4311,  ..., -0.3217,  0.4120,  0.0432],\n",
      "        [ 0.4278,  0.0900, -0.4331,  ..., -0.4846,  0.2068,  0.0856]])\n",
      "tensor([-276.6008, -257.7414, -268.7680, -257.0078])\n",
      "[array([3., 0.]), array([2., 1.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]])\n",
      "tensor([-262.8743, -264.0276, -256.0292, -270.3751])\n",
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]])\n",
      "tensor([-262.8743, -264.0276, -256.0292, -270.3751])\n",
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]])\n",
      "tensor([-262.8743, -264.0276, -256.0292, -270.3751])\n",
      "tensor([[ 0.4641,  0.2270, -0.0442,  ..., -0.5645,  0.5754, -0.6869],\n",
      "        [ 0.5371, -0.1211, -0.1727,  ..., -0.5288,  0.1132,  0.1254],\n",
      "        [ 1.0453,  0.0924,  0.3596,  ..., -0.4572,  0.1936,  0.1107],\n",
      "        [ 0.4543, -0.0549, -0.1443,  ..., -0.4803,  0.0073,  0.3440]])\n",
      "tensor([-262.8743, -264.0276, -256.0292, -270.3751])\n",
      "[array([3., 1.]), array([2., 0.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]])\n",
      "tensor([-260.2888, -261.9232, -274.5767, -261.5992])\n",
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]])\n",
      "tensor([-260.2888, -261.9232, -274.5767, -261.5992])\n",
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]])\n",
      "tensor([-260.2888, -261.9232, -274.5767, -261.5992])\n",
      "tensor([[ 0.8482, -0.0148,  0.2314,  ..., -0.3818,  0.5245, -0.0465],\n",
      "        [ 0.6352, -0.4698, -0.1750,  ..., -0.4889,  0.2693,  0.1165],\n",
      "        [ 0.8977, -0.0496, -0.2737,  ..., -0.1827,  0.3254,  0.2449],\n",
      "        [ 0.1257, -0.2261, -0.3078,  ..., -0.1631,  0.0702, -0.0879]])\n",
      "tensor([-260.2888, -261.9232, -274.5767, -261.5992])\n",
      "[array([2., 1.]), array([3., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from training.models.FOMAML import FOMAML\n",
    "\n",
    "fomaml = FOMAML(None, None, None, None, None)\n",
    "fomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ccbedc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 5., 7., 3., 5., 3.])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([1,2,3,4,5,6,7,3,4,5,3])\n",
    "labels = torch.Tensor([1,2,2,2,1,2,1,1,2,1,1])\n",
    "print(x[torch.where(labels == 1)[0]])\n",
    "print(x[torch.where(labels == 1)[0]].mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11deaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
