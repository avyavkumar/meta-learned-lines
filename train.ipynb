{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5482b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 931.03it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1439.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "cola = load_dataset('glue','cola')\n",
    "# print(cola)\n",
    "# sst2 = load_dataset('glue','sst2')\n",
    "# print(sst2)\n",
    "mrpc = load_dataset('glue', 'mrpc')\n",
    "# print(mrpc)\n",
    "pt_cola = GLUEDataset([cola, mrpc], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb229056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "# sampler = FewShotEpisodeSampler(pt_cola, kShot=2, nWay=4, shuffle=True)\n",
    "# train_data_loader = data.DataLoader(\n",
    "#     pt_cola,\n",
    "#     batch_sampler=sampler,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "# data, targets = next(iter(train_data_loader))\n",
    "\n",
    "# for i in range(5):\n",
    "#     data, targets = next(iter(train_data_loader))\n",
    "#     print(data)\n",
    "#     print(targets)\n",
    "\n",
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(pt_cola, kShot=2, nWay=4, batchSize=8, shuffle=True)\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    pt_cola, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.5183, 0.5480, 0.4841, 0.4798], grad_fn=<NllLossBackward0>) and loss is 2.030160665512085\n",
      "losses are tensor([0.5064, 0.5428, 0.4777, 0.4692], grad_fn=<NllLossBackward0>) and loss is 1.9961217641830444\n",
      "losses are tensor([0.4956, 0.5376, 0.4712, 0.4590], grad_fn=<NllLossBackward0>) and loss is 1.963428020477295\n",
      "losses are tensor([0.4827, 0.5345, 0.4664, 0.4472], grad_fn=<NllLossBackward0>) and loss is 1.9307979345321655\n",
      "losses are tensor([0.4731, 0.5282, 0.4587, 0.4386], grad_fn=<NllLossBackward0>) and loss is 1.8985846042633057\n",
      "losses are tensor([0.4408, 0.4907, 0.5207, 0.4262], grad_fn=<NllLossBackward0>) and loss is 1.8783413171768188\n",
      "losses are tensor([0.4284, 0.4789, 0.5138, 0.4188], grad_fn=<NllLossBackward0>) and loss is 1.8399250507354736\n",
      "losses are tensor([0.4173, 0.4684, 0.5057, 0.4105], grad_fn=<NllLossBackward0>) and loss is 1.8018122911453247\n",
      "losses are tensor([0.4056, 0.4571, 0.4982, 0.4027], grad_fn=<NllLossBackward0>) and loss is 1.76362943649292\n",
      "losses are tensor([0.3950, 0.4472, 0.4889, 0.3939], grad_fn=<NllLossBackward0>) and loss is 1.7250351905822754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.3292, 0.3214, 0.3991, 0.3255], grad_fn=<NllLossBackward0>) and loss is 1.3752589225769043\n",
      "losses are tensor([0.3086, 0.3130, 0.3887, 0.3079], grad_fn=<NllLossBackward0>) and loss is 1.3181613683700562\n",
      "losses are tensor([0.2894, 0.3059, 0.3799, 0.2911], grad_fn=<NllLossBackward0>) and loss is 1.266303539276123\n",
      "losses are tensor([0.2741, 0.2962, 0.3684, 0.2778], grad_fn=<NllLossBackward0>) and loss is 1.2164573669433594\n",
      "losses are tensor([0.2597, 0.2871, 0.3570, 0.2651], grad_fn=<NllLossBackward0>) and loss is 1.168939471244812\n",
      "losses are tensor([0.2787, 0.2910, 0.1823, 0.3130], grad_fn=<NllLossBackward0>) and loss is 1.0650256872177124\n",
      "losses are tensor([0.2700, 0.2573, 0.1616, 0.3053], grad_fn=<NllLossBackward0>) and loss is 0.9941574335098267\n",
      "losses are tensor([0.2588, 0.2314, 0.1461, 0.2943], grad_fn=<NllLossBackward0>) and loss is 0.9305151700973511\n",
      "losses are tensor([0.2465, 0.2113, 0.1335, 0.2818], grad_fn=<NllLossBackward0>) and loss is 0.8731834292411804\n",
      "losses are tensor([0.2343, 0.1940, 0.1228, 0.2690], grad_fn=<NllLossBackward0>) and loss is 0.8202314376831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.4842, 0.5086, 0.4390, 0.4391], grad_fn=<NllLossBackward0>) and loss is 1.870865821838379\n",
      "losses are tensor([0.4690, 0.5026, 0.4318, 0.4240], grad_fn=<NllLossBackward0>) and loss is 1.8273590803146362\n",
      "losses are tensor([0.4553, 0.4955, 0.4251, 0.4105], grad_fn=<NllLossBackward0>) and loss is 1.7863563299179077\n",
      "losses are tensor([0.4442, 0.4864, 0.4158, 0.3994], grad_fn=<NllLossBackward0>) and loss is 1.745758295059204\n",
      "losses are tensor([0.4335, 0.4775, 0.4067, 0.3887], grad_fn=<NllLossBackward0>) and loss is 1.706266164779663\n",
      "losses are tensor([0.3594, 0.5048, 0.4175, 0.4705], grad_fn=<NllLossBackward0>) and loss is 1.7521677017211914\n",
      "losses are tensor([0.3444, 0.4838, 0.4098, 0.4644], grad_fn=<NllLossBackward0>) and loss is 1.7024099826812744\n",
      "losses are tensor([0.3321, 0.4667, 0.3996, 0.4556], grad_fn=<NllLossBackward0>) and loss is 1.6539751291275024\n",
      "losses are tensor([0.3198, 0.4493, 0.3896, 0.4469], grad_fn=<NllLossBackward0>) and loss is 1.6055325269699097\n",
      "losses are tensor([0.3087, 0.4334, 0.3789, 0.4367], grad_fn=<NllLossBackward0>) and loss is 1.5576446056365967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.3621, 0.4516, 0.3440, 0.3584], grad_fn=<NllLossBackward0>) and loss is 1.5161285400390625\n",
      "losses are tensor([0.3438, 0.4386, 0.3322, 0.3427], grad_fn=<NllLossBackward0>) and loss is 1.457349181175232\n",
      "losses are tensor([0.3270, 0.4249, 0.3204, 0.3286], grad_fn=<NllLossBackward0>) and loss is 1.400792121887207\n",
      "losses are tensor([0.3117, 0.4102, 0.3083, 0.3158], grad_fn=<NllLossBackward0>) and loss is 1.3459789752960205\n",
      "losses are tensor([0.2976, 0.3951, 0.2964, 0.3040], grad_fn=<NllLossBackward0>) and loss is 1.2930103540420532\n",
      "losses are tensor([0.2440, 0.3568, 0.4119, 0.3791], grad_fn=<NllLossBackward0>) and loss is 1.3918545246124268\n",
      "losses are tensor([0.2305, 0.3450, 0.3918, 0.3670], grad_fn=<NllLossBackward0>) and loss is 1.3342469930648804\n",
      "losses are tensor([0.2194, 0.3319, 0.3752, 0.3537], grad_fn=<NllLossBackward0>) and loss is 1.2802025079727173\n",
      "losses are tensor([0.2085, 0.3205, 0.3584, 0.3420], grad_fn=<NllLossBackward0>) and loss is 1.2294092178344727\n",
      "losses are tensor([0.1991, 0.3081, 0.3437, 0.3291], grad_fn=<NllLossBackward0>) and loss is 1.1801024675369263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.3473, 0.7428, 0.4489, 0.6147, 0.3819, 0.4736],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 3.009168863296509\n",
      "losses are tensor([0.3294, 0.7194, 0.4216, 0.5756, 0.3431, 0.4223],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 2.8113434314727783\n",
      "losses are tensor([0.3129, 0.6930, 0.3941, 0.5349, 0.3123, 0.3822],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 2.629389762878418\n",
      "losses are tensor([0.2979, 0.6664, 0.3682, 0.4961, 0.2857, 0.3474],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 2.461681604385376\n",
      "losses are tensor([0.2834, 0.6396, 0.3439, 0.4589, 0.2633, 0.3181],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 2.3072028160095215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.2830, 0.3204, 0.2123, 0.1113, 0.4193, 0.4463],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.7925889492034912\n",
      "losses are tensor([0.2552, 0.2894, 0.1756, 0.0969, 0.3957, 0.4147],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.6274380683898926\n",
      "losses are tensor([0.2320, 0.2630, 0.1490, 0.0859, 0.3719, 0.3827],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.4844284057617188\n",
      "losses are tensor([0.2126, 0.2408, 0.1288, 0.0769, 0.3484, 0.3517],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.3592939376831055\n",
      "losses are tensor([0.1955, 0.2220, 0.1130, 0.0694, 0.3260, 0.3227],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.2486417293548584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.0460, 0.1848, 0.1009, 0.3090, 0.2101, 0.3219],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.1725306510925293\n",
      "losses are tensor([0.0406, 0.1785, 0.0858, 0.2836, 0.1999, 0.2972],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.08543062210083\n",
      "losses are tensor([0.0365, 0.1704, 0.0747, 0.2624, 0.1890, 0.2760],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.0090223550796509\n",
      "losses are tensor([0.0332, 0.1624, 0.0660, 0.2447, 0.1784, 0.2570],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 0.941743016242981\n",
      "losses are tensor([0.0304, 0.1548, 0.0589, 0.2290, 0.1686, 0.2401],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 0.8818447589874268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([0.2541, 0.0314, 0.3233, 0.1270, 0.2657, 0.2142],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.215673804283142\n",
      "losses are tensor([0.2355, 0.0271, 0.2983, 0.1049, 0.2429, 0.1899],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 1.0986073017120361\n",
      "losses are tensor([0.2166, 0.0239, 0.2770, 0.0891, 0.2239, 0.1693],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 0.9997536540031433\n",
      "losses are tensor([0.1998, 0.0214, 0.2573, 0.0774, 0.2065, 0.1514],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 0.9139213562011719\n",
      "losses are tensor([0.1846, 0.0193, 0.2396, 0.0680, 0.1922, 0.1366],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 0.8404543399810791\n"
     ]
    }
   ],
   "source": [
    "from training.models.FOMAML import FOMAML\n",
    "\n",
    "fomaml = FOMAML(outerLR=5e-5, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "fomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ccbedc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 5., 7., 3., 5., 3.])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([1,2,3,4,5,6,7,3,4,5,3])\n",
    "labels = torch.Tensor([1,2,2,2,1,2,1,1,2,1,1])\n",
    "print(x[torch.where(labels == 1)[0]])\n",
    "print(x[torch.where(labels == 1)[0]].mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a108e3a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "tunable_layers = {str(l) for l in range(12)}\n",
    "for name, param in model.named_parameters():\n",
    "    print(param.grad)\n",
    "#     if not set.intersection(set(name.split('.')), tunable_layers):\n",
    "#         param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e316ebb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "print(model.parameters().data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bbf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
