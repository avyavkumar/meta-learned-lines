{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb68d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1289.36it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 732.32it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1436.73it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1047.96it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 332.34it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1238.72it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1276.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "meta_ds = GLUEMetaDataset(k=4,numTasks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d24829a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(meta_ds, kShot=4, batchSize=4)\n",
    "\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    meta_ds, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=4)\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3670cc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=4)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "for i in range(1):\n",
    "    batch = next(iter(val_protomaml_loader))\n",
    "    for episode_i in range(len(batch[0])):\n",
    "        data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "        supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)\n",
    "#         print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef372a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "from training.models.ProtoNet import ProtoNet\n",
    "\n",
    "# pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "# pfomaml.validation_step(next(iter(val_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0cb485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | metaLearner | PrototypeMetaModel | 108 M \n",
      "---------------------------------------------------\n",
      "393 K     Trainable params\n",
      "108 M     Non-trainable params\n",
      "108 M     Total params\n",
      "434.816   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ProtoFOMAML with parameters \"batchSize\":   8\n",
      "\"innerLR\":     0.001\n",
      "\"outerLR\":     0.0005\n",
      "\"outputLR\":    0.01\n",
      "\"steps\":       5\n",
      "\"warmupSteps\": 0\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.179638385772705 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.231433868408203 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 11.463162660598755 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 11.127881526947021 and accuracy is 1.0\n",
      "inner loop training loss is 12.115628480911255 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.25 and loss is 13.138765811920166\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.020815849304199 and accuracy is 0.875\n",
      "inner loop training loss is 3.639312505722046 and accuracy is 1.0\n",
      "inner loop training loss is 3.6645801067352295 and accuracy is 1.0\n",
      "inner loop training loss is 4.10959529876709 and accuracy is 0.875\n",
      "inner loop training loss is 4.065356731414795 and accuracy is 0.875\n",
      "outer loop training accuracy is 0.375 and loss is 6.304055690765381\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.511903524398804 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.243494033813477 and accuracy is 0.5833333333333334\n",
      "inner loop training loss is 10.928996086120605 and accuracy is 0.75\n",
      "inner loop training loss is 11.072431802749634 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 10.34057331085205 and accuracy is 0.9166666666666666\n",
      "outer loop training accuracy is 0.4166666666666667 and loss is 13.318746089935303\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.611923694610596 and accuracy is 0.75\n",
      "inner loop training loss is 5.234160423278809 and accuracy is 0.625\n",
      "inner loop training loss is 4.704345226287842 and accuracy is 0.875\n",
      "inner loop training loss is 4.635591506958008 and accuracy is 0.875\n",
      "inner loop training loss is 4.802205562591553 and accuracy is 0.625\n",
      "outer loop training accuracy is 0.5 and loss is 5.933948040008545\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.425379276275635 and accuracy is 1.0\n",
      "inner loop training loss is 4.41115140914917 and accuracy is 1.0\n",
      "inner loop training loss is 4.349356651306152 and accuracy is 1.0\n",
      "inner loop training loss is 4.35987663269043 and accuracy is 1.0\n",
      "inner loop training loss is 4.004876136779785 and accuracy is 1.0\n",
      "outer loop training accuracy is 0.625 and loss is 5.444874286651611\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.499599456787109 and accuracy is 0.75\n",
      "inner loop training loss is 5.185992240905762 and accuracy is 0.75\n",
      "inner loop training loss is 5.357812881469727 and accuracy is 0.625\n",
      "inner loop training loss is 4.945460796356201 and accuracy is 0.625\n",
      "inner loop training loss is 4.658703327178955 and accuracy is 0.625\n",
      "outer loop training accuracy is 0.625 and loss is 5.077314376831055\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.4907917976379395 and accuracy is 0.875\n",
      "inner loop training loss is 4.292596817016602 and accuracy is 0.875\n",
      "inner loop training loss is 4.623067855834961 and accuracy is 0.875\n",
      "inner loop training loss is 3.904256820678711 and accuracy is 0.875\n",
      "inner loop training loss is 5.352363586425781 and accuracy is 0.625\n",
      "outer loop training accuracy is 0.5 and loss is 5.688411235809326\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.529449939727783 and accuracy is 0.75\n",
      "inner loop training loss is 11.653937339782715 and accuracy is 0.5833333333333334\n",
      "inner loop training loss is 12.070552825927734 and accuracy is 0.4166666666666667\n",
      "inner loop training loss is 10.86940884590149 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 11.090556859970093 and accuracy is 0.5833333333333334\n",
      "outer loop training accuracy is 0.25 and loss is 13.931778907775879\n",
      "outer loop accuracy is 0.42105263157894735 and total loss is tensor(68.8379, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.303834915161133 and accuracy is 0.75\n",
      "inner loop training loss is 10.948730230331421 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 10.399428606033325 and accuracy is 0.75\n",
      "inner loop training loss is 10.181949615478516 and accuracy is 0.75\n",
      "inner loop training loss is 10.074029684066772 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.3333333333333333 and loss is 13.953200817108154\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.5719428062438965 and accuracy is 1.0\n",
      "inner loop training loss is 4.424367427825928 and accuracy is 1.0\n",
      "inner loop training loss is 4.390249252319336 and accuracy is 0.875\n",
      "inner loop training loss is 4.502505779266357 and accuracy is 0.875\n",
      "inner loop training loss is 4.437375068664551 and accuracy is 1.0\n",
      "outer loop training accuracy is 0.625 and loss is 5.587550640106201\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.170370101928711 and accuracy is 1.0\n",
      "inner loop training loss is 4.190937042236328 and accuracy is 1.0\n",
      "inner loop training loss is 4.754265785217285 and accuracy is 0.875\n",
      "inner loop training loss is 4.285611152648926 and accuracy is 1.0\n",
      "inner loop training loss is 4.3213396072387695 and accuracy is 1.0\n",
      "outer loop training accuracy is 0.5 and loss is 5.29979944229126\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.352962970733643 and accuracy is 1.0\n",
      "inner loop training loss is 4.316431045532227 and accuracy is 1.0\n",
      "inner loop training loss is 4.565059661865234 and accuracy is 1.0\n",
      "inner loop training loss is 4.527361869812012 and accuracy is 1.0\n",
      "inner loop training loss is 4.360918045043945 and accuracy is 1.0\n",
      "outer loop training accuracy is 0.5 and loss is 5.684140682220459\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.445281744003296 and accuracy is 0.75\n",
      "inner loop training loss is 11.353616952896118 and accuracy is 0.5833333333333334\n",
      "inner loop training loss is 11.866549015045166 and accuracy is 0.5\n",
      "inner loop training loss is 12.359143495559692 and accuracy is 0.4166666666666667\n",
      "inner loop training loss is 11.457735300064087 and accuracy is 0.6666666666666666\n",
      "outer loop training accuracy is 0.25 and loss is 13.104970455169678\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.800920724868774 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 11.011412620544434 and accuracy is 0.75\n",
      "inner loop training loss is 10.505865812301636 and accuracy is 1.0\n",
      "inner loop training loss is 10.179200172424316 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 11.320558309555054 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.25 and loss is 12.815989971160889\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.248534679412842 and accuracy is 0.875\n",
      "inner loop training loss is 5.086650371551514 and accuracy is 0.5\n",
      "inner loop training loss is 4.68517541885376 and accuracy is 0.875\n",
      "inner loop training loss is 4.564904689788818 and accuracy is 1.0\n",
      "inner loop training loss is 4.6547722816467285 and accuracy is 0.875\n",
      "outer loop training accuracy is 0.375 and loss is 5.8256964683532715\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.147838592529297 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 10.677572250366211 and accuracy is 1.0\n",
      "inner loop training loss is 11.505176305770874 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 10.993013620376587 and accuracy is 1.0\n",
      "inner loop training loss is 11.132493495941162 and accuracy is 0.8333333333333334\n",
      "outer loop training accuracy is 0.3333333333333333 and loss is 13.35673475265503\n",
      "outer loop accuracy is 0.375 and total loss is tensor(75.6281, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 3.814931869506836 and accuracy is 1.0\n",
      "inner loop training loss is 4.8954644203186035 and accuracy is 0.625\n",
      "inner loop training loss is 5.166024684906006 and accuracy is 0.625\n",
      "inner loop training loss is 4.97104549407959 and accuracy is 0.875\n",
      "inner loop training loss is 4.834670543670654 and accuracy is 0.625\n",
      "outer loop training accuracy is 0.375 and loss is 6.065747261047363\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.418947696685791 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 11.574256420135498 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.484865665435791 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.343554019927979 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 11.360633373260498 and accuracy is 0.8333333333333334\n",
      "outer loop training accuracy is 0.16666666666666666 and loss is 13.492199897766113\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.827977180480957 and accuracy is 0.75\n",
      "inner loop training loss is 4.95857048034668 and accuracy is 0.75\n",
      "inner loop training loss is 4.707982540130615 and accuracy is 0.875\n",
      "inner loop training loss is 4.455455780029297 and accuracy is 0.875\n",
      "inner loop training loss is 4.779839992523193 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.5 and loss is 5.657146453857422\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.60159158706665 and accuracy is 0.75\n",
      "inner loop training loss is 3.6912097930908203 and accuracy is 1.0\n",
      "inner loop training loss is 4.4340338706970215 and accuracy is 0.875\n",
      "inner loop training loss is 4.916141510009766 and accuracy is 0.75\n",
      "inner loop training loss is 4.665468215942383 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.5 and loss is 6.03008508682251\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.172104835510254 and accuracy is 1.0\n",
      "inner loop training loss is 4.859106063842773 and accuracy is 0.875\n",
      "inner loop training loss is 4.457629680633545 and accuracy is 0.875\n",
      "inner loop training loss is 4.4544596672058105 and accuracy is 0.75\n",
      "inner loop training loss is 4.123047828674316 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.5 and loss is 5.460790634155273\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.4540181159973145 and accuracy is 0.625\n",
      "inner loop training loss is 4.65133810043335 and accuracy is 0.75\n",
      "inner loop training loss is 5.013976097106934 and accuracy is 0.75\n",
      "inner loop training loss is 4.439364910125732 and accuracy is 0.875\n",
      "inner loop training loss is 4.403628826141357 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.5 and loss is 5.63287353515625\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.270633459091187 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.510698556900024 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 11.666439294815063 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 11.28444242477417 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 11.439124822616577 and accuracy is 0.6666666666666666\n",
      "outer loop training accuracy is 0.3333333333333333 and loss is 12.849885940551758\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.729489803314209 and accuracy is 0.75\n",
      "inner loop training loss is 4.501097202301025 and accuracy is 1.0\n",
      "inner loop training loss is 4.787534713745117 and accuracy is 0.75\n",
      "inner loop training loss is 4.922124862670898 and accuracy is 0.75\n",
      "inner loop training loss is 4.705630302429199 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.125 and loss is 6.110167026519775\n",
      "outer loop accuracy is 0.3611111111111111 and total loss is tensor(61.2989, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.822483062744141 and accuracy is 0.875\n",
      "inner loop training loss is 4.826746940612793 and accuracy is 1.0\n",
      "inner loop training loss is 4.6443610191345215 and accuracy is 1.0\n",
      "inner loop training loss is 4.649045944213867 and accuracy is 1.0\n",
      "inner loop training loss is 4.780853748321533 and accuracy is 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer loop training accuracy is 0.75 and loss is 5.265951633453369\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 11.425241470336914 and accuracy is 0.6666666666666666\n",
      "inner loop training loss is 10.475454330444336 and accuracy is 0.75\n",
      "inner loop training loss is 12.691738843917847 and accuracy is 0.5\n",
      "inner loop training loss is 10.605561256408691 and accuracy is 0.75\n",
      "inner loop training loss is 10.279190301895142 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.25 and loss is 14.24005937576294\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 12.0438814163208 and accuracy is 0.5833333333333334\n",
      "inner loop training loss is 11.628462791442871 and accuracy is 0.75\n",
      "inner loop training loss is 11.320552825927734 and accuracy is 0.75\n",
      "inner loop training loss is 10.822057485580444 and accuracy is 0.9166666666666666\n",
      "inner loop training loss is 11.506050109863281 and accuracy is 0.5833333333333334\n",
      "outer loop training accuracy is 0.25 and loss is 13.553143501281738\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.687159538269043 and accuracy is 0.625\n",
      "inner loop training loss is 4.553664207458496 and accuracy is 0.75\n",
      "inner loop training loss is 5.243227005004883 and accuracy is 0.75\n",
      "inner loop training loss is 4.765778541564941 and accuracy is 0.75\n",
      "inner loop training loss is 4.248384475708008 and accuracy is 0.875\n",
      "outer loop training accuracy is 0.5 and loss is 5.50038480758667\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 5.188620567321777 and accuracy is 0.75\n",
      "inner loop training loss is 4.960515975952148 and accuracy is 0.5\n",
      "inner loop training loss is 5.029330730438232 and accuracy is 0.625\n",
      "inner loop training loss is 5.25617790222168 and accuracy is 0.75\n",
      "inner loop training loss is 4.980719089508057 and accuracy is 0.625\n",
      "outer loop training accuracy is 0.625 and loss is 5.4287309646606445\n",
      "Number of labels in the episode are 3 and lines are 1\n",
      "inner loop training loss is 10.156059741973877 and accuracy is 0.8333333333333334\n",
      "inner loop training loss is 10.406766176223755 and accuracy is 0.75\n",
      "inner loop training loss is 9.813308000564575 and accuracy is 0.75\n",
      "inner loop training loss is 9.61333966255188 and accuracy is 1.0\n",
      "inner loop training loss is 10.555377006530762 and accuracy is 0.8333333333333334\n",
      "outer loop training accuracy is 0.3333333333333333 and loss is 13.871827125549316\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 3.3985610008239746 and accuracy is 0.875\n",
      "inner loop training loss is 3.432232141494751 and accuracy is 0.875\n",
      "inner loop training loss is 3.4039924144744873 and accuracy is 0.875\n",
      "inner loop training loss is 5.106674671173096 and accuracy is 0.625\n",
      "inner loop training loss is 4.638412952423096 and accuracy is 0.75\n",
      "outer loop training accuracy is 0.25 and loss is 6.158435821533203\n",
      "Number of labels in the episode are 2 and lines are 1\n",
      "inner loop training loss is 4.552972793579102 and accuracy is 1.0\n",
      "inner loop training loss is 5.219995498657227 and accuracy is 0.75\n",
      "inner loop training loss is 4.766849040985107 and accuracy is 1.0\n",
      "inner loop training loss is 4.4038496017456055 and accuracy is 1.0\n",
      "inner loop training loss is 4.587236404418945 and accuracy is 1.0\n",
      "outer loop training accuracy is 0.375 and loss is 6.053606986999512\n",
      "outer loop accuracy is 0.39473684210526316 and total loss is tensor(70.0722, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "Number of labels in the episode are 2 and lines are 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/aksingh/Documents/meta-learned-lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m protomaml_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mProtoFOMAML\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_protomaml_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_protomaml_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mouterLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minnerLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmupSteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meta-learned-lines/training/trainer.py:40\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(modelType, train_loader, val_loader, seed, **args)\u001b[0m\n\u001b[1;32m     38\u001b[0m pl\u001b[38;5;241m.\u001b[39mseed_everything(seed)\n\u001b[1;32m     39\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_loader)\n\u001b[0;32m---> 40\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1531\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1459\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:60\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_checkpoint\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Union[Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m], Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     53\u001b[0m     checkpoint_path: Union[_PATH, IO],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 60\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     64\u001b[0m         checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:50\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     47\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/fsspec/spec.py:1199\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1198\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1199\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/fsspec/implementations/local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/fsspec/implementations/local.py:314\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/fsspec/implementations/local.py:319\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    321\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/aksingh/Documents/meta-learned-lines'"
     ]
    }
   ],
   "source": [
    "from training.trainer import train_model\n",
    "import torch.utils.data as data\n",
    "\n",
    "protomaml_model = train_model(\n",
    "    ProtoFOMAML,\n",
    "    train_loader=train_protomaml_loader,\n",
    "    val_loader=val_protomaml_loader,\n",
    "    outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=8, warmupSteps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "meta_ds = GLUEMetaDataset(k=4,numTasks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94698b12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training.models.ProtoNet import ProtoNet\n",
    "import torch.utils.data as data\n",
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from training.trainer import train_model\n",
    "\n",
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "from validation_datasets.GLUEMetaValidationDataset import GLUEMetaValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=4)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "validation_dataset = GLUEMetaValidationDataset(k=8, numTasks=5000)\n",
    "val_protomaml_loader = data.DataLoader(validation_dataset, num_workers=2)\n",
    "\n",
    "train_protonet_loader = data.DataLoader(\n",
    "    meta_ds, num_workers=4)\n",
    "\n",
    "protomaml_model = train_model(\n",
    "    ProtoNet,\n",
    "    train_loader=train_protonet_loader,\n",
    "    val_loader=val_protomaml_loader,\n",
    "    metaLearningRate=5e-5, prototypeLearningRate=1e-2, steps=5, batchSize=8, warmupSteps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169679d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
