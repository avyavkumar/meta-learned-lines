{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5482b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1004.46it/s]\n",
      "Found cached dataset glue (/home/aksingh/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1419.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from training_datasets.GLUEDataset import GLUEDataset\n",
    "\n",
    "cola = load_dataset('glue','cola')\n",
    "# print(cola)\n",
    "# sst2 = load_dataset('glue','sst2')\n",
    "# print(sst2)\n",
    "mrpc = load_dataset('glue', 'mrpc')\n",
    "# print(mrpc)\n",
    "pt_cola = GLUEDataset([cola, mrpc], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb229056",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from training_datasets.GLUEMetaDataset import GLUEMetaDataset\n",
    "from samplers.FewShotEpisodeSampler import FewShotEpisodeSampler\n",
    "from samplers.FewShotEpisodeBatchSampler import FewShotEpisodeBatchSampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "# sampler = FewShotEpisodeSampler(pt_cola, kShot=2, nWay=4, shuffle=True)\n",
    "# train_data_loader = data.DataLoader(\n",
    "#     pt_cola,\n",
    "#     batch_sampler=sampler,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "# data, targets = next(iter(train_data_loader))\n",
    "\n",
    "# for i in range(5):\n",
    "#     data, targets = next(iter(train_data_loader))\n",
    "#     print(data)\n",
    "#     print(targets)\n",
    "\n",
    "train_protomaml_sampler = FewShotEpisodeBatchSampler(pt_cola, kShot=3, nWay=4, batchSize=8, shuffle=True)\n",
    "train_protomaml_loader = data.DataLoader(\n",
    "    pt_cola, batch_sampler=train_protomaml_sampler, collate_fn=train_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_protomaml_loader))\n",
    "#     for episode_i in range(len(batch[0])):\n",
    "#         data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "#         supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2] \n",
    "#         print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300d2f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# episodes = []\n",
    "# for i in range(1):\n",
    "#     episodes.append(gm_ds.getTask())\n",
    "# for i in range(len(episodes)):\n",
    "#     classes = len(set(episodes[i][1].tolist()))\n",
    "#     # print(classes)\n",
    "#     print(episodes[i][0], episodes[i][1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec3922a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1001, 1.1574, 1.0943, 1.0817, 1.1202, 1.1844],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.738034248352051\n",
      "losses are tensor([1.0504, 1.1269, 1.0447, 1.0479, 1.0618, 1.1556],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.487463474273682\n",
      "losses are tensor([1.0056, 1.0982, 1.0023, 1.0167, 1.0097, 1.1286],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.261122703552246\n",
      "losses are tensor([0.9645, 1.0712, 0.9641, 0.9872, 0.9620, 1.1034],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.052452087402344\n",
      "losses are tensor([0.9280, 1.0454, 0.9306, 0.9593, 0.9197, 1.0794],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.862411022186279\n",
      "losses are tensor([0.8569, 1.1338, 0.9527, 1.3150, 1.1297, 1.1033],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.49141788482666\n",
      "losses are tensor([0.7838, 1.0822, 0.8850, 1.2682, 1.0851, 1.0573],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.161527633666992\n",
      "losses are tensor([0.7198, 1.0343, 0.8254, 1.2256, 1.0439, 1.0170],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.8658576011657715\n",
      "losses are tensor([0.6629, 0.9889, 0.7722, 1.1868, 1.0047, 0.9783],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.593874931335449\n",
      "losses are tensor([0.6116, 0.9463, 0.7241, 1.1506, 0.9671, 0.9419],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.341608047485352\n",
      "tensor([3., 3., 1., 0., 1., 2., 0., 3., 2., 0., 1., 2.])\n",
      "outer loop losses are tensor([1.5391, 1.5841, 1.1491, 1.1380, 1.5550, 1.0070],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 7.972301959991455\n",
      "local param grad is tensor([[-1.3130e-04,  1.0786e-04, -1.1229e-04,  ...,  8.1228e-06,\n",
      "          7.6855e-05, -6.1997e-05],\n",
      "        [-2.3402e-05, -1.6729e-05,  1.0845e-04,  ...,  1.5249e-05,\n",
      "         -3.3057e-05, -1.4483e-05],\n",
      "        [ 6.2630e-06,  3.1595e-05, -4.3934e-05,  ..., -2.9877e-05,\n",
      "         -7.9534e-05, -9.3631e-06],\n",
      "        ...,\n",
      "        [ 3.2432e-05, -1.9955e-04,  3.2829e-04,  ...,  1.4846e-04,\n",
      "         -9.0868e-05, -1.5991e-04],\n",
      "        [ 4.7471e-06,  7.6875e-05,  1.3965e-04,  ..., -1.1303e-05,\n",
      "         -1.0386e-04, -2.8000e-04],\n",
      "        [-4.5359e-04, -2.0356e-04,  6.3974e-04,  ...,  8.0740e-04,\n",
      "          2.8734e-04,  4.0020e-05]])\n",
      "local param grad is tensor([[-9.3819e-05,  7.6180e-05, -8.6483e-05,  ...,  7.0133e-06,\n",
      "          3.0573e-05, -6.6523e-05],\n",
      "        [-1.0962e-05, -8.3457e-06,  8.3775e-05,  ...,  1.2936e-06,\n",
      "         -3.2377e-05, -1.2105e-05],\n",
      "        [-1.5719e-05,  1.6513e-05, -5.3507e-05,  ..., -1.3517e-05,\n",
      "         -5.3660e-05, -9.2103e-06],\n",
      "        ...,\n",
      "        [-1.6564e-05, -1.6281e-04,  2.5020e-04,  ...,  1.6972e-04,\n",
      "         -1.5376e-05, -9.1065e-05],\n",
      "        [ 1.1941e-05,  8.6644e-05,  9.1521e-05,  ..., -4.0575e-05,\n",
      "         -7.8736e-05, -2.0456e-04],\n",
      "        [-3.6238e-04, -1.7653e-04,  5.7850e-04,  ...,  7.0572e-04,\n",
      "          2.8581e-04,  4.0861e-05]])\n",
      "meta param grad is tensor([[-2.2512e-04,  1.8404e-04, -1.9877e-04,  ...,  1.5136e-05,\n",
      "          1.0743e-04, -1.2852e-04],\n",
      "        [-3.4364e-05, -2.5075e-05,  1.9222e-04,  ...,  1.6543e-05,\n",
      "         -6.5434e-05, -2.6588e-05],\n",
      "        [-9.4556e-06,  4.8108e-05, -9.7441e-05,  ..., -4.3393e-05,\n",
      "         -1.3319e-04, -1.8573e-05],\n",
      "        ...,\n",
      "        [ 1.5868e-05, -3.6237e-04,  5.7849e-04,  ...,  3.1819e-04,\n",
      "         -1.0624e-04, -2.5097e-04],\n",
      "        [ 1.6688e-05,  1.6352e-04,  2.3118e-04,  ..., -5.1879e-05,\n",
      "         -1.8260e-04, -4.8457e-04],\n",
      "        [-8.1597e-04, -3.8009e-04,  1.2182e-03,  ...,  1.5131e-03,\n",
      "          5.7315e-04,  8.0880e-05]])\n",
      "[1, 1, 0, 2, 1, 2]\n",
      "[0. 2.]\n",
      "tensor([3., 3., 1., 0., 1., 2., 0., 3., 2., 0., 1., 2.])\n",
      "outer loop losses are tensor([1.0850, 1.0945, 1.5044, 1.5513, 1.0363, 1.4182],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 7.689769744873047\n",
      "local param grad is tensor([[ 8.6815e-05, -1.6483e-05, -5.9238e-05,  ..., -5.3958e-05,\n",
      "         -6.0712e-05,  2.0191e-05],\n",
      "        [-1.2418e-06,  6.6753e-05, -3.6476e-05,  ...,  1.1567e-05,\n",
      "         -4.6806e-05,  1.4349e-05],\n",
      "        [-2.3785e-06,  1.7761e-05, -1.0635e-05,  ...,  9.5921e-05,\n",
      "          2.3210e-07, -1.8011e-05],\n",
      "        ...,\n",
      "        [-7.0107e-05, -1.6587e-04,  1.7072e-04,  ...,  9.0687e-05,\n",
      "          2.0321e-06,  2.2304e-04],\n",
      "        [ 7.4550e-05,  1.1317e-04, -1.0323e-04,  ..., -7.8517e-06,\n",
      "         -8.1698e-05,  5.7544e-05],\n",
      "        [-9.1224e-05, -1.2591e-05,  2.0638e-04,  ...,  3.3891e-06,\n",
      "          1.3806e-05, -9.3325e-05]])\n",
      "local param grad is tensor([[ 1.0907e-04, -2.2304e-05, -7.7170e-05,  ..., -6.8218e-05,\n",
      "         -1.0288e-04,  3.2795e-05],\n",
      "        [-9.0787e-06,  9.3840e-05, -4.9303e-05,  ...,  4.0615e-05,\n",
      "         -8.9792e-05,  3.8118e-06],\n",
      "        [-5.3150e-06,  3.4212e-05, -8.2250e-06,  ...,  1.3391e-04,\n",
      "          5.2082e-05, -2.8586e-05],\n",
      "        ...,\n",
      "        [-1.5095e-04, -1.5820e-04,  2.1659e-04,  ...,  1.1543e-04,\n",
      "          1.0593e-04,  3.0303e-04],\n",
      "        [ 9.4521e-05,  1.6082e-04, -1.8162e-04,  ..., -3.2452e-06,\n",
      "         -1.0533e-04,  4.5158e-05],\n",
      "        [-1.9209e-04, -3.8002e-05,  3.6173e-04,  ...,  1.4643e-05,\n",
      "          5.7027e-05, -1.0095e-04]])\n",
      "meta param grad is tensor([[-2.9236e-05,  1.4525e-04, -3.3518e-04,  ..., -1.0704e-04,\n",
      "         -5.6165e-05, -7.5535e-05],\n",
      "        [-4.4685e-05,  1.3552e-04,  1.0645e-04,  ...,  6.8725e-05,\n",
      "         -2.0203e-04, -8.4277e-06],\n",
      "        [-1.7149e-05,  1.0008e-04, -1.1630e-04,  ...,  1.8644e-04,\n",
      "         -8.0881e-05, -6.5170e-05],\n",
      "        ...,\n",
      "        [-2.0519e-04, -6.8644e-04,  9.6581e-04,  ...,  5.2430e-04,\n",
      "          1.7141e-06,  2.7510e-04],\n",
      "        [ 1.8576e-04,  4.3751e-04, -5.3679e-05,  ..., -6.2975e-05,\n",
      "         -3.6962e-04, -3.8186e-04],\n",
      "        [-1.0993e-03, -4.3068e-04,  1.7864e-03,  ...,  1.5312e-03,\n",
      "          6.4399e-04, -1.1340e-04]])\n",
      "[3, 3, 0, 2, 3, 0]\n",
      "[1. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.2010, 1.0766, 1.1957, 1.0548, 1.1543, 1.1092],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.791520595550537\n",
      "losses are tensor([1.1651, 1.0390, 1.1543, 1.0097, 1.1080, 1.0720],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.5482177734375\n",
      "losses are tensor([1.1323, 1.0044, 1.1166, 0.9685, 1.0664, 1.0376],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.325778961181641\n",
      "losses are tensor([1.1029, 0.9724, 1.0814, 0.9307, 1.0289, 1.0053],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.1215338706970215\n",
      "losses are tensor([1.0754, 0.9436, 1.0495, 0.8951, 0.9938, 0.9768],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.934208869934082\n",
      "losses are tensor([1.2184, 1.0368, 1.1296, 1.0396, 1.0793, 0.9988],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.502643585205078\n",
      "losses are tensor([1.1732, 0.9748, 1.0725, 0.9757, 1.0244, 0.9556],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.176210403442383\n",
      "losses are tensor([1.1293, 0.9174, 1.0185, 0.9163, 0.9714, 0.9140],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.866948127746582\n",
      "losses are tensor([1.0865, 0.8644, 0.9683, 0.8624, 0.9200, 0.8741],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.575599193572998\n",
      "losses are tensor([1.0451, 0.8156, 0.9215, 0.8123, 0.8712, 0.8361],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.301823139190674\n",
      "tensor([0., 1., 2., 1., 2., 3., 2., 0., 0., 3., 3., 1.])\n",
      "outer loop losses are tensor([1.2033, 1.5633, 1.6328, 1.1685], grad_fn=<NllLossBackward0>) and loss is 5.567978858947754\n",
      "local param grad is tensor([[-1.4402e-05, -1.1063e-05, -9.0225e-06,  ..., -5.1796e-05,\n",
      "         -1.7888e-05, -6.1957e-06],\n",
      "        [-3.1441e-05,  4.9295e-05, -5.4102e-05,  ...,  3.1064e-05,\n",
      "         -3.2412e-05, -4.2360e-05],\n",
      "        [ 3.5309e-05, -1.8660e-05, -4.5604e-05,  ...,  1.3989e-04,\n",
      "          1.6313e-05,  1.2704e-04],\n",
      "        ...,\n",
      "        [-4.1098e-04, -4.8838e-05,  3.5787e-04,  ...,  3.5523e-04,\n",
      "          3.7890e-04, -3.5937e-06],\n",
      "        [ 5.3689e-05, -2.9095e-05, -4.1107e-04,  ..., -6.9111e-05,\n",
      "          9.6619e-05,  1.6676e-04],\n",
      "        [-1.7202e-04, -7.1340e-06,  3.5692e-04,  ...,  3.0229e-04,\n",
      "          1.7286e-04, -3.3743e-06]])\n",
      "local param grad is tensor([[-1.4323e-05, -9.2757e-06, -1.3817e-05,  ..., -5.0761e-05,\n",
      "         -3.0866e-05,  1.5461e-05],\n",
      "        [-4.1452e-05,  4.1817e-05, -6.3902e-05,  ...,  2.5782e-05,\n",
      "         -2.9745e-05, -4.5185e-05],\n",
      "        [ 3.5115e-05, -3.3537e-05, -5.5167e-05,  ...,  1.4866e-04,\n",
      "          3.0181e-05,  1.4453e-04],\n",
      "        ...,\n",
      "        [-4.7999e-04, -4.6683e-05,  4.0348e-04,  ...,  4.1302e-04,\n",
      "          4.5275e-04,  1.1458e-05],\n",
      "        [ 8.5834e-05, -4.0590e-05, -4.8148e-04,  ..., -1.1316e-04,\n",
      "          8.5031e-05,  1.5979e-04],\n",
      "        [-1.7383e-04, -1.9778e-05,  3.4847e-04,  ...,  3.3533e-04,\n",
      "          1.7257e-04,  1.0183e-05]])\n",
      "meta param grad is tensor([[-5.7961e-05,  1.2492e-04, -3.5802e-04,  ..., -2.0960e-04,\n",
      "         -1.0492e-04, -6.6269e-05],\n",
      "        [-1.1758e-04,  2.2663e-04, -1.1558e-05,  ...,  1.2557e-04,\n",
      "         -2.6419e-04, -9.5973e-05],\n",
      "        [ 5.3275e-05,  4.7885e-05, -2.1707e-04,  ...,  4.7499e-04,\n",
      "         -3.4387e-05,  2.0640e-04],\n",
      "        ...,\n",
      "        [-1.0962e-03, -7.8197e-04,  1.7272e-03,  ...,  1.2926e-03,\n",
      "          8.3336e-04,  2.8297e-04],\n",
      "        [ 3.2528e-04,  3.6782e-04, -9.4623e-04,  ..., -2.4525e-04,\n",
      "         -1.8797e-04, -5.5318e-05],\n",
      "        [-1.4451e-03, -4.5760e-04,  2.4917e-03,  ...,  2.1688e-03,\n",
      "          9.8941e-04, -1.0659e-04]])\n",
      "[0, 2, 2, 0]\n",
      "[0. 3.]\n",
      "tensor([0., 1., 2., 1., 2., 3., 2., 0., 0., 3., 3., 1.])\n",
      "outer loop losses are tensor([0.9875, 0.9470, 1.1233, 1.4081, 1.4494, 1.4027, 1.3699, 1.0616],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.749405860900879\n",
      "local param grad is tensor([[-8.8353e-05, -1.1694e-04, -5.4913e-05,  ...,  1.4256e-04,\n",
      "         -4.9581e-06,  3.8142e-05],\n",
      "        [-6.8715e-06,  6.4043e-05, -3.4988e-05,  ...,  4.5649e-05,\n",
      "         -5.0607e-05,  1.0087e-04],\n",
      "        [ 7.5586e-05, -5.2872e-05,  3.2512e-05,  ...,  1.4367e-04,\n",
      "         -3.0557e-05, -9.2512e-05],\n",
      "        ...,\n",
      "        [ 1.5470e-04, -8.6531e-05, -2.0512e-04,  ..., -1.0095e-04,\n",
      "         -1.8653e-04, -2.0957e-05],\n",
      "        [ 2.2574e-04,  1.2561e-04, -1.1240e-04,  ..., -3.3646e-04,\n",
      "         -9.3483e-05, -3.0705e-04],\n",
      "        [-1.3089e-05, -1.7171e-04,  2.0277e-04,  ..., -1.9046e-04,\n",
      "          4.1183e-04, -3.9113e-04]])\n",
      "local param grad is tensor([[-6.9634e-05, -1.0928e-04, -1.1734e-04,  ...,  1.0088e-04,\n",
      "         -4.0343e-05,  5.1907e-05],\n",
      "        [-4.6083e-06,  6.2769e-05,  1.4845e-05,  ...,  3.1994e-05,\n",
      "         -5.4354e-05,  9.3140e-05],\n",
      "        [ 5.1004e-05, -5.0616e-05,  3.5981e-05,  ...,  1.2274e-04,\n",
      "          3.3997e-06, -1.8332e-04],\n",
      "        ...,\n",
      "        [ 1.8176e-04, -1.4390e-04, -5.0552e-04,  ..., -1.8195e-04,\n",
      "         -2.3887e-04, -1.1229e-04],\n",
      "        [ 1.4894e-04,  1.6986e-04,  1.8082e-05,  ..., -1.9898e-04,\n",
      "         -4.7118e-05, -3.2611e-04],\n",
      "        [-1.9316e-04, -2.8812e-04,  3.6103e-04,  ..., -4.5512e-05,\n",
      "          6.7243e-04, -4.8147e-04]])\n",
      "meta param grad is tensor([[-2.1595e-04, -1.0131e-04, -5.3027e-04,  ...,  3.3834e-05,\n",
      "         -1.5022e-04,  2.3781e-05],\n",
      "        [-1.2906e-04,  3.5344e-04, -3.1700e-05,  ...,  2.0321e-04,\n",
      "         -3.6915e-04,  9.8035e-05],\n",
      "        [ 1.7987e-04, -5.5603e-05, -1.4858e-04,  ...,  7.4139e-04,\n",
      "         -6.1544e-05, -6.9428e-05],\n",
      "        ...,\n",
      "        [-7.5970e-04, -1.0124e-03,  1.0165e-03,  ...,  1.0097e-03,\n",
      "          4.0796e-04,  1.4972e-04],\n",
      "        [ 6.9997e-04,  6.6330e-04, -1.0405e-03,  ..., -7.8069e-04,\n",
      "         -3.2857e-04, -6.8848e-04],\n",
      "        [-1.6514e-03, -9.1743e-04,  3.0555e-03,  ...,  1.9328e-03,\n",
      "          2.0737e-03, -9.7919e-04]])\n",
      "[1, 2, 1, 3, 0, 3, 3, 1]\n",
      "[1. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1293, 1.1720, 1.1597, 1.1510, 1.1197, 1.1796],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.911285877227783\n",
      "losses are tensor([1.0853, 1.1433, 1.1287, 1.1053, 1.0789, 1.1502],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.691709995269775\n",
      "losses are tensor([1.0447, 1.1163, 1.0991, 1.0631, 1.0409, 1.1222],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.486149787902832\n",
      "losses are tensor([1.0069, 1.0910, 1.0713, 1.0236, 1.0052, 1.0958],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.293750762939453\n",
      "losses are tensor([0.9726, 1.0665, 1.0446, 0.9877, 0.9726, 1.0703],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.11430549621582\n",
      "losses are tensor([1.1257, 1.0788, 1.1248, 1.0671, 1.0795, 1.0817],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.557638168334961\n",
      "losses are tensor([1.0603, 1.0144, 1.0680, 1.0072, 1.0321, 1.0371],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.219048023223877\n",
      "losses are tensor([0.9978, 0.9584, 1.0133, 0.9535, 0.9862, 0.9966],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.9058990478515625\n",
      "losses are tensor([0.9377, 0.9104, 0.9602, 0.9053, 0.9415, 0.9596],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.61470365524292\n",
      "losses are tensor([0.8799, 0.8670, 0.9090, 0.8622, 0.8982, 0.9258],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.3420729637146\n",
      "tensor([3., 3., 2., 0., 2., 1., 0., 2., 0., 1., 1., 3.])\n",
      "outer loop losses are tensor([1.5352, 1.1520, 1.3794, 1.0075, 1.1553, 1.1826, 1.4487, 1.4160],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 10.276537895202637\n",
      "local param grad is tensor([[-1.5157e-04, -4.2882e-05,  6.6914e-05,  ...,  1.5507e-04,\n",
      "          1.1105e-05,  1.2302e-04],\n",
      "        [-5.1355e-05, -5.9602e-06, -3.7904e-05,  ...,  4.0547e-05,\n",
      "         -1.1699e-05,  9.7473e-05],\n",
      "        [-1.1716e-06, -1.4239e-04, -3.4563e-05,  ..., -2.0625e-05,\n",
      "         -2.4926e-05,  3.5127e-05],\n",
      "        ...,\n",
      "        [ 6.3103e-05, -1.9909e-04, -2.3780e-04,  ..., -1.4078e-04,\n",
      "         -2.1200e-04, -1.3744e-04],\n",
      "        [-1.1607e-04,  1.8930e-04,  1.5952e-05,  ...,  3.8336e-04,\n",
      "          1.6325e-04,  1.0392e-04],\n",
      "        [-8.8226e-05, -1.0459e-04,  2.5150e-04,  ..., -1.6144e-06,\n",
      "          1.8644e-04, -3.1076e-04]])\n",
      "local param grad is tensor([[-1.4738e-04, -3.4349e-05,  6.2799e-05,  ...,  1.1866e-04,\n",
      "         -7.5088e-07,  1.1720e-04],\n",
      "        [-4.7242e-05, -3.1130e-05, -4.5508e-05,  ...,  3.5371e-05,\n",
      "         -3.5055e-05,  1.4339e-04],\n",
      "        [-2.8898e-06, -9.3918e-05, -7.1679e-05,  ..., -3.2866e-05,\n",
      "          2.0034e-05,  1.9822e-05],\n",
      "        ...,\n",
      "        [-4.4287e-06, -1.7485e-04, -1.5283e-04,  ..., -7.4915e-05,\n",
      "         -1.5538e-04, -1.2745e-04],\n",
      "        [-3.2293e-05,  1.3975e-04, -7.1898e-05,  ...,  3.7090e-04,\n",
      "          1.1137e-04,  7.6143e-05],\n",
      "        [-2.0414e-04, -1.3848e-04,  3.8630e-04,  ...,  2.0508e-04,\n",
      "          2.7287e-04, -3.3941e-04]])\n",
      "meta param grad is tensor([[-5.1489e-04, -1.7854e-04, -4.0056e-04,  ...,  3.0756e-04,\n",
      "         -1.3987e-04,  2.6400e-04],\n",
      "        [-2.2765e-04,  3.1635e-04, -1.1511e-04,  ...,  2.7913e-04,\n",
      "         -4.1590e-04,  3.3889e-04],\n",
      "        [ 1.7580e-04, -2.9191e-04, -2.5482e-04,  ...,  6.8790e-04,\n",
      "         -6.6437e-05, -1.4479e-05],\n",
      "        ...,\n",
      "        [-7.0103e-04, -1.3863e-03,  6.2590e-04,  ...,  7.9396e-04,\n",
      "          4.0580e-05, -1.1517e-04],\n",
      "        [ 5.5161e-04,  9.9235e-04, -1.0965e-03,  ..., -2.6427e-05,\n",
      "         -5.3954e-05, -5.0842e-04],\n",
      "        [-1.9437e-03, -1.1605e-03,  3.6933e-03,  ...,  2.1363e-03,\n",
      "          2.5330e-03, -1.6294e-03]])\n",
      "[3, 0, 1, 0, 2, 0, 1, 1]\n",
      "[0. 2.]\n",
      "tensor([3., 3., 2., 0., 2., 1., 0., 2., 0., 1., 1., 3.])\n",
      "outer loop losses are tensor([1.0960, 1.4095, 1.4477, 1.1023], grad_fn=<NllLossBackward0>) and loss is 5.055449962615967\n",
      "local param grad is tensor([[ 2.6085e-05,  6.3479e-06, -1.0045e-04,  ..., -6.9157e-05,\n",
      "          1.3968e-05, -8.8158e-06],\n",
      "        [ 8.5674e-06, -3.0864e-06, -6.3634e-05,  ..., -4.9915e-05,\n",
      "         -6.5773e-05,  1.0297e-05],\n",
      "        [ 2.9343e-05, -8.4888e-05,  3.5669e-05,  ...,  2.3811e-05,\n",
      "         -6.0188e-05,  8.5411e-05],\n",
      "        ...,\n",
      "        [-2.5565e-04, -1.0568e-04,  5.3771e-04,  ...,  3.1089e-04,\n",
      "          2.6065e-04,  2.1807e-04],\n",
      "        [ 1.7104e-04,  9.9990e-05, -3.2595e-04,  ..., -2.6023e-04,\n",
      "         -2.0074e-04, -1.6159e-04],\n",
      "        [-7.3635e-05,  3.4612e-05,  1.3907e-03,  ...,  6.7764e-04,\n",
      "          8.1115e-04, -2.1525e-04]])\n",
      "local param grad is tensor([[ 5.2850e-05, -2.3359e-07, -1.5224e-04,  ..., -1.0845e-04,\n",
      "          3.1024e-06, -5.6378e-06],\n",
      "        [ 1.3524e-05, -2.3627e-06, -1.1970e-04,  ..., -1.0066e-04,\n",
      "         -1.0043e-04,  1.1301e-05],\n",
      "        [ 4.1125e-05, -1.8845e-04,  5.6144e-05,  ...,  7.6416e-05,\n",
      "         -8.2850e-05,  1.6231e-04],\n",
      "        ...,\n",
      "        [-6.1121e-04, -2.7373e-04,  1.2665e-03,  ...,  8.2124e-04,\n",
      "          5.6056e-04,  4.7657e-04],\n",
      "        [ 4.0545e-04,  1.7135e-04, -7.3865e-04,  ..., -5.6495e-04,\n",
      "         -4.6192e-04, -3.1827e-04],\n",
      "        [-2.6499e-04,  7.7951e-05,  2.5944e-03,  ...,  1.3308e-03,\n",
      "          1.4102e-03, -2.7927e-04]])\n",
      "meta param grad is tensor([[-0.0004, -0.0002, -0.0007,  ...,  0.0001, -0.0001,  0.0002],\n",
      "        [-0.0002,  0.0003, -0.0003,  ...,  0.0001, -0.0006,  0.0004],\n",
      "        [ 0.0002, -0.0006, -0.0002,  ...,  0.0008, -0.0002,  0.0002],\n",
      "        ...,\n",
      "        [-0.0016, -0.0018,  0.0024,  ...,  0.0019,  0.0009,  0.0006],\n",
      "        [ 0.0011,  0.0013, -0.0022,  ..., -0.0009, -0.0007, -0.0010],\n",
      "        [-0.0023, -0.0010,  0.0077,  ...,  0.0041,  0.0048, -0.0021]])\n",
      "[3, 2, 2, 3]\n",
      "[1. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.0486, 1.0783, 1.0893, 1.1878, 1.0958, 1.0511],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.550873756408691\n",
      "losses are tensor([0.9896, 1.0149, 1.0348, 1.1512, 1.0521, 1.0098],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.2524333000183105\n",
      "losses are tensor([0.9370, 0.9581, 0.9862, 1.1174, 1.0128, 0.9716],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.983054161071777\n",
      "losses are tensor([0.8905, 0.9073, 0.9425, 1.0860, 0.9766, 0.9362],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.739190578460693\n",
      "losses are tensor([0.8491, 0.8621, 0.9042, 1.0577, 0.9434, 0.9045],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.520956039428711\n",
      "losses are tensor([1.0880, 0.9464, 0.9025, 1.0254, 1.0493, 0.9869],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.998319149017334\n",
      "losses are tensor([0.9894, 0.8268, 0.7972, 0.9650, 0.9785, 0.9097],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.46663761138916\n",
      "losses are tensor([0.9052, 0.7264, 0.7084, 0.9078, 0.9125, 0.8384],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.998780727386475\n",
      "losses are tensor([0.8327, 0.6420, 0.6335, 0.8527, 0.8494, 0.7711],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.581370830535889\n",
      "losses are tensor([0.7682, 0.5686, 0.5683, 0.8008, 0.7910, 0.7090],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 4.205869197845459\n",
      "tensor([0., 2., 2., 3., 3., 0., 2., 0., 1., 1., 3., 1.])\n",
      "outer loop losses are tensor([0.9797, 1.0274, 1.5249, 0.9570, 1.4573, 1.4222, 1.5316, 1.4830],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 10.382981300354004\n",
      "local param grad is tensor([[-5.9493e-05, -4.4923e-05,  3.9343e-05,  ...,  1.2611e-04,\n",
      "         -1.3975e-04, -1.7361e-04],\n",
      "        [ 6.8121e-05,  7.1177e-05, -2.3425e-05,  ..., -7.1721e-05,\n",
      "         -6.9480e-05, -8.1316e-05],\n",
      "        [ 3.7290e-05, -1.3876e-04,  3.0303e-05,  ..., -1.3674e-04,\n",
      "          1.6546e-04,  1.9944e-04],\n",
      "        ...,\n",
      "        [ 6.6992e-04, -2.8750e-04,  4.5676e-04,  ...,  8.4542e-05,\n",
      "         -1.4998e-06,  2.6491e-04],\n",
      "        [ 6.9255e-04, -1.4665e-04, -6.1417e-04,  ..., -9.3669e-05,\n",
      "         -2.0633e-04, -4.0159e-04],\n",
      "        [-5.8825e-04, -3.7284e-04,  1.5432e-03,  ...,  4.2532e-04,\n",
      "          1.3346e-03,  4.5820e-04]])\n",
      "local param grad is tensor([[-5.2694e-05, -1.7267e-05,  2.1511e-06,  ...,  9.7820e-05,\n",
      "         -1.2196e-04, -1.5404e-04],\n",
      "        [ 6.6674e-05,  6.8662e-05, -3.2741e-05,  ..., -9.9748e-05,\n",
      "         -6.9344e-05, -7.8100e-05],\n",
      "        [ 5.6517e-05, -1.0179e-04,  2.3014e-05,  ..., -1.3123e-04,\n",
      "          1.4198e-04,  2.7461e-04],\n",
      "        ...,\n",
      "        [ 7.7392e-04, -3.1213e-04,  3.6822e-04,  ...,  4.1941e-05,\n",
      "          9.5408e-05,  4.3305e-04],\n",
      "        [ 8.4395e-04, -1.3793e-04, -7.9184e-04,  ..., -2.7099e-04,\n",
      "         -3.1223e-04, -3.3015e-04],\n",
      "        [-7.9817e-04, -4.3460e-04,  1.7675e-03,  ...,  6.3060e-04,\n",
      "          1.4436e-03,  6.1625e-04]])\n",
      "meta param grad is tensor([[-5.4814e-04, -2.3462e-04, -6.1175e-04,  ...,  3.5388e-04,\n",
      "         -3.8451e-04, -7.8102e-05],\n",
      "        [-7.0768e-05,  4.5074e-04, -3.5461e-04,  ..., -4.2907e-05,\n",
      "         -7.2093e-04,  2.0108e-04],\n",
      "        [ 3.4008e-04, -8.0579e-04, -1.0969e-04,  ...,  5.2016e-04,\n",
      "          9.7963e-05,  7.0729e-04],\n",
      "        ...,\n",
      "        [-1.2404e-04, -2.3654e-03,  3.2551e-03,  ...,  2.0526e-03,\n",
      "          9.5570e-04,  1.2774e-03],\n",
      "        [ 2.6646e-03,  9.7911e-04, -3.5671e-03,  ..., -1.2163e-03,\n",
      "         -1.2352e-03, -1.7200e-03],\n",
      "        [-3.6688e-03, -1.8554e-03,  1.0989e-02,  ...,  5.2007e-03,\n",
      "          7.5325e-03, -1.0494e-03]])\n",
      "[0, 2, 3, 0, 1, 1, 3, 1]\n",
      "[0. 2.]\n",
      "tensor([0., 2., 2., 3., 3., 0., 2., 0., 1., 1., 3., 1.])\n",
      "outer loop losses are tensor([1.3692, 1.3342, 1.3116, 1.3828], grad_fn=<NllLossBackward0>) and loss is 5.397810935974121\n",
      "local param grad is tensor([[ 5.9879e-05, -7.7212e-07,  1.6487e-05,  ...,  3.0178e-05,\n",
      "          6.8882e-05,  1.7813e-06],\n",
      "        [ 2.6952e-05, -1.1902e-05,  2.9067e-06,  ..., -1.3762e-05,\n",
      "          1.6598e-05,  2.3065e-05],\n",
      "        [-1.0424e-06, -4.8435e-06, -4.6175e-05,  ...,  1.3242e-05,\n",
      "         -9.7455e-05,  9.7105e-05],\n",
      "        ...,\n",
      "        [ 5.0421e-04,  1.1265e-05, -2.5301e-04,  ..., -1.1326e-04,\n",
      "         -2.1303e-04,  1.0209e-04],\n",
      "        [ 8.2857e-05,  9.6795e-05, -8.3568e-05,  ..., -1.9041e-04,\n",
      "          1.5888e-04, -2.8974e-05],\n",
      "        [ 1.6598e-04, -1.7482e-04, -1.7888e-04,  ...,  5.8397e-06,\n",
      "         -9.3469e-05, -5.5758e-05]])\n",
      "local param grad is tensor([[ 2.3331e-05, -5.5085e-06,  5.0687e-05,  ...,  1.0192e-04,\n",
      "          1.2559e-04,  3.3862e-06],\n",
      "        [ 1.1720e-05, -1.3232e-05,  1.2330e-05,  ..., -1.2476e-05,\n",
      "          4.0946e-05,  3.6026e-05],\n",
      "        [-1.5751e-05,  1.4835e-05, -5.3651e-05,  ...,  6.9621e-06,\n",
      "         -1.4935e-04,  1.1484e-04],\n",
      "        ...,\n",
      "        [ 7.9219e-04,  4.6224e-06, -4.1280e-04,  ..., -2.5459e-04,\n",
      "         -4.1438e-04,  9.7733e-05],\n",
      "        [ 5.8255e-06,  1.0573e-04, -4.0632e-05,  ..., -1.7901e-04,\n",
      "          2.5394e-04,  6.2637e-05],\n",
      "        [ 4.9443e-04, -2.4867e-04, -4.5500e-04,  ..., -2.5606e-04,\n",
      "         -3.1958e-04, -1.0184e-04]])\n",
      "meta param grad is tensor([[-4.6493e-04, -2.4090e-04, -5.4458e-04,  ...,  4.8598e-04,\n",
      "         -1.9003e-04, -7.2934e-05],\n",
      "        [-3.2097e-05,  4.2561e-04, -3.3938e-04,  ..., -6.9145e-05,\n",
      "         -6.6338e-04,  2.6017e-04],\n",
      "        [ 3.2329e-04, -7.9580e-04, -2.0952e-04,  ...,  5.4036e-04,\n",
      "         -1.4884e-04,  9.1924e-04],\n",
      "        ...,\n",
      "        [ 1.1724e-03, -2.3495e-03,  2.5893e-03,  ...,  1.6847e-03,\n",
      "          3.2829e-04,  1.4772e-03],\n",
      "        [ 2.7533e-03,  1.1816e-03, -3.6913e-03,  ..., -1.5857e-03,\n",
      "         -8.2235e-04, -1.6864e-03],\n",
      "        [-3.0084e-03, -2.2789e-03,  1.0355e-02,  ...,  4.9505e-03,\n",
      "          7.1195e-03, -1.2070e-03]])\n",
      "[2, 3, 2, 0]\n",
      "[1. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1857, 1.0574, 1.1733, 0.9701, 1.1890, 1.1497, 1.1598, 1.2118, 1.0963],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 10.193069458007812\n",
      "losses are tensor([1.1715, 0.9862, 1.1557, 0.8952, 1.1827, 1.1402, 1.1512, 1.1891, 1.0380],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.909817695617676\n",
      "losses are tensor([1.1599, 0.9254, 1.1425, 0.8328, 1.1748, 1.1295, 1.1412, 1.1718, 0.9888],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.666851997375488\n",
      "losses are tensor([1.1507, 0.8730, 1.1326, 0.7803, 1.1655, 1.1178, 1.1301, 1.1581, 0.9459],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.454046249389648\n",
      "losses are tensor([1.1424, 0.8270, 1.1241, 0.7352, 1.1546, 1.1048, 1.1175, 1.1458, 0.9078],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.259134292602539\n",
      "losses are tensor([1.0367, 1.2022, 0.9988], grad_fn=<NllLossBackward0>) and loss is 3.2377614974975586\n",
      "losses are tensor([0.9648, 1.1199, 0.9182], grad_fn=<NllLossBackward0>) and loss is 3.0029163360595703\n",
      "losses are tensor([0.8992, 1.0440, 0.8447], grad_fn=<NllLossBackward0>) and loss is 2.7878990173339844\n",
      "losses are tensor([0.8396, 0.9754, 0.7781], grad_fn=<NllLossBackward0>) and loss is 2.593179225921631\n",
      "losses are tensor([0.7852, 0.9120, 0.7179], grad_fn=<NllLossBackward0>) and loss is 2.415064811706543\n",
      "tensor([1., 2., 2., 0., 3., 1., 3., 0., 1., 2., 0., 3.])\n",
      "outer loop losses are tensor([1.4677, 1.2051, 1.2236, 1.2755, 1.1413, 1.5113, 1.2456, 1.6606, 1.2373,\n",
      "        1.2760, 1.2388], grad_fn=<NllLossBackward0>) and loss is 14.482869148254395\n",
      "local param grad is tensor([[-2.1739e-05, -2.3874e-05,  7.4111e-05,  ...,  1.4091e-04,\n",
      "          5.3846e-05, -7.6469e-05],\n",
      "        [ 2.3564e-05, -1.5100e-04,  6.9828e-05,  ...,  8.8676e-05,\n",
      "          6.7624e-05, -8.5147e-05],\n",
      "        [ 2.0358e-05, -4.4554e-05,  1.0322e-04,  ...,  4.6014e-05,\n",
      "          3.7468e-05,  6.0442e-05],\n",
      "        ...,\n",
      "        [ 2.5839e-04, -1.2308e-05, -1.0042e-03,  ..., -7.7315e-04,\n",
      "         -4.2578e-05, -2.8788e-04],\n",
      "        [ 6.7663e-04,  4.4171e-04, -7.1979e-04,  ..., -6.8969e-04,\n",
      "         -4.9481e-04, -5.0883e-04],\n",
      "        [-6.4646e-04, -9.0237e-05, -6.1154e-04,  ..., -9.2698e-04,\n",
      "          2.5541e-04, -1.2920e-05]])\n",
      "local param grad is tensor([[-2.1814e-06, -1.0946e-05,  8.2305e-05,  ...,  1.0624e-04,\n",
      "          6.4328e-05, -6.2253e-05],\n",
      "        [ 2.5892e-05, -1.3687e-04,  5.5775e-05,  ...,  9.9884e-05,\n",
      "          9.8830e-05, -7.1022e-05],\n",
      "        [ 1.8570e-05, -4.7859e-05,  7.6559e-05,  ...,  3.0974e-05,\n",
      "          3.1986e-05,  8.2701e-05],\n",
      "        ...,\n",
      "        [ 1.9060e-04,  1.1989e-05, -9.1689e-04,  ..., -7.3978e-04,\n",
      "         -7.4290e-06, -3.4777e-04],\n",
      "        [ 5.1336e-04,  2.9681e-04, -5.7143e-04,  ..., -5.6566e-04,\n",
      "         -3.5299e-04, -4.1319e-04],\n",
      "        [-6.1033e-04,  6.7512e-05, -5.2145e-04,  ..., -9.3448e-04,\n",
      "          1.7462e-04, -6.2019e-05]])\n",
      "meta param grad is tensor([[-4.8885e-04, -2.7572e-04, -3.8816e-04,  ...,  7.3313e-04,\n",
      "         -7.1859e-05, -2.1166e-04],\n",
      "        [ 1.7359e-05,  1.3773e-04, -2.1377e-04,  ...,  1.1942e-04,\n",
      "         -4.9693e-04,  1.0400e-04],\n",
      "        [ 3.6221e-04, -8.8821e-04, -2.9740e-05,  ...,  6.1735e-04,\n",
      "         -7.9383e-05,  1.0624e-03],\n",
      "        ...,\n",
      "        [ 1.6214e-03, -2.3498e-03,  6.6816e-04,  ...,  1.7180e-04,\n",
      "          2.7828e-04,  8.4160e-04],\n",
      "        [ 3.9433e-03,  1.9201e-03, -4.9825e-03,  ..., -2.8411e-03,\n",
      "         -1.6702e-03, -2.6084e-03],\n",
      "        [-4.2652e-03, -2.3016e-03,  9.2222e-03,  ...,  3.0890e-03,\n",
      "          7.5495e-03, -1.2820e-03]])\n",
      "[1, 2, 2, 0, 3, 1, 3, 1, 2, 0, 3]\n",
      "[0. 2. 3.]\n",
      "tensor([1., 2., 2., 0., 3., 1., 3., 0., 1., 2., 0., 3.])\n",
      "outer loop losses are tensor([1.3836], grad_fn=<NllLossBackward0>) and loss is 1.3835859298706055\n",
      "local param grad is tensor([[ 6.5025e-06, -1.6902e-05,  5.8510e-06,  ...,  3.0016e-05,\n",
      "         -7.2066e-06,  5.4673e-06],\n",
      "        [ 5.8643e-06, -9.6738e-06,  5.7735e-06,  ...,  2.2252e-05,\n",
      "          2.1124e-06,  1.4318e-05],\n",
      "        [-5.9209e-06, -3.3323e-06,  2.2770e-06,  ...,  8.8967e-07,\n",
      "         -2.5260e-06, -3.5274e-06],\n",
      "        ...,\n",
      "        [-1.4175e-06, -3.9721e-05, -3.4994e-06,  ...,  6.7469e-05,\n",
      "         -5.7671e-05,  2.8583e-05],\n",
      "        [-4.7111e-08,  1.3830e-05,  2.7762e-06,  ..., -2.5146e-05,\n",
      "          1.8877e-05, -6.1547e-06],\n",
      "        [ 7.9213e-06, -1.5719e-05,  1.0342e-06,  ...,  2.1601e-05,\n",
      "         -2.3636e-05,  7.4435e-06]])\n",
      "local param grad is tensor([[ 6.5025e-06, -1.6902e-05,  5.8510e-06,  ...,  3.0016e-05,\n",
      "         -7.2066e-06,  5.4673e-06],\n",
      "        [ 5.8643e-06, -9.6738e-06,  5.7735e-06,  ...,  2.2252e-05,\n",
      "          2.1124e-06,  1.4318e-05],\n",
      "        [-5.9209e-06, -3.3323e-06,  2.2770e-06,  ...,  8.8967e-07,\n",
      "         -2.5260e-06, -3.5274e-06],\n",
      "        ...,\n",
      "        [-1.4175e-06, -3.9721e-05, -3.4994e-06,  ...,  6.7469e-05,\n",
      "         -5.7671e-05,  2.8583e-05],\n",
      "        [-4.7111e-08,  1.3830e-05,  2.7762e-06,  ..., -2.5146e-05,\n",
      "          1.8877e-05, -6.1547e-06],\n",
      "        [ 7.9213e-06, -1.5719e-05,  1.0342e-06,  ...,  2.1601e-05,\n",
      "         -2.3636e-05,  7.4435e-06]])\n",
      "meta param grad is tensor([[-4.7585e-04, -3.0952e-04, -3.7646e-04,  ...,  7.9316e-04,\n",
      "         -8.6272e-05, -2.0072e-04],\n",
      "        [ 2.9088e-05,  1.1838e-04, -2.0223e-04,  ...,  1.6392e-04,\n",
      "         -4.9270e-04,  1.3264e-04],\n",
      "        [ 3.5037e-04, -8.9488e-04, -2.5186e-05,  ...,  6.1913e-04,\n",
      "         -8.4435e-05,  1.0553e-03],\n",
      "        ...,\n",
      "        [ 1.6185e-03, -2.4293e-03,  6.6116e-04,  ...,  3.0674e-04,\n",
      "          1.6294e-04,  8.9876e-04],\n",
      "        [ 3.9432e-03,  1.9478e-03, -4.9770e-03,  ..., -2.8913e-03,\n",
      "         -1.6324e-03, -2.6207e-03],\n",
      "        [-4.2493e-03, -2.3330e-03,  9.2243e-03,  ...,  3.1322e-03,\n",
      "          7.5023e-03, -1.2671e-03]])\n",
      "[0]\n",
      "[1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.0977, 1.0635, 1.1239, 1.1353, 1.1815, 1.1581],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.75984525680542\n",
      "losses are tensor([1.0152, 0.9888, 1.0603, 1.0787, 1.1261, 1.0885],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.357565879821777\n",
      "losses are tensor([0.9476, 0.9275, 1.0072, 1.0314, 1.0787, 1.0328],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.025228023529053\n",
      "losses are tensor([0.8908, 0.8755, 0.9628, 0.9913, 1.0376, 0.9857],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.743682384490967\n",
      "losses are tensor([0.8442, 0.8328, 0.9250, 0.9575, 1.0019, 0.9465],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.507814884185791\n",
      "losses are tensor([1.1342, 1.1369, 1.1578, 1.0710, 1.0746, 1.0717],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.646263122558594\n",
      "losses are tensor([1.0602, 1.0653, 1.0872, 0.9751, 0.9825, 1.0050],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.175422668457031\n",
      "losses are tensor([1.0041, 1.0098, 1.0325, 0.9040, 0.9140, 0.9533],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.8176655769348145\n",
      "losses are tensor([0.9583, 0.9655, 0.9881, 0.8474, 0.8604, 0.9112],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.530882358551025\n",
      "losses are tensor([0.9205, 0.9283, 0.9510, 0.8009, 0.8170, 0.8767],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.294518947601318\n",
      "tensor([1., 1., 2., 0., 0., 3., 3., 1., 2., 3., 2., 0.])\n",
      "outer loop losses are tensor([1.1146, 1.1135, 0.9960, 1.0098, 1.1912, 1.0189],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.444209098815918\n",
      "local param grad is tensor([[-6.0144e-05, -3.8247e-05,  3.7034e-05,  ...,  2.6339e-05,\n",
      "         -2.2781e-05, -3.3000e-05],\n",
      "        [ 1.2884e-06, -9.1902e-06,  4.6931e-05,  ..., -1.3592e-05,\n",
      "          1.1059e-05, -7.5890e-05],\n",
      "        [-4.5531e-05,  3.6853e-06, -2.7429e-05,  ...,  1.8121e-05,\n",
      "         -1.7408e-05,  4.7560e-05],\n",
      "        ...,\n",
      "        [ 1.4943e-04, -6.1899e-05, -6.1696e-05,  ..., -2.6324e-04,\n",
      "         -8.1118e-05,  2.0228e-05],\n",
      "        [ 1.6384e-04, -8.2631e-05, -1.7000e-04,  ..., -2.7186e-04,\n",
      "         -1.9053e-04,  4.4571e-05],\n",
      "        [-4.7440e-05,  1.6341e-04,  8.5141e-06,  ..., -1.1385e-04,\n",
      "         -6.2934e-05, -5.1203e-04]])\n",
      "local param grad is tensor([[-4.2578e-05, -3.2345e-05,  3.9623e-05,  ...,  2.2653e-05,\n",
      "         -9.7810e-06, -2.2350e-05],\n",
      "        [-2.4685e-06, -5.3625e-06,  4.6311e-05,  ..., -5.6887e-06,\n",
      "          1.3186e-05, -5.7284e-05],\n",
      "        [-3.9986e-05,  1.0125e-05, -3.1306e-05,  ...,  1.5987e-05,\n",
      "         -1.7067e-05,  2.8870e-05],\n",
      "        ...,\n",
      "        [ 1.3674e-04, -6.0900e-05, -1.5617e-05,  ..., -2.1914e-04,\n",
      "         -6.7640e-05,  3.0679e-05],\n",
      "        [ 1.3340e-04, -6.7171e-05, -1.1789e-04,  ..., -2.3355e-04,\n",
      "         -1.4824e-04,  4.3294e-05],\n",
      "        [-3.2677e-05,  1.5365e-04, -2.2591e-05,  ..., -1.1018e-04,\n",
      "         -5.1247e-05, -4.3632e-04]])\n",
      "meta param grad is tensor([[-5.7857e-04, -3.8012e-04, -2.9980e-04,  ...,  8.4215e-04,\n",
      "         -1.1883e-04, -2.5607e-04],\n",
      "        [ 2.7908e-05,  1.0383e-04, -1.0899e-04,  ...,  1.4464e-04,\n",
      "         -4.6846e-04, -5.3849e-07],\n",
      "        [ 2.6486e-04, -8.8107e-04, -8.3921e-05,  ...,  6.5324e-04,\n",
      "         -1.1891e-04,  1.1318e-03],\n",
      "        ...,\n",
      "        [ 1.9047e-03, -2.5521e-03,  5.8385e-04,  ..., -1.7564e-04,\n",
      "          1.4181e-05,  9.4967e-04],\n",
      "        [ 4.2404e-03,  1.7980e-03, -5.2649e-03,  ..., -3.3967e-03,\n",
      "         -1.9712e-03, -2.5328e-03],\n",
      "        [-4.3294e-03, -2.0160e-03,  9.2102e-03,  ...,  2.9082e-03,\n",
      "          7.3881e-03, -2.2154e-03]])\n",
      "[1, 1, 0, 0, 1, 0]\n",
      "[0. 1.]\n",
      "tensor([1., 1., 2., 0., 0., 3., 3., 1., 2., 3., 2., 0.])\n",
      "outer loop losses are tensor([1.0495, 0.9552, 0.9897, 0.8995, 1.0493, 1.0483],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.9914703369140625\n",
      "local param grad is tensor([[ 2.4612e-05, -4.3521e-05, -5.3854e-06,  ...,  5.3763e-06,\n",
      "         -5.7718e-06,  8.9285e-06],\n",
      "        [ 1.3925e-05,  2.8205e-05,  4.3582e-05,  ...,  3.7084e-06,\n",
      "          1.9554e-05,  2.5550e-06],\n",
      "        [ 4.2149e-05, -6.8673e-05, -3.6453e-05,  ..., -2.6189e-05,\n",
      "          2.7717e-05,  1.8858e-05],\n",
      "        ...,\n",
      "        [ 1.0754e-04,  4.5418e-05,  2.7925e-04,  ...,  9.6181e-05,\n",
      "         -4.5078e-05, -6.6352e-05],\n",
      "        [ 1.1144e-04,  3.6492e-05, -2.1633e-04,  ..., -8.2696e-05,\n",
      "         -1.4654e-05,  6.7495e-05],\n",
      "        [-2.7635e-04, -1.5094e-04,  3.2172e-04,  ...,  1.0508e-04,\n",
      "          8.6166e-05, -4.8817e-05]])\n",
      "local param grad is tensor([[ 3.1093e-05, -4.1374e-05,  3.1369e-06,  ...,  1.7615e-05,\n",
      "         -6.8808e-06,  1.1143e-05],\n",
      "        [ 1.5878e-05,  3.6548e-05,  6.1995e-05,  ...,  1.6785e-05,\n",
      "          8.3307e-06,  2.4453e-06],\n",
      "        [ 6.9250e-05, -7.3788e-05, -4.0463e-05,  ..., -1.8642e-05,\n",
      "          3.8234e-05,  3.6903e-05],\n",
      "        ...,\n",
      "        [ 1.1631e-04,  3.7439e-05,  3.6138e-04,  ...,  1.9918e-04,\n",
      "         -1.1571e-06, -5.6337e-05],\n",
      "        [ 1.2507e-04,  4.3315e-05, -2.8346e-04,  ..., -1.6502e-04,\n",
      "         -7.3796e-05,  5.6493e-05],\n",
      "        [-2.7559e-04, -1.6190e-04,  3.9902e-04,  ...,  2.2396e-04,\n",
      "          1.4954e-04, -4.9487e-05]])\n",
      "meta param grad is tensor([[-5.2287e-04, -4.6501e-04, -3.0205e-04,  ...,  8.6515e-04,\n",
      "         -1.3149e-04, -2.3600e-04],\n",
      "        [ 5.7711e-05,  1.6858e-04, -3.4083e-06,  ...,  1.6513e-04,\n",
      "         -4.4057e-04,  4.4619e-06],\n",
      "        [ 3.7625e-04, -1.0235e-03, -1.6084e-04,  ...,  6.0841e-04,\n",
      "         -5.2960e-05,  1.1875e-03],\n",
      "        ...,\n",
      "        [ 2.1285e-03, -2.4692e-03,  1.2245e-03,  ...,  1.1972e-04,\n",
      "         -3.2054e-05,  8.2698e-04],\n",
      "        [ 4.4769e-03,  1.8778e-03, -5.7646e-03,  ..., -3.6445e-03,\n",
      "         -2.0596e-03, -2.4088e-03],\n",
      "        [-4.8814e-03, -2.3288e-03,  9.9310e-03,  ...,  3.2372e-03,\n",
      "          7.6238e-03, -2.3137e-03]])\n",
      "[2, 3, 3, 2, 3, 2]\n",
      "[2. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.2302, 1.1767, 1.1402, 1.0605, 1.1634, 1.1744],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.945544719696045\n",
      "losses are tensor([1.1802, 1.1276, 1.0822, 1.0030, 1.1151, 1.1276],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.6356329917907715\n",
      "losses are tensor([1.1351, 1.0818, 1.0289, 0.9506, 1.0702, 1.0850],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.351590156555176\n",
      "losses are tensor([1.0941, 1.0391, 0.9796, 0.9028, 1.0282, 1.0458],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.089634895324707\n",
      "losses are tensor([1.0566, 0.9992, 0.9336, 0.8590, 0.9894, 1.0096],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.84744119644165\n",
      "losses are tensor([1.1556, 1.1550, 1.1271, 1.2184, 1.1829, 1.1523],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.991264343261719\n",
      "losses are tensor([1.1184, 1.1146, 1.0790, 1.1639, 1.1486, 1.0963],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.720758438110352\n",
      "losses are tensor([1.0831, 1.0763, 1.0342, 1.1134, 1.1159, 1.0458],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.468744277954102\n",
      "losses are tensor([1.0494, 1.0395, 0.9942, 1.0667, 1.0845, 1.0005],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.234692573547363\n",
      "losses are tensor([1.0170, 1.0042, 0.9579, 1.0251, 1.0541, 0.9593],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.01754903793335\n",
      "tensor([3., 3., 1., 0., 1., 2., 0., 0., 3., 2., 2., 1.])\n",
      "outer loop losses are tensor([1.3157, 1.4489, 1.0670, 1.4410, 1.4098], grad_fn=<NllLossBackward0>) and loss is 6.682400703430176\n",
      "local param grad is tensor([[-1.6209e-05,  1.6191e-05, -1.5996e-05,  ...,  8.9737e-06,\n",
      "         -4.0774e-05,  2.8713e-05],\n",
      "        [-6.6659e-06, -2.9689e-05, -1.7159e-05,  ...,  1.6005e-05,\n",
      "          1.3389e-05,  3.4212e-05],\n",
      "        [ 2.8214e-05,  8.4997e-05, -5.1039e-05,  ...,  1.5227e-05,\n",
      "         -9.6886e-06,  6.8964e-07],\n",
      "        ...,\n",
      "        [-2.0520e-06,  7.1201e-06,  3.7725e-05,  ..., -9.5335e-06,\n",
      "          8.0682e-05, -1.2119e-04],\n",
      "        [ 7.5852e-06, -8.8436e-05, -2.2161e-05,  ...,  1.0034e-04,\n",
      "         -2.0334e-05,  8.5080e-05],\n",
      "        [-8.9834e-05,  1.8250e-04,  1.0557e-04,  ...,  2.1972e-05,\n",
      "          2.6653e-04, -2.7185e-04]])\n",
      "local param grad is tensor([[-1.1383e-05,  3.2388e-05, -2.3770e-05,  ...,  1.8024e-05,\n",
      "         -5.1573e-05,  4.2232e-05],\n",
      "        [-1.7256e-05, -2.3501e-05, -3.9690e-05,  ...,  2.5106e-05,\n",
      "          4.8261e-06,  3.5531e-05],\n",
      "        [ 5.7938e-05,  1.1035e-04, -4.1135e-05,  ...,  1.5524e-05,\n",
      "          1.6821e-05, -1.1506e-05],\n",
      "        ...,\n",
      "        [-6.5091e-05, -5.8090e-07,  5.2225e-05,  ...,  1.5568e-06,\n",
      "          1.1677e-04, -1.9051e-04],\n",
      "        [ 7.1968e-05, -9.8620e-05, -5.1786e-05,  ...,  1.0087e-04,\n",
      "         -1.3187e-04,  1.0007e-04],\n",
      "        [-2.0962e-04,  2.4980e-04,  1.3321e-04,  ...,  1.2874e-05,\n",
      "          4.3989e-04, -4.1152e-04]])\n",
      "meta param grad is tensor([[-5.5046e-04, -4.1643e-04, -3.4182e-04,  ...,  8.9214e-04,\n",
      "         -2.2383e-04, -1.6506e-04],\n",
      "        [ 3.3789e-05,  1.1539e-04, -6.0258e-05,  ...,  2.0624e-04,\n",
      "         -4.2236e-04,  7.4205e-05],\n",
      "        [ 4.6241e-04, -8.2818e-04, -2.5301e-04,  ...,  6.3916e-04,\n",
      "         -4.5828e-05,  1.1767e-03],\n",
      "        ...,\n",
      "        [ 2.0614e-03, -2.4627e-03,  1.3144e-03,  ...,  1.1175e-04,\n",
      "          1.6540e-04,  5.1528e-04],\n",
      "        [ 4.5565e-03,  1.6908e-03, -5.8386e-03,  ..., -3.4432e-03,\n",
      "         -2.2118e-03, -2.2237e-03],\n",
      "        [-5.1808e-03, -1.8965e-03,  1.0170e-02,  ...,  3.2720e-03,\n",
      "          8.3302e-03, -2.9971e-03]])\n",
      "[3, 1, 3, 2, 2]\n",
      "[0. 3.]\n",
      "tensor([3., 3., 1., 0., 1., 2., 0., 0., 3., 2., 2., 1.])\n",
      "outer loop losses are tensor([1.4326, 1.5273, 1.1903, 1.1389, 1.5136, 1.5411, 1.1802],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 9.52407169342041\n",
      "local param grad is tensor([[ 2.0900e-05,  9.1099e-05, -6.5146e-05,  ..., -3.4748e-05,\n",
      "          5.3685e-05, -1.5588e-05],\n",
      "        [ 1.3623e-05,  2.4456e-05,  5.8797e-05,  ...,  4.5013e-06,\n",
      "         -1.6407e-05,  7.2600e-05],\n",
      "        [ 1.2211e-05, -8.0559e-05,  4.6054e-05,  ...,  8.9688e-06,\n",
      "         -1.3891e-04, -2.5052e-08],\n",
      "        ...,\n",
      "        [ 1.2637e-04,  1.0879e-04, -6.4466e-05,  ..., -2.1496e-04,\n",
      "         -7.5396e-05, -1.8665e-04],\n",
      "        [ 1.9269e-04, -2.0581e-05, -2.1083e-04,  ..., -1.4940e-05,\n",
      "         -3.1150e-04, -1.2701e-04],\n",
      "        [ 1.1392e-05, -5.5560e-05,  1.3444e-04,  ...,  7.4400e-05,\n",
      "         -4.8726e-05, -1.4573e-04]])\n",
      "local param grad is tensor([[ 1.8559e-05,  9.0948e-05, -5.7727e-05,  ..., -3.1789e-05,\n",
      "          3.4003e-05,  3.0246e-05],\n",
      "        [ 7.5184e-06,  3.3796e-05,  6.9233e-05,  ...,  8.6278e-06,\n",
      "         -8.6415e-06,  9.0435e-05],\n",
      "        [ 1.8588e-05, -6.7472e-05,  4.1053e-05,  ...,  9.9264e-06,\n",
      "         -1.4496e-04, -9.1198e-06],\n",
      "        ...,\n",
      "        [ 1.1540e-04,  1.4396e-04, -6.8418e-05,  ..., -2.2220e-04,\n",
      "         -5.1938e-05, -1.7308e-04],\n",
      "        [ 2.3257e-04,  4.9469e-06, -2.3499e-04,  ..., -4.7109e-05,\n",
      "         -3.5671e-04, -1.2326e-04],\n",
      "        [ 7.4117e-06, -8.6781e-05,  1.6971e-04,  ...,  8.4354e-05,\n",
      "         -2.2368e-05, -1.5293e-04]])\n",
      "meta param grad is tensor([[-5.1100e-04, -2.3438e-04, -4.6469e-04,  ...,  8.2561e-04,\n",
      "         -1.3615e-04, -1.5040e-04],\n",
      "        [ 5.4930e-05,  1.7364e-04,  6.7772e-05,  ...,  2.1937e-04,\n",
      "         -4.4741e-04,  2.3724e-04],\n",
      "        [ 4.9321e-04, -9.7621e-04, -1.6591e-04,  ...,  6.5805e-04,\n",
      "         -3.2970e-04,  1.1676e-03],\n",
      "        ...,\n",
      "        [ 2.3032e-03, -2.2099e-03,  1.1815e-03,  ..., -3.2541e-04,\n",
      "          3.8063e-05,  1.5554e-04],\n",
      "        [ 4.9817e-03,  1.6751e-03, -6.2844e-03,  ..., -3.5053e-03,\n",
      "         -2.8800e-03, -2.4740e-03],\n",
      "        [-5.1620e-03, -2.0388e-03,  1.0474e-02,  ...,  3.4308e-03,\n",
      "          8.2591e-03, -3.2958e-03]])\n",
      "[3, 0, 1, 2, 0, 0, 1]\n",
      "[1. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses are tensor([1.1566, 1.1345, 1.1562, 1.0961, 1.1795, 1.2048],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.927670001983643\n",
      "losses are tensor([1.1240, 1.0865, 1.1282, 1.0510, 1.1349, 1.1722],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.696740627288818\n",
      "losses are tensor([1.0939, 1.0424, 1.1040, 1.0099, 1.0939, 1.1419],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.486083984375\n",
      "losses are tensor([1.0676, 1.0018, 1.0828, 0.9728, 1.0562, 1.1149],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.296173572540283\n",
      "losses are tensor([1.0427, 0.9653, 1.0626, 0.9400, 1.0216, 1.0891],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.121336460113525\n",
      "losses are tensor([1.1515, 1.0647, 1.0750, 1.0913, 1.2529, 1.1209],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.756390571594238\n",
      "losses are tensor([1.1054, 1.0234, 1.0363, 1.0406, 1.2179, 1.0784],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.502008438110352\n",
      "losses are tensor([1.0620, 0.9838, 0.9988, 0.9926, 1.1843, 1.0380],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.259434223175049\n",
      "losses are tensor([1.0205, 0.9479, 0.9636, 0.9465, 1.1519, 0.9988],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 6.029187202453613\n",
      "losses are tensor([0.9804, 0.9138, 0.9307, 0.9019, 1.1225, 0.9607],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 5.810020446777344\n",
      "tensor([0., 1., 2., 1., 0., 3., 3., 2., 0., 2., 3., 1.])\n",
      "outer loop losses are tensor([1.4872, 1.5290, 1.1369, 1.1201, 1.1068, 1.4813],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 7.861334800720215\n",
      "local param grad is tensor([[-1.4348e-05, -6.8897e-07,  8.6297e-05,  ...,  5.8245e-05,\n",
      "          1.9800e-05,  8.8477e-05],\n",
      "        [-1.5861e-06,  5.6771e-05, -1.8472e-05,  ..., -6.1908e-06,\n",
      "          5.6324e-05,  3.2049e-05],\n",
      "        [ 7.8213e-05, -5.1677e-05, -8.3189e-05,  ..., -1.8134e-05,\n",
      "         -1.6096e-05,  1.8798e-05],\n",
      "        ...,\n",
      "        [ 6.5099e-05, -2.7711e-04, -1.6437e-04,  ..., -4.8241e-04,\n",
      "         -2.4896e-04,  9.9455e-06],\n",
      "        [-1.7346e-04,  5.0716e-04,  1.9728e-04,  ..., -1.4084e-04,\n",
      "         -1.0891e-04, -4.1813e-04],\n",
      "        [ 9.2238e-05, -2.7672e-04,  3.1228e-04,  ..., -5.3498e-04,\n",
      "          1.4574e-04, -5.2773e-04]])\n",
      "local param grad is tensor([[-1.7913e-05,  1.3643e-06,  8.6514e-05,  ...,  5.8036e-05,\n",
      "          2.1601e-05,  7.5439e-05],\n",
      "        [-1.1946e-05,  6.3869e-05, -9.3542e-06,  ..., -4.0065e-06,\n",
      "          7.6029e-05,  2.5919e-05],\n",
      "        [ 7.5312e-05, -6.5020e-05, -5.7134e-05,  ...,  3.3661e-06,\n",
      "          5.7881e-06,  5.1138e-05],\n",
      "        ...,\n",
      "        [ 1.0059e-05, -3.4111e-04, -7.4992e-05,  ..., -3.8044e-04,\n",
      "         -1.9640e-04,  4.5562e-05],\n",
      "        [-2.2922e-04,  5.8837e-04,  2.0687e-04,  ..., -2.4215e-04,\n",
      "         -7.8553e-05, -5.0449e-04],\n",
      "        [ 3.7431e-05, -3.7404e-04,  4.4669e-04,  ..., -4.3344e-04,\n",
      "          1.1664e-04, -5.2571e-04]])\n",
      "meta param grad is tensor([[-5.4326e-04, -2.3371e-04, -2.9188e-04,  ...,  9.4189e-04,\n",
      "         -9.4745e-05,  1.3519e-05],\n",
      "        [ 4.1398e-05,  2.9428e-04,  3.9946e-05,  ...,  2.0918e-04,\n",
      "         -3.1505e-04,  2.9521e-04],\n",
      "        [ 6.4673e-04, -1.0929e-03, -3.0623e-04,  ...,  6.4328e-04,\n",
      "         -3.4001e-04,  1.2375e-03],\n",
      "        ...,\n",
      "        [ 2.3783e-03, -2.8281e-03,  9.4218e-04,  ..., -1.1883e-03,\n",
      "         -4.0729e-04,  2.1105e-04],\n",
      "        [ 4.5791e-03,  2.7706e-03, -5.8803e-03,  ..., -3.8883e-03,\n",
      "         -3.0675e-03, -3.3966e-03],\n",
      "        [-5.0324e-03, -2.6896e-03,  1.1233e-02,  ...,  2.4624e-03,\n",
      "          8.5215e-03, -4.3492e-03]])\n",
      "[2, 1, 0, 3, 0, 1]\n",
      "[0. 3.]\n",
      "tensor([0., 1., 2., 1., 0., 3., 3., 2., 0., 2., 3., 1.])\n",
      "outer loop losses are tensor([1.4489, 1.1819, 1.5049, 1.1061, 1.0899, 1.4337],\n",
      "       grad_fn=<NllLossBackward0>) and loss is 7.7653021812438965\n",
      "local param grad is tensor([[-1.8643e-05,  2.4087e-05,  1.1531e-04,  ..., -8.6390e-06,\n",
      "         -4.3791e-07,  3.2819e-05],\n",
      "        [-1.4158e-05, -6.5355e-05,  4.0977e-06,  ...,  2.2584e-05,\n",
      "         -7.6879e-05,  9.1799e-05],\n",
      "        [ 5.2615e-05, -4.3679e-05,  7.4596e-06,  ..., -3.1703e-05,\n",
      "          2.2700e-05,  1.3268e-07],\n",
      "        ...,\n",
      "        [-1.6313e-05, -1.0335e-05, -3.9187e-04,  ..., -1.7609e-05,\n",
      "          8.0751e-05, -2.1517e-04],\n",
      "        [-4.9564e-05,  1.3128e-04,  1.0806e-04,  ..., -2.1666e-05,\n",
      "          6.2316e-05, -7.1189e-05],\n",
      "        [-7.7872e-05, -4.2509e-05,  8.1588e-05,  ...,  2.9361e-04,\n",
      "          1.5526e-04,  1.2733e-06]])\n",
      "local param grad is tensor([[-3.6449e-05,  2.5522e-05,  1.4229e-04,  ..., -4.6247e-06,\n",
      "          1.8250e-05,  1.9274e-05],\n",
      "        [ 7.1427e-06, -6.2444e-05, -1.4459e-05,  ...,  3.4262e-06,\n",
      "         -8.3697e-05,  1.0724e-04],\n",
      "        [ 5.6592e-05, -9.5341e-05, -2.5692e-06,  ..., -3.2395e-05,\n",
      "          3.6995e-05,  2.3815e-05],\n",
      "        ...,\n",
      "        [-8.9700e-05, -8.1987e-05, -4.6763e-04,  ..., -4.8693e-05,\n",
      "         -1.2710e-05, -1.5252e-04],\n",
      "        [-4.6173e-05,  1.9406e-04,  9.4077e-05,  ..., -8.1137e-05,\n",
      "          6.5477e-05, -1.3144e-04],\n",
      "        [-1.2170e-04, -8.0020e-05,  1.4993e-04,  ...,  4.0563e-04,\n",
      "          1.4542e-04,  6.7180e-05]])\n",
      "meta param grad is tensor([[-5.9835e-04, -1.8410e-04, -3.4276e-05,  ...,  9.2862e-04,\n",
      "         -7.6933e-05,  6.5611e-05],\n",
      "        [ 3.4383e-05,  1.6649e-04,  2.9584e-05,  ...,  2.3519e-04,\n",
      "         -4.7563e-04,  4.9425e-04],\n",
      "        [ 7.5594e-04, -1.2319e-03, -3.0134e-04,  ...,  5.7919e-04,\n",
      "         -2.8032e-04,  1.2614e-03],\n",
      "        ...,\n",
      "        [ 2.2723e-03, -2.9205e-03,  8.2669e-05,  ..., -1.2546e-03,\n",
      "         -3.3925e-04, -1.5664e-04],\n",
      "        [ 4.4833e-03,  3.0960e-03, -5.6781e-03,  ..., -3.9911e-03,\n",
      "         -2.9397e-03, -3.5992e-03],\n",
      "        [-5.2319e-03, -2.8121e-03,  1.1464e-02,  ...,  3.1616e-03,\n",
      "          8.8222e-03, -4.2807e-03]])\n",
      "[0, 1, 3, 2, 2, 3]\n",
      "[1. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksingh/miniconda3/envs/meta-learned-lines/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from training.models.ProtoFOMAML import ProtoFOMAML\n",
    "\n",
    "pfomaml = ProtoFOMAML(outerLR=5e-4, innerLR=1e-3, outputLR=1e-2, steps=5, batchSize=16, warmupSteps=0)\n",
    "pfomaml.training_step(next(iter(train_protomaml_loader)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3670cc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from samplers.FewShotValidationEpisodeBatchSampler import FewShotValidationEpisodeBatchSampler\n",
    "from samplers.FewShotValidationEpisodeSampler import FewShotValidationEpisodeSampler\n",
    "from validation_datasets.ValidationDataset import ValidationDataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "ds = ValidationDataset()\n",
    "\n",
    "val_protomaml_sampler = FewShotValidationEpisodeBatchSampler(ds, kShot=2)\n",
    "val_protomaml_loader = data.DataLoader(\n",
    "    ds, batch_sampler=val_protomaml_sampler, collate_fn=val_protomaml_sampler.getCollateFunction(), num_workers=1\n",
    ")\n",
    "\n",
    "# val_protomaml_loader = data.DataLoader(\n",
    "#     None, batch_sampler=val_protomaml_sampler, num_workers=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b555c562",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0]\n",
      "[4, 4, 2, 2, 3, 3]\n",
      "[5, 7, 6, 6, 5, 7]\n",
      "[10, 8, 9, 9, 8, 10]\n",
      "[12, 13, 11, 13, 11, 12]\n",
      "[14, 15, 14, 15]\n",
      "[17, 16, 17, 16]\n",
      "[18, 18, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    batch = next(iter(val_protomaml_loader))\n",
    "#     print(batch)\n",
    "    for episode_i in range(len(batch[0])):\n",
    "        data, labels = batch[0][episode_i], batch[1][episode_i]\n",
    "        supportSet, supportLabels = data[0:len(data)//2], labels[0:len(data)//2]\n",
    "        print(supportLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
